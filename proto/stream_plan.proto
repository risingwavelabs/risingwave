syntax = "proto3";

package stream_plan;

import "catalog.proto";
import "common.proto";
import "data.proto";
import "expr.proto";
import "plan_common.proto";
import "source.proto";

option optimize_for = SPEED;

message AddMutation {
  message Dispatchers {
    repeated Dispatcher dispatchers = 1;
  }
  // New dispatchers for each actor.
  map<uint32, Dispatchers> actor_dispatchers = 1;
  // We may embed a source change split mutation here.
  // TODO: we may allow multiple mutations in a single barrier.
  map<uint32, source.ConnectorSplits> actor_splits = 2;
}

message StopMutation {
  repeated uint32 actors = 1;
}

message UpdateMutation {
  message DispatcherUpdate {
    // Dispatcher can be uniquely identified by a combination of actor id and dispatcher id.
    uint32 actor_id = 1;
    uint64 dispatcher_id = 2;
    // The hash mapping for consistent hash.
    // For dispatcher types other than HASH, this is ignored.
    ActorMapping hash_mapping = 3;
    // Added downstream actors.
    repeated uint32 added_downstream_actor_id = 4;
    // Removed downstream actors.
    repeated uint32 removed_downstream_actor_id = 5;
  }
  // TODO: These actor ids should be the same as those in `DispatcherUpdate`.
  // We may find a way to deduplicate this.
  message MergeUpdate {
    // Merge executor can be uniquely identified by a combination of actor id and upstream fragment id.
    uint32 actor_id = 1;
    uint32 upstream_fragment_id = 2;
    // Added upstream actors.
    repeated uint32 added_upstream_actor_id = 3;
    // Removed upstream actors.
    repeated uint32 removed_upstream_actor_id = 4;
  }
  // Dispatcher updates for each upstream actor.
  repeated DispatcherUpdate actor_dispatcher_update = 1;
  // Merge updates for each downstream actor.
  repeated MergeUpdate actor_merge_update = 2;
  // Vnode bitmap updates for each actor.
  map<uint32, common.Buffer> actor_vnode_bitmap_update = 3;
  // All actors to be dropped in this update.
  repeated uint32 dropped_actors = 4;
}

message SourceChangeSplitMutation {
  map<uint32, source.ConnectorSplits> actor_splits = 2;
}

message PauseMutation {}

message ResumeMutation {}

message Barrier {
  data.Epoch epoch = 1;
  oneof mutation {
    // Add new dispatchers to some actors, used for creating materialized views.
    AddMutation add = 3;
    // Stop a set of actors, used for dropping materialized views. Empty dispatchers will be
    // automatically removed.
    StopMutation stop = 4;
    // Update outputs and hash mappings for some dispatchers, used for scaling.
    UpdateMutation update = 5;
    // Change the split of some sources.
    SourceChangeSplitMutation splits = 6;
    // Pause the dataflow of the whole streaming graph.
    PauseMutation pause = 7;
    // Resume the dataflow of the whole streaming graph.
    ResumeMutation resume = 8;
  }
  // Used for tracing.
  bytes span = 2;
  // Whether this barrier do checkpoint
  bool checkpoint = 9;

  // Record the actors that the barrier has passed. Only used for debugging.
  repeated uint32 passed_actors = 255;
}

message StreamMessage {
  oneof stream_message {
    data.StreamChunk stream_chunk = 1;
    Barrier barrier = 2;
  }
}

// Hash mapping for compute node. Stores mapping from virtual node to actor id.
message ActorMapping {
  repeated uint64 original_indices = 1;
  repeated uint32 data = 2;
}

// todo: StreamSourceNode or TableSourceNode
message SourceNode {
  enum SourceType {
    UNSPECIFIED = 0;
    TABLE = 1;
    SOURCE = 2;
  }
  uint32 source_id = 1; // use source_id to fetch SourceDesc from local source manager
  repeated int32 column_ids = 2;
  SourceType source_type = 3;
  uint32 state_table_id = 4; // use state_table_id as state store prefix
}

message SinkNode {
  uint32 table_id = 1;
  repeated int32 column_ids = 2;
  map<string, string> properties = 3;
}

message ProjectNode {
  repeated expr.ExprNode select_list = 1;
}

message FilterNode {
  expr.ExprNode search_condition = 1;
}

// A materialized view is regarded as a table.
// In addition, we also specify primary key to MV for efficient point lookup during update and deletion.
//
// The node will be used for both create mv and create index.
// - When creating mv, `pk == distribution_key == column_orders`.
// - When creating index, `column_orders` will contain both
//   arrange columns and pk columns, while distribution key will be arrange columns.
message MaterializeNode {
  uint32 table_id = 1;
  // Column indexes and orders of primary key
  repeated plan_common.ColumnOrder column_orders = 2;
  // Used for internal table states.
  catalog.Table table = 3;
}

// Remark by Yanghao: for both local and global we use the same node in the protobuf.
// Local and global aggregator distinguish with each other in PlanNode definition.
message SimpleAggNode {
  repeated expr.AggCall agg_calls = 1;
  // Only used for local simple agg, not used for global simple agg.
  repeated uint32 distribution_key = 2;
  repeated catalog.Table internal_tables = 3;
  repeated ColumnMapping column_mappings = 4;
  // Whether to optimize for append only stream.
  // It is true when the input is append-only
  bool is_append_only = 5;
}

message ColumnMapping {
  repeated uint32 indices = 1;
}
message HashAggNode {
  repeated uint32 group_key = 1;
  repeated expr.AggCall agg_calls = 2;
  repeated catalog.Table internal_tables = 3;
  repeated ColumnMapping column_mappings = 4;
  // Whether to optimize for append only stream.
  // It is true when the input is append-only
  bool is_append_only = 5;
}

message TopNNode {
  // 0 means no limit as limit of 0 means this node should be optimized away
  uint64 limit = 1;
  uint64 offset = 2;
  catalog.Table table = 3;
}

message GroupTopNNode {
  // 0 means no limit as limit of 0 means this node should be optimized away
  uint64 limit = 1;
  uint64 offset = 2;
  repeated uint32 group_key = 3;
  catalog.Table table = 4;
}

message HashJoinNode {
  plan_common.JoinType join_type = 1;
  repeated int32 left_key = 2;
  repeated int32 right_key = 3;
  expr.ExprNode condition = 4;
  // Used for internal table states.
  catalog.Table left_table = 6;
  // Used for internal table states.
  catalog.Table right_table = 7;
  // Used for internal table states.
  catalog.Table left_degree_table = 8;
  // Used for internal table states.
  catalog.Table right_degree_table = 9;
  // The output indices of current node
  repeated uint32 output_indices = 10;
  repeated bool null_safe = 11;
  // Whether to optimize for append only stream.
  // It is true when the input is append-only
  bool is_append_only = 12;
}

message DynamicFilterNode {
  uint32 left_key = 1;
  // Must be one of <, <=, >, >=
  expr.ExprNode condition = 2;
  // Left table stores all states with predicate possibly not NULL.
  catalog.Table left_table = 3;
  // Right table stores single value from RHS of predicate.
  catalog.Table right_table = 4;
  // It is true when the right side of the inequality predicate is monotonically:
  // - decreasing for <, <=, increasing for >, >=
  // bool is_monotonic = 10;
  // the output indices of current node
  // repeated uint32 output_indices = 11;
}

// Delta join with two indexes. This is a pseudo plan node generated on frontend. On meta
// service, it will be rewritten into lookup joins.
message DeltaIndexJoinNode {
  plan_common.JoinType join_type = 1;
  repeated int32 left_key = 2;
  repeated int32 right_key = 3;
  expr.ExprNode condition = 4;
  // Table id of the left index.
  uint32 left_table_id = 7;
  // Table id of the right index.
  uint32 right_table_id = 8;
  // Info about the left index
  ArrangementInfo left_info = 9;
  // Info about the right index
  ArrangementInfo right_info = 10;
  // the output indices of current node
  repeated uint32 output_indices = 11;
}

message HopWindowNode {
  expr.InputRefExpr time_col = 1;
  data.IntervalUnit window_slide = 2;
  data.IntervalUnit window_size = 3;
  repeated uint32 output_indices = 4;
}

message MergeNode {
  repeated uint32 upstream_actor_id = 1;
  uint32 upstream_fragment_id = 2;
  // Type of the upstream dispatcher. If there's always one upstream according to this
  // type, the compute node may use the `ReceiverExecutor` as an optimization.
  DispatcherType upstream_dispatcher_type = 3;
  // The schema of input columns. TODO: remove this field.
  repeated plan_common.Field fields = 4;
}

// passed from frontend to meta, used by fragmenter to generate `MergeNode`
// and maybe `DispatcherNode` later.
message ExchangeNode {
  DispatchStrategy strategy = 1;
}

// ChainNode is used for mv on mv.
// ChainNode is like a "UNION" on mv snapshot and streaming. So it takes two inputs with fixed order:
//   1. MergeNode (as a placeholder) for streaming read.
//   2. BatchPlanNode for snapshot read.
message ChainNode {
  uint32 table_id = 1;
  // The schema of input stream, which will be used to build a MergeNode
  repeated plan_common.Field upstream_fields = 2;
  // Which columns from upstream are used in this Chain node.
  repeated uint32 upstream_column_indices = 3;
  // Generally, the barrier needs to be rearranged during the MV creation process, so that data can
  // be flushed to shared buffer periodically, instead of making the first epoch from batch query extra
  // large. However, in some cases, e.g., shared state, the barrier cannot be rearranged in ChainNode.
  // This option is used to disable barrier rearrangement.
  bool disable_rearrange = 4;
  // Whether to place this chain on the same worker node as upstream actors.
  bool same_worker_node = 5;
  // Whether the upstream materialize is and this chain should be a singleton.
  // FIXME: This is a workaround for fragmenter since the distribution info will be lost if there's only one
  // fragment in the downstream mview. Remove this when we refactor the fragmenter.
  bool is_singleton = 6;
}

// BatchPlanNode is used for mv on mv snapshot read.
// BatchPlanNode is supposed to carry a batch plan that can be optimized with the streaming plan_common.
// Currently, streaming to batch push down is not yet supported, BatchPlanNode is simply a table scan.
message BatchPlanNode {
  plan_common.StorageTableDesc table_desc = 1;
  repeated int32 column_ids = 2;
}

message ArrangementInfo {
  // Order key of the arrangement, including order by columns and pk from the materialize
  // executor.
  repeated plan_common.ColumnOrder arrange_key_orders = 1;
  // Column descs of the arrangement
  repeated plan_common.ColumnDesc column_descs = 2;
}

// Special node for shared state, which will only be produced in fragmenter. ArrangeNode will
// produce a special Materialize executor, which materializes data for downstream to query.
message ArrangeNode {
  // Info about the arrangement
  ArrangementInfo table_info = 1;
  // Hash key of the materialize node, which is a subset of pk.
  repeated uint32 distribution_key = 2;
  // Used for internal table states.
  catalog.Table table = 3;
}

// Special node for shared state. LookupNode will join an arrangement with a stream.
message LookupNode {
  // Join key of the arrangement side
  repeated int32 arrange_key = 1;
  // Join key of the stream side
  repeated int32 stream_key = 2;
  // Whether to join the current epoch of arrangement
  bool use_current_epoch = 3;
  // Sometimes we need to re-order the output data to meet the requirement of schema.
  // By default, lookup executor will produce `<arrangement side, stream side>`. We
  // will then apply the column mapping to the combined result.
  repeated int32 column_mapping = 4;
  oneof arrangement_table_id {
    // Table Id of the arrangement (when created along with join plan)
    uint32 table_id = 5;
    // Table Id of the arrangement (when using index)
    uint32 index_id = 6;
  }
  // Info about the arrangement
  ArrangementInfo arrangement_table_info = 7;
  // Internal table of arrangement.
  catalog.Table arrangement_table = 8;
}

// Acts like a merger, but on different inputs.
message UnionNode {}

// Special node for shared state. Merge and align barrier from upstreams. Pipe inputs in order.
message LookupUnionNode {
  repeated uint32 order = 1;
}

message ExpandNode {
  message Subset {
    repeated uint32 column_indices = 1;
  }
  repeated Subset column_subsets = 1;
}

message ProjectSetNode {
  repeated expr.ProjectSetSelectItem select_list = 1;
}

message StreamNode {
  oneof node_body {
    SourceNode source = 100;
    ProjectNode project = 101;
    FilterNode filter = 102;
    MaterializeNode materialize = 103;
    SimpleAggNode local_simple_agg = 104;
    SimpleAggNode global_simple_agg = 105;
    HashAggNode hash_agg = 106;
    TopNNode append_only_top_n = 107;
    HashJoinNode hash_join = 108;
    TopNNode top_n = 109;
    HopWindowNode hop_window = 110;
    MergeNode merge = 111;
    ExchangeNode exchange = 112;
    ChainNode chain = 113;
    BatchPlanNode batch_plan = 114;
    LookupNode lookup = 115;
    ArrangeNode arrange = 116;
    LookupUnionNode lookup_union = 117;
    UnionNode union = 118;
    DeltaIndexJoinNode delta_index_join = 119;
    SinkNode sink = 120;
    ExpandNode expand = 121;
    DynamicFilterNode dynamic_filter = 122;
    ProjectSetNode project_set = 123;
    GroupTopNNode group_top_n = 124;
  }
  // The id for the operator. This is local per mview.
  // TODO: should better be a uint32.
  uint64 operator_id = 1;
  // Child node in plan aka. upstream nodes in the streaming DAG
  repeated StreamNode input = 3;
  repeated uint32 stream_key = 2;
  bool append_only = 24;
  string identity = 18;
  // The schema of the plan node
  repeated plan_common.Field fields = 19;
}

enum DispatcherType {
  UNSPECIFIED = 0;
  // Dispatch by hash key, hashed by consistent hash.
  HASH = 1;
  // Broadcast to all downstreams.
  //
  // Note a broadcast cannot be represented as multiple simple dispatchers, since they are
  // different when we update dispatchers during scaling.
  BROADCAST = 2;
  // Only one downstream.
  SIMPLE = 3;
  // A special kind of exchange that doesn't involve shuffle. The upstream actor will be directly
  // piped into the downstream actor, if there are the same number of actors. If number of actors
  // are not the same, should use hash instead. Should be only used when distribution is the same.
  NO_SHUFFLE = 4;
}

message DispatchStrategy {
  DispatcherType type = 1;
  repeated uint32 column_indices = 2;
}

// A dispatcher redistribute messages.
// We encode both the type and other usage information in the proto.
message Dispatcher {
  DispatcherType type = 1;
  // Indices of the columns to be used for hashing.
  // For dispatcher types other than HASH, this is ignored.
  repeated uint32 column_indices = 2;
  // The hash mapping for consistent hash.
  // For dispatcher types other than HASH, this is ignored.
  ActorMapping hash_mapping = 3;
  // Dispatcher can be uniquely identified by a combination of actor id and dispatcher id.
  // - For dispatchers within actors, the id is the same as operator_id of the exchange plan node.
  // - For MV on MV, the id is the same as the actor id of chain node in the downstream MV.
  uint64 dispatcher_id = 4;
  // Number of downstreams decides how many endpoints a dispatcher should dispatch.
  repeated uint32 downstream_actor_id = 5;
}

// A StreamActor is a running fragment of the overall stream graph,
message StreamActor {
  uint32 actor_id = 1;
  uint32 fragment_id = 2;
  StreamNode nodes = 3;
  repeated Dispatcher dispatcher = 4;
  // The actors that send messages to this actor.
  // Note that upstream actor ids are also stored in the proto of merge nodes.
  // It is painstaking to traverse through the node tree and get upstream actor id from the root StreamNode.
  // We duplicate the information here to ease the parsing logic in stream manager.
  repeated uint32 upstream_actor_id = 6;
  // Placement rule for actor, need to stay on the same node as upstream.
  bool same_worker_node_as_upstream = 7;
  // Vnodes that the executors in this actor own. If this actor is the only actor in its fragment, `vnode_bitmap`
  // will be empty.
  common.Buffer vnode_bitmap = 8;
}

enum FragmentType {
  FRAGMENT_UNSPECIFIED = 0;
  OTHERS = 1;
  SOURCE = 2;
  // TODO: change it to MATERIALIZED_VIEW or other name, since we have sink type now.
  SINK = 3;
}

message StreamFragmentGraph {
  message StreamFragment {
    // 0-based on frontend, and will be rewritten to global id on meta.
    uint32 fragment_id = 1;
    // root stream node in this fragment.
    StreamNode node = 2;
    FragmentType fragment_type = 3;
    // mark whether this fragment should only have one actor.
    bool is_singleton = 4;
    // Number of table ids (stateful states) for this fragment.
    uint32 table_ids_cnt = 5;
    // Mark the upstream table ids of this fragment, Used for fragments with `Chain`s.
    repeated uint32 upstream_table_ids = 6;
  }

  message StreamFragmentEdge {
    // Dispatch strategy for the fragment.
    DispatchStrategy dispatch_strategy = 1;
    // Whether the two linked nodes should be placed on the same worker node
    bool same_worker_node = 2;
    // A unique identifier of this edge. Generally it should be exchange node's operator id. When
    // rewriting fragments into delta joins or when inserting 1-to-1 exchange, there will be
    // virtual links generated.
    uint64 link_id = 3;
    uint32 upstream_id = 4;
    uint32 downstream_id = 5;
  }
  // all the fragments in the graph.
  map<uint32, StreamFragment> fragments = 1;
  // edges between fragments.
  repeated StreamFragmentEdge edges = 2;

  repeated uint32 dependent_table_ids = 3;
  uint32 table_ids_cnt = 4;
}
