syntax = "proto3";

package meta;

import "backup_service.proto";
import "catalog.proto";
import "common.proto";
import "hummock.proto";
import "source.proto";
import "stream_plan.proto";
import "user.proto";

option java_package = "com.risingwave.proto";
option optimize_for = SPEED;

message GetTelemetryInfoRequest {}

message TelemetryInfoResponse {
  optional string tracking_id = 1;
}

service TelemetryInfoService {
  // Request telemetry info from meta node
  rpc GetTelemetryInfo(GetTelemetryInfoRequest) returns (TelemetryInfoResponse);
}

message HeartbeatRequest {
  message ExtraInfo {
    oneof info {
      uint64 hummock_gc_watermark = 1;
    }
  }
  uint32 node_id = 1;
  // Lightweight info piggybacked by heartbeat request.
  repeated ExtraInfo info = 2;
}

message HeartbeatResponse {
  common.Status status = 1;
}

service HeartbeatService {
  rpc Heartbeat(HeartbeatRequest) returns (HeartbeatResponse);
}

// Fragments of a Streaming Job
message TableFragments {
  // The state of the fragments of this table
  enum State {
    UNSPECIFIED = 0;
    // The streaming job is initial.
    INITIAL = 1;
    // The streaming job is creating.
    CREATING = 2;
    // The streaming job has been created.
    CREATED = 3;
  }
  // Runtime information of an actor
  message ActorStatus {
    // Current state of actor
    enum ActorState {
      UNSPECIFIED = 0;
      // Initial state after creation
      INACTIVE = 1;
      // Running normally
      RUNNING = 2;
    }
    // Current on which worker
    common.ActorLocation location = 1;
    // Current state
    ActorState state = 2;
  }
  message Fragment {
    enum FragmentDistributionType {
      UNSPECIFIED = 0;
      SINGLE = 1;
      HASH = 2;
    }
    uint32 fragment_id = 1;
    // Bitwise-OR of FragmentTypeFlags
    uint32 fragment_type_mask = 2;
    FragmentDistributionType distribution_type = 3;
    repeated stream_plan.StreamActor actors = 4;

    // NOTE: vnode_mapping is deprecated, we will generate the vnode_mapping by actors' bitmaps
    // Vnode mapping (which should be set in upstream dispatcher) of the fragment.
    // This field is always set to `Some`. For singleton, the parallel unit for all vnodes will be the same.
    reserved 5;
    reserved "vnode_mapping";

    repeated uint32 state_table_ids = 6;
    // Note that this can be derived backwards from the upstream actors of the Actor held by the Fragment,
    // but in some scenarios (e.g. Scaling) it will lead to a lot of duplicate code,
    // so we pre-generate and store it here, this member will only be initialized when creating the Fragment
    // and modified when creating the mv-on-mv
    repeated uint32 upstream_fragment_ids = 7;
  }
  uint32 table_id = 1;
  State state = 2;
  map<uint32, Fragment> fragments = 3;
  map<uint32, ActorStatus> actor_status = 4;
  // `Source` and `SourceBackfill` are handled together here.
  map<uint32, source.ConnectorSplits> actor_splits = 5;

  stream_plan.StreamContext ctx = 6;
  TableParallelism parallelism = 7;

  // Actors of a materialize view, sink, or table can only be scheduled on nodes with matching node_label.
  string node_label = 8;

  // If this is a materialized view: True if backfill is done, else false.
  // If this is a regular table: Always true.
  bool backfill_done = 9;
}

/// Worker slot mapping with fragment id, used for notification.
message FragmentWorkerSlotMapping {
  uint32 fragment_id = 1;
  common.WorkerSlotMapping mapping = 2;
}

message FragmentWorkerSlotMappings {
  repeated FragmentWorkerSlotMapping mappings = 1;
}

// TODO: remove this when dashboard refactored.
message ActorLocation {
  common.WorkerNode node = 1;
  repeated stream_plan.StreamActor actors = 2;
}

message MigrationPlan {
  // NOTE: parallel_unit_migration_plan is deprecated, using worker_slot_migration_plan instead
  // map<parallel_unit_id, parallel_unit>, the plan indicates that the actors will be migrated from old parallel unit to the new one.
  reserved 1;
  reserved "parallel_unit_migration_plan";

  // map<src_worker_slot_id, dst_worker_slot_id>, the plan indicates that the actors will be migrated from old worker_slot to the new one.
  map<uint64, uint64> worker_slot_migration_plan = 2;
}

message FlushRequest {
  bool checkpoint = 1;
}

message FlushResponse {
  common.Status status = 1;
  uint64 hummock_version_id = 2;
}

// The reason why the data sources in the cluster are paused.
enum PausedReason {
  PAUSED_REASON_UNSPECIFIED = 0;
  // The cluster is paused due to configuration change, e.g. altering table schema and scaling.
  PAUSED_REASON_CONFIG_CHANGE = 1;
  // The cluster is paused due to manual operation, e.g. `risectl` command or the
  // `pause_on_next_bootstrap` system variable.
  PAUSED_REASON_MANUAL = 2;
}

message PauseRequest {}

message PauseResponse {}

message ResumeRequest {}

message ResumeResponse {}

message CancelCreatingJobsRequest {
  message CreatingJobInfo {
    uint32 database_id = 1;
    uint32 schema_id = 2;
    string name = 3;
  }

  message CreatingJobInfos {
    repeated CreatingJobInfo infos = 1;
  }

  message CreatingJobIds {
    repeated uint32 job_ids = 1;
  }

  oneof jobs {
    CreatingJobInfos infos = 1;
    CreatingJobIds ids = 2;
  }
}

message CancelCreatingJobsResponse {
  common.Status status = 1;
  repeated uint32 canceled_jobs = 2;
}

message ListTableFragmentsRequest {
  repeated uint32 table_ids = 1;
}

message ListTableFragmentsResponse {
  message ActorInfo {
    uint32 id = 1;
    stream_plan.StreamNode node = 2;
    repeated stream_plan.Dispatcher dispatcher = 3;
  }
  message FragmentInfo {
    uint32 id = 1;
    repeated ActorInfo actors = 4;
  }
  message TableFragmentInfo {
    repeated FragmentInfo fragments = 1;
    stream_plan.StreamContext ctx = 2;
  }
  map<uint32, TableFragmentInfo> table_fragments = 1;
}

message ListTableFragmentStatesRequest {}

message ListTableFragmentStatesResponse {
  message TableFragmentState {
    uint32 table_id = 1;
    TableFragments.State state = 2;
    TableParallelism parallelism = 3;
  }
  repeated TableFragmentState states = 1;
}

message ListFragmentDistributionRequest {}

message ListFragmentDistributionResponse {
  message FragmentDistribution {
    uint32 fragment_id = 1;
    uint32 table_id = 2;
    TableFragments.Fragment.FragmentDistributionType distribution_type = 3;
    repeated uint32 state_table_ids = 4;
    repeated uint32 upstream_fragment_ids = 5;
    uint32 fragment_type_mask = 6;
    uint32 parallelism = 7;
  }
  repeated FragmentDistribution distributions = 1;
}

message ListActorStatesRequest {}

message ListActorStatesResponse {
  message ActorState {
    uint32 actor_id = 1;
    uint32 fragment_id = 2;
    reserved 3;
    reserved "parallel_unit_id";
    TableFragments.ActorStatus.ActorState state = 4;
    uint32 worker_id = 5;
  }
  repeated ActorState states = 1;
}

message ListObjectDependenciesRequest {}

message ListObjectDependenciesResponse {
  message ObjectDependencies {
    uint32 object_id = 1;
    uint32 referenced_object_id = 2;
  }
  repeated ObjectDependencies dependencies = 1;
}

enum ThrottleTarget {
  THROTTLE_TARGET_UNSPECIFIED = 0;
  SOURCE = 1;
  MV = 2;
  TABLE_WITH_SOURCE = 3;
  CDC_TABLE = 4;
}

message ApplyThrottleRequest {
  ThrottleTarget kind = 1;
  uint32 id = 2;
  optional uint32 rate = 3;
}

message ApplyThrottleResponse {
  common.Status status = 1;
}

message RecoverRequest {}

message RecoverResponse {}

service StreamManagerService {
  rpc Flush(FlushRequest) returns (FlushResponse);
  rpc Pause(PauseRequest) returns (PauseResponse);
  rpc Resume(ResumeRequest) returns (ResumeResponse);
  rpc CancelCreatingJobs(CancelCreatingJobsRequest) returns (CancelCreatingJobsResponse);
  rpc ListTableFragments(ListTableFragmentsRequest) returns (ListTableFragmentsResponse);
  rpc ListTableFragmentStates(ListTableFragmentStatesRequest) returns (ListTableFragmentStatesResponse);
  rpc ListFragmentDistribution(ListFragmentDistributionRequest) returns (ListFragmentDistributionResponse);
  rpc ListActorStates(ListActorStatesRequest) returns (ListActorStatesResponse);
  rpc ListObjectDependencies(ListObjectDependenciesRequest) returns (ListObjectDependenciesResponse);
  rpc ApplyThrottle(ApplyThrottleRequest) returns (ApplyThrottleResponse);
  rpc Recover(RecoverRequest) returns (RecoverResponse);
}

// Below for cluster service.

message AddWorkerNodeRequest {
  message Property {
    uint64 worker_node_parallelism = 1;
    bool is_streaming = 2;
    bool is_serving = 3;
    bool is_unschedulable = 4;
    // This is used for frontend node to register its rpc address
    string internal_rpc_host_addr = 5;
  }
  common.WorkerType worker_type = 1;
  common.HostAddress host = 2;
  reserved 3;
  Property property = 4;
  common.WorkerNode.Resource resource = 5;
}

message AddWorkerNodeResponse {
  optional uint32 node_id = 2;
  string cluster_id = 4;
}

message ActivateWorkerNodeRequest {
  common.HostAddress host = 1;
  uint32 node_id = 2;
}

message ActivateWorkerNodeResponse {
  common.Status status = 1;
}

message DeleteWorkerNodeRequest {
  common.HostAddress host = 1;
}

message DeleteWorkerNodeResponse {
  common.Status status = 1;
}

// Mark CN as schedulable or as unschedulable
message UpdateWorkerNodeSchedulabilityRequest {
  enum Schedulability {
    UNSPECIFIED = 0;
    SCHEDULABLE = 1;
    UNSCHEDULABLE = 2;
  }

  repeated uint32 worker_ids = 1;
  Schedulability schedulability = 2;
}

message UpdateWorkerNodeSchedulabilityResponse {
  common.Status status = 1;
}

message ListAllNodesRequest {
  optional common.WorkerType worker_type = 1;
  // Whether to include nodes still starting
  bool include_starting_nodes = 2;
}

message ListAllNodesResponse {
  common.Status status = 1;
  repeated common.WorkerNode nodes = 2;
}

message GetClusterRecoveryStatusRequest {}

enum RecoveryStatus {
  STATUS_UNSPECIFIED = 0;
  STATUS_STARTING = 1;
  STATUS_RECOVERING = 2;
  STATUS_RUNNING = 3;
}

message GetClusterRecoveryStatusResponse {
  RecoveryStatus status = 1;
}

service ClusterService {
  rpc AddWorkerNode(AddWorkerNodeRequest) returns (AddWorkerNodeResponse);
  rpc ActivateWorkerNode(ActivateWorkerNodeRequest) returns (ActivateWorkerNodeResponse);
  rpc DeleteWorkerNode(DeleteWorkerNodeRequest) returns (DeleteWorkerNodeResponse);
  rpc UpdateWorkerNodeSchedulability(UpdateWorkerNodeSchedulabilityRequest) returns (UpdateWorkerNodeSchedulabilityResponse);
  rpc ListAllNodes(ListAllNodesRequest) returns (ListAllNodesResponse);
  rpc GetClusterRecoveryStatus(GetClusterRecoveryStatusRequest) returns (GetClusterRecoveryStatusResponse);
}

enum SubscribeType {
  UNSPECIFIED = 0;
  FRONTEND = 1;
  HUMMOCK = 2;
  COMPACTOR = 3;
  COMPUTE = 4;
}

// Below for notification service.
message SubscribeRequest {
  SubscribeType subscribe_type = 1;
  common.HostAddress host = 2;
  uint32 worker_id = 3;
}

message MetaSnapshot {
  message SnapshotVersion {
    uint64 catalog_version = 1;
    reserved 2;
    reserved "parallel_unit_mapping_version";
    uint64 worker_node_version = 3;
    uint64 streaming_worker_slot_mapping_version = 4;
  }
  repeated catalog.Database databases = 1;
  repeated catalog.Schema schemas = 2;
  repeated catalog.Source sources = 3;
  repeated catalog.Sink sinks = 4;
  repeated catalog.Table tables = 5;
  repeated catalog.Index indexes = 6;
  repeated catalog.View views = 7;
  repeated catalog.Function functions = 15;
  repeated catalog.Connection connections = 17;
  repeated catalog.Subscription subscriptions = 19;
  repeated user.UserInfo users = 8;
  reserved 9;
  reserved "parallel_unit_mappings";
  GetSessionParamsResponse session_params = 20;
  repeated catalog.Secret secrets = 23;
  repeated common.WorkerNode nodes = 10;
  hummock.HummockVersion hummock_version = 12;
  backup_service.MetaBackupManifestId meta_backup_manifest_id = 14;
  hummock.WriteLimits hummock_write_limits = 16;
  reserved 18;
  reserved "serving_parallel_unit_mappings";

  // for streaming
  repeated FragmentWorkerSlotMapping streaming_worker_slot_mappings = 21;
  repeated FragmentWorkerSlotMapping serving_worker_slot_mappings = 22;

  SnapshotVersion version = 13;
}

message Relation {
  oneof relation_info {
    catalog.Table table = 1;
    catalog.Source source = 2;
    catalog.Sink sink = 3;
    catalog.Index index = 4;
    catalog.View view = 5;
    catalog.Subscription subscription = 6;
  }
}

message RelationGroup {
  repeated Relation relations = 1;
}

message Recovery {}

message SubscribeResponse {
  enum Operation {
    UNSPECIFIED = 0;
    ADD = 1;
    DELETE = 2;
    UPDATE = 3;
    SNAPSHOT = 4;
  }
  common.Status status = 1;
  Operation operation = 2;

  // Catalog version
  uint64 version = 3;

  oneof info {
    catalog.Database database = 4;
    catalog.Schema schema = 5;
    catalog.Function function = 6;
    user.UserInfo user = 11;
    SetSessionParamRequest session_param = 26;
    common.WorkerNode node = 13;
    hummock.HummockVersionDeltas hummock_version_deltas = 15;
    MetaSnapshot snapshot = 16;
    backup_service.MetaBackupManifestId meta_backup_manifest_id = 17;
    SystemParams system_params = 19;
    hummock.WriteLimits hummock_write_limits = 20;
    RelationGroup relation_group = 21;
    catalog.Connection connection = 22;
    hummock.HummockVersionStats hummock_stats = 24;
    Recovery recovery = 25;
    FragmentWorkerSlotMapping streaming_worker_slot_mapping = 27;
    FragmentWorkerSlotMappings serving_worker_slot_mappings = 28;
    catalog.Secret secret = 29;
  }
  reserved 12;
  reserved "parallel_unit_mapping";
  reserved 23;
  reserved "serving_parallel_unit_mappings";
}

service NotificationService {
  rpc Subscribe(SubscribeRequest) returns (stream SubscribeResponse);
}

message GetClusterInfoRequest {}

message GetClusterInfoResponse {
  repeated common.WorkerNode worker_nodes = 1;
  repeated TableFragments table_fragments = 2;
  // `Source` and `SourceBackfill` are handled together here.
  map<uint32, source.ConnectorSplits> actor_splits = 3;
  map<uint32, catalog.Source> source_infos = 4;
  uint64 revision = 5;
}

// For each fragment that needs to be rescheduled, there will be a WorkerReschedule,
// indicating on which workers the actors of this fragment need to be changed and by how many.
message WorkerReschedule {
  // worker_id -> actor_diff
  map<uint32, int32> worker_actor_diff = 1;
}

message RescheduleRequest {
  reserved "reschedules";
  reserved 1;
  uint64 revision = 2;
  bool resolve_no_shuffle_upstream = 3;
  map<uint32, WorkerReschedule> worker_reschedules = 4;
}

message RescheduleResponse {
  bool success = 1;
  uint64 revision = 2;
}

message TableParallelism {
  message FixedParallelism {
    uint32 parallelism = 1;
  }

  // deprecated, treated as AdaptiveParallelism
  message AutoParallelism {}

  message AdaptiveParallelism {}

  message CustomParallelism {}

  oneof parallelism {
    FixedParallelism fixed = 1;
    AutoParallelism auto = 2;
    CustomParallelism custom = 3;
    AdaptiveParallelism adaptive = 4;
  }
}

// Changes a streaming job in place by overwriting its node_label.
// This may cause the re-scheduling of the streaming job actors.
message UpdateStreamingJobNodeLabelsRequest {
  // Id of the materialized view, table, or sink which we want to update
  uint32 id = 1;

  // replace the node_label of the streaming job with a given id with below value
  string node_label = 2;
}

// We do not need to add an explicit status field here, we can just use the RPC status
message UpdateStreamingJobNodeLabelsResponse {}

message GetServerlessStreamingJobsStatusRequest {}

// Descriptions of MVs and sinks
message GetServerlessStreamingJobsStatusResponse {
  message Status {
    uint32 table_id = 1;
    string node_label = 2;
    bool backfill_done = 3;
  }

  repeated Status streaming_job_statuses = 1;
}

// This is used by `risectl`
service ScaleService {
  rpc GetClusterInfo(GetClusterInfoRequest) returns (GetClusterInfoResponse);
  rpc Reschedule(RescheduleRequest) returns (RescheduleResponse);
  rpc UpdateStreamingJobNodeLabels(UpdateStreamingJobNodeLabelsRequest) returns (UpdateStreamingJobNodeLabelsResponse);
  rpc GetServerlessStreamingJobsStatus(GetServerlessStreamingJobsStatusRequest) returns (GetServerlessStreamingJobsStatusResponse);
}

message MembersRequest {}

message MetaMember {
  common.HostAddress address = 1;
  bool is_leader = 2;
}

message MembersResponse {
  repeated MetaMember members = 1;
}

service MetaMemberService {
  rpc Members(MembersRequest) returns (MembersResponse);
}

// The schema for persisted system parameters.
// Note on backward compatibility:
// - Do not remove deprecated fields. Mark them as deprecated instead.
// - Do not rename existing fields, since each field is stored separately in the meta store with the field name as the key.
// - To modify (rename, change the type or semantic of) a field, introduce a new field suffixed by the version.
message SystemParams {
  optional uint32 barrier_interval_ms = 1;
  optional uint64 checkpoint_frequency = 2;
  optional uint32 sstable_size_mb = 3;
  optional uint32 block_size_kb = 4;
  optional double bloom_false_positive = 5;
  optional string state_store = 6;
  optional string data_directory = 7;
  optional string backup_storage_url = 8;
  optional string backup_storage_directory = 9;
  // Deprecated. Use config file instead.
  optional bool telemetry_enabled = 10 [deprecated = true];
  optional uint32 parallel_compact_size_mb = 11;
  optional uint32 max_concurrent_creating_streaming_jobs = 12;
  optional bool pause_on_next_bootstrap = 13;
  optional string wasm_storage_url = 14 [deprecated = true];
  optional bool enable_tracing = 15;
  optional bool use_new_object_prefix_strategy = 16;
  optional string license_key = 17;
  optional uint64 time_travel_retention_ms = 18;
}

message GetSystemParamsRequest {}

message GetSystemParamsResponse {
  SystemParams params = 1;
}

message SetSystemParamRequest {
  string param = 1;
  // None means set to default value.
  optional string value = 2;
}

message SetSystemParamResponse {
  optional SystemParams params = 1;
}

service SystemParamsService {
  rpc GetSystemParams(GetSystemParamsRequest) returns (GetSystemParamsResponse);
  rpc SetSystemParam(SetSystemParamRequest) returns (SetSystemParamResponse);
}

message GetSessionParamsRequest {}

message GetSessionParamsResponse {
  string params = 1;
}

message SetSessionParamRequest {
  string param = 1;
  // None means set to default value.
  optional string value = 2;
}

message SetSessionParamResponse {
  string param = 1;
}

// Used for alter system wide default parameters
service SessionParamService {
  rpc GetSessionParams(GetSessionParamsRequest) returns (GetSessionParamsResponse);
  rpc SetSessionParam(SetSessionParamRequest) returns (SetSessionParamResponse);
}

message GetServingVnodeMappingsRequest {}

message GetServingVnodeMappingsResponse {
  reserved 1;
  reserved "mappings";
  map<uint32, uint32> fragment_to_table = 2;
  repeated FragmentWorkerSlotMapping worker_slot_mappings = 3;
}

service ServingService {
  rpc GetServingVnodeMappings(GetServingVnodeMappingsRequest) returns (GetServingVnodeMappingsResponse);
}

message EventLog {
  message EventMetaNodeStart {
    string advertise_addr = 1;
    string listen_addr = 2;
    string opts = 3;
  }
  message EventCreateStreamJobFail {
    uint32 id = 1;
    string name = 2;
    string definition = 3;
    string error = 4;
  }
  message EventDirtyStreamJobClear {
    uint32 id = 1;
    string name = 2;
    string definition = 3;
    string error = 4;
  }
  message EventBarrierComplete {
    uint64 prev_epoch = 1;
    uint64 cur_epoch = 2;
    double duration_sec = 3;
    string command = 4;
    string barrier_kind = 5;
  }
  message EventInjectBarrierFail {
    uint64 prev_epoch = 1;
    uint64 cur_epoch = 2;
    string error = 3;
  }
  message EventCollectBarrierFail {
    uint64 prev_epoch = 1;
    uint64 cur_epoch = 2;
    string error = 3;
  }
  message EventWorkerNodePanic {
    uint32 worker_id = 1;
    common.WorkerType worker_type = 2;
    common.HostAddress host_addr = 3;
    string panic_info = 4;
  }
  message EventAutoSchemaChangeFail {
    uint32 table_id = 1;
    string table_name = 2;
    string cdc_table_id = 3;
    string upstream_ddl = 4;
  }
  // Event logs identifier, which should be populated by event log service.
  optional string unique_id = 1;
  // Processing time, which should be populated by event log service.
  optional uint64 timestamp = 2;
  oneof event {
    EventCreateStreamJobFail create_stream_job_fail = 3;
    EventDirtyStreamJobClear dirty_stream_job_clear = 4;
    EventMetaNodeStart meta_node_start = 5;
    EventBarrierComplete barrier_complete = 6;
    EventInjectBarrierFail inject_barrier_fail = 7;
    EventCollectBarrierFail collect_barrier_fail = 8;
    EventLog.EventWorkerNodePanic worker_node_panic = 9;
    EventLog.EventAutoSchemaChangeFail auto_schema_change_fail = 10;
  }
}

message ListEventLogRequest {}

message ListEventLogResponse {
  repeated EventLog event_logs = 1;
}

message AddEventLogRequest {
  // A subset of EventLog.event that can be added by non meta node.
  oneof event {
    EventLog.EventWorkerNodePanic worker_node_panic = 1;
  }
}

message AddEventLogResponse {}

service EventLogService {
  rpc ListEventLog(ListEventLogRequest) returns (ListEventLogResponse);
  rpc AddEventLog(AddEventLogRequest) returns (AddEventLogResponse);
}

message ActorIds {
  repeated uint32 ids = 1;
}

message FragmentIdToActorIdMap {
  map<uint32, ActorIds> map = 1;
}

/// Provides all the ids: relation_id, fragment_id, actor_id
/// in an hierarchical format.
/// relation_id -> [fragment_id]
/// fragment_id -> [actor_id]
message RelationIdInfos {
  // relation_id -> FragmentIdToActorIdMap
  map<uint32, FragmentIdToActorIdMap> map = 1;
}

message FragmentVertexToRelationMap {
  // fragment_id -> relation_id
  map<uint32, uint32> in_map = 1;
  map<uint32, uint32> out_map = 2;
}

message ActorCountPerParallelism {
  message WorkerActorCount {
    uint64 actor_count = 1;
    uint64 parallelism = 2;
  }
  map<uint32, WorkerActorCount> worker_id_to_actor_count = 1;
  uint64 hard_limit = 2;
  uint64 soft_limit = 3;
}

message ClusterLimit {
  oneof limit {
    ActorCountPerParallelism actor_count = 1;
    // TODO: limit DDL using compaction pending bytes
  }
}

message GetClusterLimitsRequest {}

message GetClusterLimitsResponse {
  repeated ClusterLimit active_limits = 1;
}

service ClusterLimitService {
  rpc GetClusterLimits(GetClusterLimitsRequest) returns (GetClusterLimitsResponse);
}
