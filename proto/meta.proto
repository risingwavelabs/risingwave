syntax = "proto3";

package meta;

import "backup_service.proto";
import "catalog.proto";
import "common.proto";
import "hummock.proto";
import "source.proto";
import "stream_plan.proto";
import "user.proto";

option java_package = "com.risingwave.proto";
option optimize_for = SPEED;

message GetTelemetryInfoRequest {}

message TelemetryInfoResponse {
  optional string tracking_id = 1;
}

service TelemetryInfoService {
  // Request telemetry info from meta node
  rpc GetTelemetryInfo(GetTelemetryInfoRequest) returns (TelemetryInfoResponse);
}

message HeartbeatRequest {
  message ExtraInfo {
    oneof info {
      uint64 hummock_gc_watermark = 1;
    }
  }
  uint32 node_id = 1;
  // Lightweight info piggybacked by heartbeat request.
  repeated ExtraInfo info = 2;
}

message HeartbeatResponse {
  common.Status status = 1;
}

service HeartbeatService {
  rpc Heartbeat(HeartbeatRequest) returns (HeartbeatResponse);
}

// Fragments of a Streaming Job
message TableFragments {
  // The state of the fragments of this table
  enum State {
    UNSPECIFIED = 0;
    // The streaming job is initial.
    INITIAL = 1;
    // The streaming job is creating.
    CREATING = 2;
    // The streaming job has been created.
    CREATED = 3;
  }
  // Runtime information of an actor
  message ActorStatus {
    // Current state of actor
    enum ActorState {
      UNSPECIFIED = 0;
      // Initial state after creation
      INACTIVE = 1;
      // Running normally
      RUNNING = 2;
    }
    // Current on which parallel unit
    common.ParallelUnit parallel_unit = 1;
    // Current state
    ActorState state = 2;
  }
  message Fragment {
    enum FragmentDistributionType {
      UNSPECIFIED = 0;
      SINGLE = 1;
      HASH = 2;
    }
    uint32 fragment_id = 1;
    // Bitwise-OR of FragmentTypeFlags
    uint32 fragment_type_mask = 2;
    FragmentDistributionType distribution_type = 3;
    repeated stream_plan.StreamActor actors = 4;
    // Vnode mapping (which should be set in upstream dispatcher) of the fragment.
    // This field is always set to `Some`. For singleton, the parallel unit for all vnodes will be the same.
    common.ParallelUnitMapping vnode_mapping = 5;
    repeated uint32 state_table_ids = 6;
    // Note that this can be derived backwards from the upstream actors of the Actor held by the Fragment,
    // but in some scenarios (e.g. Scaling) it will lead to a lot of duplicate code,
    // so we pre-generate and store it here, this member will only be initialized when creating the Fragment
    // and modified when creating the mv-on-mv
    repeated uint32 upstream_fragment_ids = 7;
  }
  uint32 table_id = 1;
  State state = 2;
  map<uint32, Fragment> fragments = 3;
  map<uint32, ActorStatus> actor_status = 4;
  map<uint32, source.ConnectorSplits> actor_splits = 5;

  stream_plan.StreamEnvironment env = 6;
}

/// Parallel unit mapping with fragment id, used for notification.
message FragmentParallelUnitMapping {
  uint32 fragment_id = 1;
  common.ParallelUnitMapping mapping = 2;
}

message FragmentParallelUnitMappings {
  repeated FragmentParallelUnitMapping mappings = 1;
}

// TODO: remove this when dashboard refactored.
message ActorLocation {
  common.WorkerNode node = 1;
  repeated stream_plan.StreamActor actors = 2;
}

message MigrationPlan {
  // map<parallel_unit_id, parallel_unit>, the plan indicates that the actors will be migrated from old parallel unit to the new one.
  map<uint32, common.ParallelUnit> parallel_unit_migration_plan = 1;
}

message FlushRequest {
  bool checkpoint = 1;
}

message FlushResponse {
  common.Status status = 1;
  hummock.HummockSnapshot snapshot = 2;
}

message CreatingJobInfo {
  uint32 database_id = 1;
  uint32 schema_id = 2;
  string name = 3;
}

message CancelCreatingJobsRequest {
  repeated CreatingJobInfo infos = 1;
}

message CancelCreatingJobsResponse {
  common.Status status = 1;
}

message ListTableFragmentsRequest {
  repeated uint32 table_ids = 1;
}

message ListTableFragmentsResponse {
  message ActorInfo {
    uint32 id = 1;
    stream_plan.StreamNode node = 2;
    repeated stream_plan.Dispatcher dispatcher = 3;
  }
  message FragmentInfo {
    uint32 id = 1;
    repeated ActorInfo actors = 4;
  }
  message TableFragmentInfo {
    repeated FragmentInfo fragments = 1;
    stream_plan.StreamEnvironment env = 2;
  }
  map<uint32, TableFragmentInfo> table_fragments = 1;
}

service StreamManagerService {
  rpc Flush(FlushRequest) returns (FlushResponse);
  rpc CancelCreatingJobs(CancelCreatingJobsRequest) returns (CancelCreatingJobsResponse);
  rpc ListTableFragments(ListTableFragmentsRequest) returns (ListTableFragmentsResponse);
}

// Below for cluster service.

message AddWorkerNodeRequest {
  message Property {
    uint64 worker_node_parallelism = 1;
    bool is_streaming = 2;
    bool is_serving = 3;
    bool is_unschedulable = 4;
  }
  common.WorkerType worker_type = 1;
  common.HostAddress host = 2;
  reserved 3;
  Property property = 4;
}

message AddWorkerNodeResponse {
  reserved 3;
  reserved "system_params";
  common.Status status = 1;
  common.WorkerNode node = 2;
}

message ActivateWorkerNodeRequest {
  common.HostAddress host = 1;
}

message ActivateWorkerNodeResponse {
  common.Status status = 1;
}

message DeleteWorkerNodeRequest {
  common.HostAddress host = 1;
}

message DeleteWorkerNodeResponse {
  common.Status status = 1;
}

// Mark CN as schedulable or as unschedulable
message UpdateWorkerNodeSchedulabilityRequest {
  uint32 worker_id = 1;
  bool set_is_unschedulable = 2;
}

message UpdateWorkerNodeSchedulabilityResponse {
  common.Status status = 1;
}

message ListAllNodesRequest {
  common.WorkerType worker_type = 1;
  // Whether to include nodes still starting
  bool include_starting_nodes = 2;
}

message ListAllNodesResponse {
  common.Status status = 1;
  repeated common.WorkerNode nodes = 2;
}

service ClusterService {
  rpc AddWorkerNode(AddWorkerNodeRequest) returns (AddWorkerNodeResponse);
  rpc ActivateWorkerNode(ActivateWorkerNodeRequest) returns (ActivateWorkerNodeResponse);
  rpc DeleteWorkerNode(DeleteWorkerNodeRequest) returns (DeleteWorkerNodeResponse);
  rpc UpdateWorkerNodeSchedulability(UpdateWorkerNodeSchedulabilityRequest) returns (UpdateWorkerNodeSchedulabilityResponse);
  rpc ListAllNodes(ListAllNodesRequest) returns (ListAllNodesResponse);
}

enum SubscribeType {
  UNSPECIFIED = 0;
  FRONTEND = 1;
  HUMMOCK = 2;
  COMPACTOR = 3;
  COMPUTE = 4;
}

// Below for notification service.
message SubscribeRequest {
  SubscribeType subscribe_type = 1;
  common.HostAddress host = 2;
  uint32 worker_id = 3;
}

message MetaSnapshot {
  message SnapshotVersion {
    uint64 catalog_version = 1;
    uint64 parallel_unit_mapping_version = 2;
    uint64 worker_node_version = 3;
  }
  repeated catalog.Database databases = 1;
  repeated catalog.Schema schemas = 2;
  repeated catalog.Source sources = 3;
  repeated catalog.Sink sinks = 4;
  repeated catalog.Table tables = 5;
  repeated catalog.Index indexes = 6;
  repeated catalog.View views = 7;
  repeated catalog.Function functions = 15;
  repeated catalog.Connection connections = 17;
  repeated user.UserInfo users = 8;
  // for streaming
  repeated FragmentParallelUnitMapping parallel_unit_mappings = 9;
  repeated common.WorkerNode nodes = 10;
  hummock.HummockSnapshot hummock_snapshot = 11;
  hummock.HummockVersion hummock_version = 12;
  backup_service.MetaBackupManifestId meta_backup_manifest_id = 14;
  hummock.WriteLimits hummock_write_limits = 16;
  // for serving
  repeated FragmentParallelUnitMapping serving_parallel_unit_mappings = 18;

  SnapshotVersion version = 13;
}

message Relation {
  oneof relation_info {
    catalog.Table table = 1;
    catalog.Source source = 2;
    catalog.Sink sink = 3;
    catalog.Index index = 4;
    catalog.View view = 5;
  }
}

message RelationGroup {
  repeated Relation relations = 1;
}

message SubscribeResponse {
  enum Operation {
    UNSPECIFIED = 0;
    ADD = 1;
    DELETE = 2;
    UPDATE = 3;
    SNAPSHOT = 4;
  }
  common.Status status = 1;
  Operation operation = 2;
  uint64 version = 3;
  oneof info {
    catalog.Database database = 4;
    catalog.Schema schema = 5;
    catalog.Function function = 6;
    user.UserInfo user = 11;
    // for streaming
    FragmentParallelUnitMapping parallel_unit_mapping = 12;
    common.WorkerNode node = 13;
    hummock.HummockSnapshot hummock_snapshot = 14;
    hummock.HummockVersionDeltas hummock_version_deltas = 15;
    MetaSnapshot snapshot = 16;
    backup_service.MetaBackupManifestId meta_backup_manifest_id = 17;
    SystemParams system_params = 19;
    hummock.WriteLimits hummock_write_limits = 20;
    RelationGroup relation_group = 21;
    catalog.Connection connection = 22;
    FragmentParallelUnitMappings serving_parallel_unit_mappings = 23;
  }
}

service NotificationService {
  rpc Subscribe(SubscribeRequest) returns (stream SubscribeResponse);
}

message PauseRequest {}

message PauseResponse {}

message ResumeRequest {}

message ResumeResponse {}

message GetClusterInfoRequest {}

message GetClusterInfoResponse {
  repeated common.WorkerNode worker_nodes = 1;
  repeated TableFragments table_fragments = 2;
  map<uint32, source.ConnectorSplits> actor_splits = 3;
  map<uint32, catalog.Source> source_infos = 4;
  uint64 revision = 5;
}

message RescheduleRequest {
  message Reschedule {
    repeated uint32 added_parallel_units = 1;
    repeated uint32 removed_parallel_units = 2;
  }
  // reschedule plan for each fragment
  map<uint32, Reschedule> reschedules = 1;
  uint64 revision = 2;
}

message RescheduleResponse {
  bool success = 1;
  uint64 revision = 2;
}

service ScaleService {
  // TODO(Kexiang): delete them when config change interface is finished
  rpc Pause(PauseRequest) returns (PauseResponse);
  rpc Resume(ResumeRequest) returns (ResumeResponse);
  rpc GetClusterInfo(GetClusterInfoRequest) returns (GetClusterInfoResponse);
  rpc Reschedule(RescheduleRequest) returns (RescheduleResponse);
}

message MembersRequest {}

message MetaMember {
  common.HostAddress address = 1;
  bool is_leader = 2;
}

message MembersResponse {
  repeated MetaMember members = 1;
}

service MetaMemberService {
  rpc Members(MembersRequest) returns (MembersResponse);
}

// The schema for persisted system parameters.
// Note on backward compatibility:
// - Do not remove deprecated fields. Mark them as deprecated both after the field definition and in `system_params/mod.rs` instead.
// - Do not rename existing fields, since each field is stored separately in the meta store with the field name as the key.
// - To modify (rename, change the type or semantic of) a field, introduce a new field suffixed by the version.
message SystemParams {
  optional uint32 barrier_interval_ms = 1;
  optional uint64 checkpoint_frequency = 2;
  optional uint32 sstable_size_mb = 3;
  optional uint32 block_size_kb = 4;
  optional double bloom_false_positive = 5;
  optional string state_store = 6;
  optional string data_directory = 7;
  optional string backup_storage_url = 8;
  optional string backup_storage_directory = 9;
  optional bool telemetry_enabled = 10;
}

message GetSystemParamsRequest {}

message GetSystemParamsResponse {
  SystemParams params = 1;
}

message SetSystemParamRequest {
  string param = 1;
  // None means set to default value.
  optional string value = 2;
}

message SetSystemParamResponse {}

service SystemParamsService {
  rpc GetSystemParams(GetSystemParamsRequest) returns (GetSystemParamsResponse);
  rpc SetSystemParam(SetSystemParamRequest) returns (SetSystemParamResponse);
}

message GetServingVnodeMappingsRequest {}

message GetServingVnodeMappingsResponse {
  repeated FragmentParallelUnitMapping mappings = 1;
  map<uint32, uint32> fragment_to_table = 2;
}

service ServingService {
  rpc GetServingVnodeMappings(GetServingVnodeMappingsRequest) returns (GetServingVnodeMappingsResponse);
}
