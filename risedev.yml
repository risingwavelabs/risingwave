# The schema for RiseDev configuration files is defined under `src/risedevtool/schemas`.
#
# You can add the following section to `.vscode/settings.json` to get hover support in VS Code:
#
# ```
#     "yaml.schemas": {
#         "src/risedevtool/schemas/risedev.json": "risedev.yml",
#         "src/risedevtool/schemas/risedev-profiles.user.json": "risedev-profiles.user.yml"
#     }
# ```

profile:
  #################################################
  ### Configuration profiles used by developers ###
  #################################################

  # The default configuration will start 1 compute node, 1 meta node and 1 frontend.
  default:
    # Specify a configuration file to override the default settings
    # config-path: src/config/example.toml
    steps:
      # If you want to use the local s3 storage, enable the following line
      # - use: minio

      # If you want to use aws-s3, configure AK and SK in env var and enable the following lines:
      # - use: aws-s3
      #   bucket: test-bucket
      # If you want to use other s3 compatible object store, open this flag:
      #   s3-compatible: false

      # If you want to create CDC source table, uncomment the following line
      # - use: connector-node

      # if you want to enable etcd backend, uncomment the following lines.
      # - use: etcd
      #   unsafe-no-fsync: true
      - use: meta-node
      - use: compute-node
      - use: frontend

      # If you want to enable compactor, uncomment the following line, and enable either minio or aws-s3 as well.
      # - use: compactor

      # If you want to enable metrics, uncomment those two lines.
      # - use: prometheus
      # - use: grafana

      # If you want to enable tracing, uncomment the following line.
      # - use: jaeger

      # If you want to create source from Kafka, uncomment the following lines
      # Note that kafka depends on zookeeper, so zookeeper must be started beforehand.
      # - use: zookeeper
      #   persist-data: true
      # - use: kafka
      #   persist-data: true

  default-v6:
    steps:
      - use: meta-node
        address: "[::1]"
        listen-address: "[::]"
      - use: compute-node
        address: "[::1]"
        listen-address: "[::]"
      - use: frontend
        address: "[::1]"
        listen-address: "[::]"

  # The minimum config to use with risectl.
  for-ctl:
    steps:
      - use: minio
      - use: meta-node
      - use: compute-node
      - use: frontend
      - use: compactor

  # `dev-compute-node` have the same settings as default except the the compute node will be started by user.
  dev-compute-node:
    steps:
      - use: meta-node
      - use: compute-node
        user-managed: true
      - use: frontend

  dev-frontend:
    steps:
      - use: meta-node
      - use: compute-node
      - use: frontend
        user-managed: true

  dev-meta:
    steps:
      - use: meta-node
        user-managed: true
      - use: compute-node
      - use: frontend

  full:
    steps:
      - use: minio
      - use: etcd
      - use: meta-node
      - use: compute-node
      - use: frontend
      - use: compactor
      - use: prometheus
      - use: grafana
      - use: zookeeper
        persist-data: true
      - use: kafka
        persist-data: true

  hdfs:
    steps:
      # - use: etcd
      - use: meta-node
      - use: compute-node
      - use: frontend
      # If you want to use hdfs as storage backend, configure hdfs namenode and root path:
      - use: opendal
        engine: hdfs
        namenode: "127.0.0.1:9000"
        root: risingwave
      - use: compactor
      # - use: prometheus
      # - use: grafana
  webhdfs:
    steps:
      # - use: etcd
      - use: meta-node
      - use: compute-node
      - use: frontend
      # If you want to use webhdfs as storage backend, configure hdfs namenode and root path:
      - use: opendal
        engine: webhdfs
        namenode: "127.0.0.1:9870"
        root: risingwave
      - use: compactor
      # - use: prometheus
      # - use: grafana

  gcs:
    steps:
      # - use: etcd
      - use: meta-node
      - use: compute-node
      - use: frontend
      # If you want to use google cloud stoage as storage backend, configure bucket name and root path:
      - use: opendal
        engine: gcs
        bucket: bucket-name
        root: risingwave
      - use: compactor
      # - use: prometheus
      # - use: grafana

  oss:
    steps:
      # - use: etcd
      - use: meta-node
      - use: compute-node
      - use: frontend
      # If you want to use oss as storage backend, configure bucket name and root path:
      - use: opendal
        engine: oss
        bucket: test-bucket
        root: risingwave
      - use: compactor
      # - use: prometheus
      # - use: grafana

  azblob:
    steps:
      # - use: etcd
      - use: meta-node
      - use: compute-node
      - use: frontend
      # If you want to use azblob as storage backend, configure bucket(container) name and root path:
      - use: opendal
        engine: azblob
        bucket: test-bucket
        root: risingwave
      - use: compactor
      # - use: prometheus
      # - use: grafana

  full-benchmark:
    steps:
      - use: minio
      - use: etcd
      - use: meta-node
      - use: compute-node
      - use: frontend
      - use: compactor
      - use: prometheus
        remote-write: true
        remote-write-region: "ap-southeast-1"
        remote-write-url: "https://aps-workspaces.ap-southeast-1.amazonaws.com/workspaces/ws-f3841dad-6a5c-420f-8f62-8f66487f512a/api/v1/remote_write"
      - use: grafana
      - use: zookeeper
        persist-data: true
      - use: kafka
        persist-data: true

  3etcd-3meta:
    steps:
      - use: minio
      - use: etcd
        unsafe-no-fsync: true
        port: 2388
        peer-port: 2389
        exporter-port: 2379
      - use: etcd
        unsafe-no-fsync: true
        port: 12388
        peer-port: 12389
        exporter-port: 12379
      - use: etcd
        unsafe-no-fsync: true
        port: 22388
        peer-port: 22389
        exporter-port: 22379
      - use: meta-node
        port: 5690
        dashboard-port: 5691
        exporter-port: 1250
      - use: meta-node
        port: 15690
        dashboard-port: 15691
        exporter-port: 11250
      - use: meta-node
        port: 25690
        dashboard-port: 25691
        exporter-port: 21250
      - use: compactor

  3etcd-3meta-1cn-1fe:
    steps:
      - use: minio
      - use: etcd
        unsafe-no-fsync: true
        port: 2388
        peer-port: 2389
        exporter-port: 2379
      - use: etcd
        unsafe-no-fsync: true
        port: 12388
        peer-port: 12389
        exporter-port: 12379
      - use: etcd
        unsafe-no-fsync: true
        port: 22388
        peer-port: 22389
        exporter-port: 22379
      - use: meta-node
        port: 5690
        dashboard-port: 5691
        exporter-port: 1250
      - use: meta-node
        port: 15690
        dashboard-port: 15691
        exporter-port: 11250
      - use: meta-node
        port: 25690
        dashboard-port: 25691
        exporter-port: 21250
      - use: compactor
      - use: compute-node
      - use: frontend

  java-binding-demo:
    steps:
      - use: minio
        address: "127.0.0.1"
        port: 9301
        root-user: hummockadmin
        root-password: hummockadmin
        hummock-bucket: hummock001
      - use: meta-node
        address: "127.0.0.1"
        port: 5690
      - use: compute-node
      - use: frontend
      - use: compactor

  ci-gen-cpu-flamegraph:
    steps:
      # NOTE(kwannoel): We do not use aws-s3 here, to avoid
      # contention over s3 bucket when multiple benchmarks at run at once.
      - use: minio
      - use: etcd
      - use: meta-node
      - use: compute-node
        parallelism: 8
      - use: frontend
      - use: compactor
      - use: prometheus
      - use: grafana
      # Do not use kafka and zookeeper here, we will spawn it separately,
      # so we don't have to re-generate data each time.
      # - use: zookeeper
      #   persist-data: true
      # RW will still be ale to talk to it.
      # - use: kafka
      #   port: 9092
      #   persist-data: true

  ######################################
  ### Configurations used in Compose ###
  ######################################

  compose:
    steps:
      - use: minio
        id: minio-0
        address: ${id}
        listen-address: "0.0.0.0"
        console-address: "0.0.0.0"

      - use: meta-node
        # Id must starts with `meta-node`, therefore to be picked up by other
        # components.
        id: meta-node-0

        # Advertise address can be `id`, so as to use docker's DNS. If running
        # in host network mode, we should use IP directly in this field.
        address: ${id}

        listen-address: "0.0.0.0"

      - use: compute-node
        id: compute-node-0
        listen-address: "0.0.0.0"
        address: ${id}

      - use: frontend
        id: frontend-node-0
        listen-address: "0.0.0.0"
        address: ${id}

      - use: compactor
        id: compactor-0
        listen-address: "0.0.0.0"
        address: ${id}

      - use: redpanda

      - use: prometheus
        id: prometheus-0
        listen-address: "0.0.0.0"
        address: ${id}

      - use: grafana
        listen-address: "0.0.0.0"
        address: ${id}
        id: grafana-0

      - use: etcd
        listen-address: "0.0.0.0"
        address: ${id}
        id: etcd-0

  # special config for deployment, see related PR for more information
  compose-3node-deploy:
    steps:
      # - use: minio
      #   id: minio-0
      #   address: ${dns-host:rw-source-0}
      #   listen-address: "0.0.0.0"
      #   console-address: "0.0.0.0"

      - use: aws-s3
        bucket: ${terraform:s3-bucket}

      # Not enabled by default as all previous benchmarks are not done with etcd.
      # Also we currently only support node-level docker volume tear down.
      # - use: etcd
      #   listen-address: "0.0.0.0"
      #   address: ${dns-host:rw-meta-0}
      #   id: etcd-0

      - use: meta-node
        # Id must starts with `meta-node`, therefore to be picked up by other
        # components.
        id: meta-node-0

        # Advertise address can be `id`, so as to use docker's DNS. If running
        # in host network mode, we should use IP directly in this field.
        address: ${dns-host:rw-meta-0}
        listen-address: "0.0.0.0"

      - use: compute-node
        id: compute-node-0
        listen-address: "0.0.0.0"
        address: ${dns-host:rw-compute-0}
        async-stack-trace: verbose
        enable-tiered-cache: true

      - use: compute-node
        id: compute-node-1
        listen-address: "0.0.0.0"
        address: ${dns-host:rw-compute-1}
        async-stack-trace: verbose
        enable-tiered-cache: true

      - use: compute-node
        id: compute-node-2
        listen-address: "0.0.0.0"
        address: ${dns-host:rw-compute-2}
        async-stack-trace: verbose
        enable-tiered-cache: true

      - use: frontend
        id: frontend-node-0
        listen-address: "0.0.0.0"
        address: ${dns-host:rw-meta-0}

      - use: compactor
        id: compactor-0
        listen-address: "0.0.0.0"
        address: ${dns-host:rw-source-0}
        max-concurrent-task-number: 15
        compaction-worker-threads-number: 15

      - use: redpanda
        address: ${dns-host:rw-source-0}

      - use: prometheus
        id: prometheus-0
        listen-address: "0.0.0.0"
        address: ${dns-host:rw-meta-0}

      - use: grafana
        listen-address: "0.0.0.0"
        address: ${dns-host:rw-meta-0}
        id: grafana-0

  #################################
  ### Configurations used on CI ###
  #################################

  ci-1cn-1fe:
    config-path: src/config/ci.toml
    steps:
      - use: minio
      - use: etcd
        unsafe-no-fsync: true
      - use: meta-node
      - use: compute-node
        enable-tiered-cache: true
      - use: frontend
      - use: compactor

  ci-3cn-1fe:
    config-path: src/config/ci.toml
    steps:
      - use: minio
      - use: etcd
        unsafe-no-fsync: true
      - use: meta-node
      - use: compute-node
        port: 5687
        exporter-port: 1222
        enable-tiered-cache: true
      - use: compute-node
        port: 5688
        exporter-port: 1223
        enable-tiered-cache: true
      - use: compute-node
        port: 5689
        exporter-port: 1224
        enable-tiered-cache: true
      - use: frontend
      - use: compactor

  ci-3cn-3fe:
    config-path: src/config/ci.toml
    steps:
      - use: minio
      - use: etcd
        unsafe-no-fsync: true
      - use: meta-node
      - use: compute-node
        port: 5687
        exporter-port: 1222
        enable-tiered-cache: true
      - use: compute-node
        port: 5688
        exporter-port: 1223
        enable-tiered-cache: true
      - use: compute-node
        port: 5689
        exporter-port: 1224
        enable-tiered-cache: true
      - use: frontend
        port: 4565
        exporter-port: 2222
        health-check-port: 6786
      - use: frontend
        port: 4566
        exporter-port: 2223
        health-check-port: 6787
      - use: frontend
        port: 4567
        exporter-port: 2224
        health-check-port: 6788
      - use: compactor

  ci-3cn-3fe-in-memory:
    config-path: src/config/ci.toml
    steps:
      - use: etcd
        unsafe-no-fsync: true
      - use: meta-node
        enable-in-memory-kv-state-backend: true
      - use: compute-node
        port: 5687
        exporter-port: 1222
      - use: compute-node
        port: 5688
        exporter-port: 1223
      - use: compute-node
        port: 5689
        exporter-port: 1224
      - use: frontend
        port: 4565
        exporter-port: 2222
        health-check-port: 6786
      - use: frontend
        port: 4566
        exporter-port: 2223
        health-check-port: 6787
      - use: frontend
        port: 4567
        exporter-port: 2224
        health-check-port: 6788

  ci-3cn-3fe-opendal-fs-backend:
    config-path: src/config/ci.toml
    steps:
      - use: etcd
        unsafe-no-fsync: true
      - use: meta-node
      - use: opendal
        engine: fs
        bucket: ""
        root: /tmp/rw_ci
      - use: compute-node
        port: 5687
        exporter-port: 1222
      - use: compute-node
        port: 5688
        exporter-port: 1223
      - use: compute-node
        port: 5689
        exporter-port: 1224
      - use: frontend
        port: 4565
        exporter-port: 2222
        health-check-port: 6786
      - use: frontend
        port: 4566
        exporter-port: 2223
        health-check-port: 6787
      - use: frontend
        port: 4567
        exporter-port: 2224
        health-check-port: 6788
      - use: compactor

  ci-3streaming-2serving-3fe:
    config-path: src/config/ci.toml
    steps:
      - use: minio
      - use: etcd
        unsafe-no-fsync: true
      - use: meta-node
      - use: compute-node
        port: 5687
        exporter-port: 1222
        enable-tiered-cache: true
        role: streaming
        parallelism: 4
      - use: compute-node
        port: 5688
        exporter-port: 1223
        enable-tiered-cache: true
        role: streaming
        parallelism: 4
      - use: compute-node
        port: 5689
        exporter-port: 1224
        enable-tiered-cache: true
        role: streaming
        parallelism: 4
      - use: compute-node
        port: 5685
        exporter-port: 1225
        enable-tiered-cache: true
        role: serving
        parallelism: 6
      - use: compute-node
        port: 5686
        exporter-port: 1226
        enable-tiered-cache: true
        role: serving
        parallelism: 6
      - use: frontend
        port: 4565
        exporter-port: 2222
        health-check-port: 6786
      - use: frontend
        port: 4566
        exporter-port: 2223
        health-check-port: 6787
      - use: frontend
        port: 4567
        exporter-port: 2224
        health-check-port: 6788
      - use: compactor

  ci-pubsub:
    config-path: src/config/ci.toml
    steps:
      - use: minio
      - use: etcd
        unsafe-no-fsync: true
      - use: meta-node
      - use: compute-node
        enable-tiered-cache: true
      - use: frontend
      - use: compactor
      - use: pubsub
        persist-data: true

  ci-kafka:
    config-path: src/config/ci.toml
    steps:
      - use: minio
      - use: etcd
        unsafe-no-fsync: true
      - use: meta-node
      - use: compute-node
        enable-tiered-cache: true
      - use: frontend
      - use: compactor
      - use: zookeeper
        persist-data: true
      - use: kafka
        persist-data: true

  ci-kafka-plus-pubsub:
    config-path: src/config/ci.toml
    steps:
      - use: minio
      - use: etcd
        unsafe-no-fsync: true
      - use: meta-node
      - use: compute-node
        enable-tiered-cache: true
      - use: frontend
      - use: compactor
      - use: zookeeper
        persist-data: true
      - use: kafka
        persist-data: true
      - use: pubsub
        persist-data: true

  ci-redis:
    config-path: src/config/ci.toml
    steps:
      - use: minio
      - use: etcd
        unsafe-no-fsync: true
      - use: meta-node
      - use: compute-node
        enable-tiered-cache: true
      - use: frontend
      - use: compactor
      - use: redis

  ci-compaction-test:
    config-path: src/config/ci-compaction-test.toml
    steps:
      - use: minio
      - use: etcd
        unsafe-no-fsync: true
      - use: meta-node
      - use: compute-node
        enable-tiered-cache: true
        total-memory-bytes: 17179869184
      - use: frontend
      - use: compactor

  ci-1cn-1fe-with-recovery:
    config-path: src/config/ci-recovery.toml
    steps:
      - use: minio
      - use: etcd
      - use: meta-node
      - use: compute-node
        enable-tiered-cache: true
      - use: frontend
      - use: compactor

  ci-meta-backup-test:
    config-path: src/config/ci-meta-backup-test.toml
    steps:
      - use: etcd
      - use: minio
      - use: meta-node
      - use: compute-node
      - use: frontend
      - use: compactor

  ci-meta-backup-test-restore:
    config-path: src/config/ci-meta-backup-test.toml
    steps:
      - use: etcd
      - use: minio

  ci-delete-range-test:
    config-path: src/config/ci-delete-range-test.toml
    steps:
      - use: minio

  ci-iceberg-test:
    config-path: src/config/ci-iceberg-test.toml
    steps:
      - use: minio
      - use: meta-node
      - use: compute-node
        enable-tiered-cache: true
      - use: frontend
      - use: compactor

compose:
  risingwave: "ghcr.io/risingwavelabs/risingwave:latest"
  prometheus: "prom/prometheus:latest"
  minio: "quay.io/minio/minio:latest"
  redpanda: "docker.vectorized.io/vectorized/redpanda:latest"
  grafana: "grafana/grafana-oss:latest"
  etcd: "quay.io/coreos/etcd:latest"

# The `use` field specified in the above `risedev` section will refer to the templates below.
template:
  minio:
    # Advertise address of MinIO s3 endpoint
    address: "127.0.0.1"

    # Advertise port of MinIO s3 endpoint
    port: 9301

    # Listen address of MinIO endpoint
    listen-address: ${address}

    # Console address of MinIO s3 endpoint
    console-address: "127.0.0.1"

    # Console port of MinIO s3 endpoint
    console-port: 9400

    # Root username (can be used to login to MinIO console)
    root-user: hummockadmin

    # Root password (can be used to login to MinIO console)
    root-password: hummockadmin

    # Bucket name to store hummock information
    hummock-bucket: hummock001

    # Id of this instance
    id: minio

    # Prometheus nodes used by this MinIO
    provide-prometheus: "prometheus*"

  etcd:
    # Id of this instance
    id: etcd-${port}

    # Advertise address of the single-node etcd.
    address: "127.0.0.1"

    # Listen port of the single-node etcd.
    port: 2388

    # Listen address
    listen-address: ${address}

    # Peer listen port of the single-node etcd.
    peer-port: 2389

    # Prometheus exporter listen port
    exporter-port: 2379

    # Whether to enable fsync (NEVER SET TO TRUE IN PRODUCTION ENVIRONMENT!)
    unsafe-no-fsync: false

    # Other etcd nodes
    provide-etcd: "etcd*"

  compute-node:
    # Compute-node advertise address
    address: "127.0.0.1"

    # Listen address
    listen-address: ${address}

    # Compute-node listen port
    port: 5688

    # Prometheus exporter listen port
    exporter-port: 1222

    # Id of this instance
    id: compute-node-${port}

    # Whether to enable async stack trace for this compute node, `off`, `on`, or `verbose`.
    # Considering the performance, `verbose` mode only effect under `release` profile with `debug_assertions` off.
    async-stack-trace: on

    # If `enable-tiered-cache` is true, hummock will use data directory as file cache.
    enable-tiered-cache: false

    # RPC endpoint for connector node
    connector-rpc-endpoint: "127.0.0.1:50051"

    # Minio instances used by this compute node
    provide-minio: "minio*"

    # OpenDAL storage backend used by this compute node
    provide-opendal: "opendal*"

    # AWS s3 bucket used by this compute node
    provide-aws-s3: "aws-s3*"

    # Meta-nodes used by this compute node
    provide-meta-node: "meta-node*"

    # Jaeger used by this compute node
    provide-jaeger: "jaeger*"

    # If `user-managed` is true, this service will be started by user with the above config
    user-managed: false

    # Total available memory for the compute node in bytes
    total-memory-bytes: 8589934592

    # Parallelism of tasks per compute node
    parallelism: 4

    role: both

  meta-node:
    # Meta-node advertise address
    address: "127.0.0.1"

    # Meta-node listen port
    port: 5690

    # Listen address
    listen-address: ${address}

    # Dashboard listen port
    dashboard-port: 5691

    # Prometheus exporter listen port
    exporter-port: 1250

    # Id of this instance
    id: meta-node-${port}

    # RPC endpoint for connector node colocated with Meta
    connector-rpc-endpoint: "127.0.0.1:50051"

    # If `user-managed` is true, this service will be started by user with the above config
    user-managed: false

    # Etcd backend config
    provide-etcd-backend: "etcd*"

    # Prometheus nodes used by dashboard service
    provide-prometheus: "prometheus*"

    # Sanity check: should use shared storage if there're multiple compute nodes
    provide-compute-node: "compute-node*"

    # Sanity check: should start at lease one compactor if using shared object store
    provide-compactor: "compactor*"

    # Minio instances used by the cluster
    provide-minio: "minio*"

    # OpenDAL storage backend used by the cluster
    provide-opendal: "opendal*"

    # AWS s3 bucket used by the cluster
    provide-aws-s3: "aws-s3*"

    # Whether to enable in-memory pure KV state backend
    enable-in-memory-kv-state-backend: false

  prometheus:
    # Advertise address of Prometheus
    address: "127.0.0.1"

    # Listen port of Prometheus
    port: 9500

    # Listen address
    listen-address: ${address}

    # Id of this instance
    id: prometheus

    # If `remote_write` is true, this Prometheus instance will push metrics to remote instance
    remote-write: false

    # AWS region of remote write
    remote-write-region: ""

    # Remote write url of this instance
    remote-write-url: ""

    # Compute-nodes used by this Prometheus instance
    provide-compute-node: "compute-node*"

    # Meta-nodes used by this Prometheus instance
    provide-meta-node: "meta-node*"

    # Minio instances used by this Prometheus instance
    provide-minio: "minio*"

    # Compactors used by this Prometheus instance
    provide-compactor: "compactor*"

    # Etcd used by this Prometheus instance
    provide-etcd: "etcd*"

    # Redpanda used by this Prometheus instance
    provide-redpanda: "redpanda*"

    # Frontend used by this Prometheus instance
    provide-frontend: "frontend*"

    # Connector-node used by this Prometheus instance
    provide-connector-node: "connector*"

  frontend:
    # Advertise address of frontend
    address: "127.0.0.1"

    # Listen port of frontend
    port: 4566

    # Listen address
    listen-address: ${address}

    # Prometheus exporter listen port
    exporter-port: 2222

    # Health check listen port
    health-check-port: 6786

    # Id of this instance
    id: frontend-${port}

    # Meta-nodes used by this frontend instance
    provide-meta-node: "meta-node*"

    # If `user-managed` is true, this service will be started by user with the above config
    user-managed: false

  compactor:
    # Compactor advertise address
    address: "127.0.0.1"

    # Compactor listen port
    port: 6660

    # Listen address
    listen-address: ${address}

    # Prometheus exporter listen port
    exporter-port: 1260

    # Id of this instance
    id: compactor-${port}

    # Minio instances used by this compactor
    provide-minio: "minio*"

    # Meta-nodes used by this compute node
    provide-meta-node: "meta-node*"

    # If `user-managed` is true, this service will be started by user with the above config
    user-managed: false

    # Hint used by meta node
    max-concurrent-task-number: 15

  connector-node:
    # Connector node advertise address
    address: "127.0.0.1"

    # Connector node listen port
    port: 50051

    # Prometheus exporter listen port
    exporter-port: 50052

    # Id of this instance
    id: connector-${port}

  grafana:
    # Listen address of Grafana
    listen-address: ${address}

    # Advertise address of Grafana
    address: "127.0.0.1"

    # Listen port of Grafana
    port: 3001

    # Id of this instance
    id: grafana

    # Prometheus used by this Grafana instance
    provide-prometheus: "prometheus*"

  jaeger:
    # Id of this instance
    id: jaeger

    # Dashboard listen address of Jaeger
    dashboard-address: "127.0.0.1"

    # Dashboard listen port of Jaeger
    dashboard-port: 16680

    # Jaeger has a lot of ports open, and we don't want to make this config more complex.
    # So we keep the default value of jaeger instead of making it part of RiseDev config.

  # opendal:
  opendal:
    id: opendal

    engine: hdfs

    namenode: 127.0.0.1:9000

    bucket: risingwave-test

    root: risingwave

  # aws-s3 is a placeholder service to provide configurations
  aws-s3:
    # Id to be picked-up by services
    id: aws-s3

    # The bucket to be used for AWS S3
    bucket: test-bucket

    # access key, secret key and region should be set in aws config (either by env var or .aws/config)

    # used to support other s3 compatible object store.
    s3-compatible: false

  # Apache Kafka service
  kafka:
    # Id to be picked-up by services
    id: kafka-${port}

    # Advertise address of Kafka
    address: "127.0.0.1"

    # Listen port of Kafka
    port: 29092

    # Listen address
    listen-address: ${address}

    # ZooKeeper used by this Kafka instance
    provide-zookeeper: "zookeeper*"

    # If set to true, data will be persisted at data/{id}.
    persist-data: true

    # Kafka broker id. If there are multiple instances of Kafka, we will need to set.
    broker-id: 0

  # Google pubsub emulator service
  pubsub:
    id: pubsub-${port}

    address: "127.0.0.1"

    port: 5980

    persist-data: true

  # Apache ZooKeeper service
  zookeeper:
    # Id to be picked-up by services
    id: zookeeper-${port}

    # Advertise address of ZooKeeper
    address: "127.0.0.1"

    # Listen address
    listen-address: ${address}

    # Listen port of ZooKeeper
    port: 2181

    # If set to true, data will be persisted at data/{id}.
    persist-data: true

  # Only supported in RiseDev compose
  redpanda:
    # Id to be picked-up by services
    id: redpanda

    # Port used inside docker-compose cluster (e.g. create MV)
    internal-port: 29092

    # Port used on host (e.g. import data, connecting using kafkacat)
    outside-port: 9092

    # Connect address
    address: ${id}

    # Number of CPUs to use
    cpus: 8

    # Memory limit for Redpanda
    memory: 16G

  # redis service
  redis:
    # Id to be picked-up by services
    id: redis

    # listen port of redis
    port: 6379

    # address of redis
    address: "127.0.0.1"
