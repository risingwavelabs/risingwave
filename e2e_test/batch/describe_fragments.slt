control substitution on

# Setup test schema
statement ok
CREATE SCHEMA describe_plan_test;

statement ok
CREATE TABLE describe_plan_test.tbl (
    id INT PRIMARY KEY,
    name VARCHAR NOT NULL,
    age INT,
    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
    data JSONB
);

# Test materialized view with complex query
statement ok
CREATE MATERIALIZED VIEW describe_plan_test.complex_mv AS
SELECT
    t.name,
    COUNT(*) as count,
    AVG(t.age) as avg_age,
    jsonb_agg(t.data) as all_data
FROM describe_plan_test.tbl t
WHERE t.age > 18
GROUP BY t.name
HAVING COUNT(*) > 1;

# Test index with included columns and custom distribution
statement ok
CREATE INDEX idx ON describe_plan_test.tbl (name DESC, age)
INCLUDE (created_at)
DISTRIBUTED BY (name);

# Test source with complex schema and options
statement ok
CREATE SOURCE describe_plan_test.src (
    id INT,
    event_time TIMESTAMP,
    payload STRUCT<name VARCHAR, value INT>,
    metadata MAP(VARCHAR, VARCHAR)
) WITH (
    connector = 'datagen',
);

# Test sink with transformation
statement ok
CREATE SINK describe_plan_test.snk AS
SELECT
    id,
    name,
    age,
    created_at,
    data->>'type' as type
FROM describe_plan_test.tbl
WHERE age >= 21
WITH (connector ='blackhole')

# Test view with subquery and joins
statement ok
CREATE VIEW describe_plan_test.complex_view AS
WITH aged_users AS (
    SELECT name, age
    FROM describe_plan_test.tbl
    WHERE age > 25
)
SELECT
    t.name,
    t.age,
    m.count as appearance_count
FROM aged_users t
LEFT JOIN describe_plan_test.complex_mv m ON t.name = m.name;

# Test DESCRIBE FRAGMENTS for each relation
# Use psql_validate.py --db since the fragment id is not deterministic

skipif madsim
system ok
psql_validate.py --db $__DATABASE__ --sql "DESCRIBE FRAGMENTS describe_plan_test.tbl" \
--expected 'Fragment % (Actor %)
StreamMaterialize { columns: [id, name, age, created_at, data], stream_key: [id], pk_columns: [id], pk_conflict: Overwrite }
├── output: [ id, name, age, created_at, data ]
├── stream key: [ id ]
└── StreamFilter { predicate: IsNotNull(name) } { output: [ id, name, age, created_at, data ], stream key: [] }
    └── StreamUnion { all: true } { output: [ id, name, age, created_at, data ], stream key: [] }
        └── MergeExecutor { output: [ id, name, age, created_at, data ], stream key: [] }
(empty)
Fragment % (Actor %)
StreamDml { columns: [id, name, age, created_at, data] } { output: [ id, name, age, created_at, data ], stream key: [] }
└── StreamSource { output: [ id, name, age, created_at, data ], stream key: [] }'


statement error
DESCRIBE FRAGMENTS describe_plan_test.src;
----
db error: ERROR: Failed to run the query

Caused by:
  Not supported: non shared source has no fragments to describe
HINT: Use `DESCRIBE` instead.



skipif madsim
system ok
psql_validate.py --db $__DATABASE__ --sql "DESCRIBE FRAGMENTS describe_plan_test.complex_mv" \
--expected 'Fragment % (Actor %)
StreamMaterialize { columns: [name, count, avg_age, all_data], stream_key: [name], pk_columns: [name], pk_conflict: NoCheck }
├── output: [ tbl.name, count, $expr1, jsonb_agg(tbl.data) ]
├── stream key: [ tbl.name ]
└── StreamProject { exprs: [tbl.name, count, (sum(tbl.age)::Decimal / count(tbl.age)::Decimal) as $expr1, jsonb_agg(tbl.data)] }
    ├── output: [ tbl.name, count, $expr1, jsonb_agg(tbl.data) ]
    ├── stream key: [ tbl.name ]
    └── StreamFilter { predicate: (count > 1:Int32) }
        ├── output: [ tbl.name, count, sum(tbl.age), count(tbl.age), jsonb_agg(tbl.data) ]
        ├── stream key: [ tbl.name ]
        └── StreamHashAgg { group_key: [tbl.name], aggs: [count, sum(tbl.age), count(tbl.age), jsonb_agg(tbl.data)] }
            ├── output: [ tbl.name, count, sum(tbl.age), count(tbl.age), jsonb_agg(tbl.data) ]
            ├── stream key: [ tbl.name ]
            └── MergeExecutor { output: [ tbl.name, tbl.age, tbl.data, tbl.id ], stream key: [ tbl.id ] }
(empty)
Fragment % (Actor %)
StreamFilter { predicate: (tbl.age > 18:Int32) } { output: [ tbl.name, tbl.age, tbl.data, tbl.id ], stream key: [ tbl.id ] }
└── StreamTableScan { table: tbl, columns: [name, age, data, id] }
    ├── output: [ tbl.name, tbl.age, tbl.data, tbl.id ]
    ├── stream key: [ tbl.id ]
    ├── Upstream { output: [ name, age, data, id ], stream key: [] }
    └── BatchPlanNode { output: [ name, age, data, id ], stream key: [] }'


skipif madsim
system ok
psql_validate.py --db $__DATABASE__ --sql "DESCRIBE FRAGMENTS describe_plan_test.idx" \
--expected 'Fragment % (Actor %)
StreamMaterialize { columns: [name, age, created_at, tbl.id(hidden)], stream_key: [tbl.id], pk_columns: [name, age, tbl.id], pk_conflict: NoCheck }
├── output: [ tbl.name, tbl.age, tbl.created_at, tbl.id ]
├── stream key: [ tbl.id ]
└── MergeExecutor { output: [ tbl.name, tbl.age, tbl.created_at, tbl.id ], stream key: [ tbl.id ] }
(empty)
Fragment % (Actor %)
StreamTableScan { table: tbl, columns: [name, age, created_at, id] }
├── output: [ tbl.name, tbl.age, tbl.created_at, tbl.id ]
├── stream key: [ tbl.id ]
├── Upstream { output: [ name, age, created_at, id ], stream key: [] }
└── BatchPlanNode { output: [ name, age, created_at, id ], stream key: [] }'


skipif madsim
system ok
psql_validate.py --db $__DATABASE__ --sql "DESCRIBE FRAGMENTS describe_plan_test.idx" \
--expected 'Fragment % (Actor %)
StreamMaterialize { columns: [name, age, created_at, tbl.id(hidden)], stream_key: [tbl.id], pk_columns: [name, age, tbl.id], pk_conflict: NoCheck }
├── output: [ tbl.name, tbl.age, tbl.created_at, tbl.id ]
├── stream key: [ tbl.id ]
└── MergeExecutor { output: [ tbl.name, tbl.age, tbl.created_at, tbl.id ], stream key: [ tbl.id ] }
(empty)
Fragment % (Actor %)
StreamTableScan { table: tbl, columns: [name, age, created_at, id] }
├── output: [ tbl.name, tbl.age, tbl.created_at, tbl.id ]
├── stream key: [ tbl.id ]
├── Upstream { output: [ name, age, created_at, id ], stream key: [] }
└── BatchPlanNode { output: [ name, age, created_at, id ], stream key: [] }'


skipif madsim
system ok
psql_validate.py --db $__DATABASE__ --sql "DESCRIBE FRAGMENTS describe_plan_test.snk" \
--expected 'Fragment % (Actor %)
StreamSink { type: upsert, columns: [id, name, age, created_at, type], downstream_pk: [] }
├── output: [ tbl.id, tbl.name, tbl.age, tbl.created_at, $expr1 ]
├── stream key: [ tbl.id ]
└── StreamProject { exprs: [tbl.id, tbl.name, tbl.age, tbl.created_at, JsonbAccessStr(tbl.data, '\''type'\'':Varchar) as $expr1] }
    ├── output: [ tbl.id, tbl.name, tbl.age, tbl.created_at, $expr1 ]
    ├── stream key: [ tbl.id ]
    └── StreamFilter { predicate: (tbl.age >= 21:Int32) }
        ├── output: [ tbl.id, tbl.name, tbl.age, tbl.created_at, tbl.data ]
        ├── stream key: [ tbl.id ]
        └── StreamTableScan { table: tbl, columns: [id, name, age, created_at, data] }
            ├── output: [ tbl.id, tbl.name, tbl.age, tbl.created_at, tbl.data ]
            ├── stream key: [ tbl.id ]
            ├── Upstream { output: [ id, name, age, created_at, data ], stream key: [] }
            └── BatchPlanNode { output: [ id, name, age, created_at, data ], stream key: [] }'


query error
DESCRIBE FRAGMENTS describe_plan_test."view-with-dashes";
----
db error: ERROR: Failed to run the query

Caused by these errors (recent errors listed first):
  1: Catalog error
  2: stream job not found: describe_plan_test."view-with-dashes"



# Test error cases
# Non-existent schema
query error
DESCRIBE FRAGMENTS non_existent_schema.some_table;
----
db error: ERROR: Failed to run the query

Caused by these errors (recent errors listed first):
  1: Catalog error
  2: stream job not found: non_existent_schema.some_table


# Non-existent relation in existing schema
query error
DESCRIBE FRAGMENTS describe_plan_test.non_existent_table;
----
db error: ERROR: Failed to run the query

Caused by these errors (recent errors listed first):
  1: Catalog error
  2: stream job not found: describe_plan_test.non_existent_table


# System table (view)
query error
DESCRIBE FRAGMENTS pg_catalog.pg_tables;
----
db error: ERROR: Failed to run the query

Caused by:
  Not supported: view has no fragments to describe
HINT: Use `DESCRIBE` instead.



# System table (should fail)
query error
DESCRIBE FRAGMENTS pg_catalog.rw_system_tables;
----
db error: ERROR: Failed to run the query

Caused by these errors (recent errors listed first):
  1: Catalog error
  2: stream job not found: pg_catalog.rw_system_tables




# Clean up
statement ok
DROP SCHEMA describe_plan_test CASCADE;

control substitution off
