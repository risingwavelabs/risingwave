# Issue #24717: Kafka Source Hangs on topics with transaction control messages
#
# When Kafka transactions are used, COMMIT/ABORT control messages occupy offsets
# but are not visible to consumers in read_committed mode. This causes backfill
# to hang because target_offset (high_watermark - 1) may point to a control message
# that will never be delivered to the consumer.
#
# This test verifies that backfill completes correctly even when the last visible
# offset is not target_offset due to transaction control messages.
#
# Note: This test requires manual setup with a Rust transactional producer
# (see issue #24717 for the producer code). For automated CI testing,
# we use a simpler test that exercises the EOF handling logic.

control substitution on

# Create a topic for transaction testing
system ok
rpk topic create issue_24717_tx_test -p 1

# Try to send messages with transaction (requires kafka-python)
# If kafka-python is not available, fall back to normal messages
system ok
python3 - ${RISEDEV_KAFKA_BOOTSTRAP_SERVERS} issue_24717_tx_test <<'PYTHON_EOF' || cat <<EOF | rpk topic produce issue_24717_tx_test -f "%v"
import sys
try:
    from kafka import KafkaProducer
    from kafka.errors import KafkaError
except ImportError:
    print("kafka-python not available, using fallback", file=sys.stderr)
    sys.exit(1)

bootstrap_servers = sys.argv[1]
topic = sys.argv[2]

producer = KafkaProducer(
    bootstrap_servers=bootstrap_servers,
    transactional_id=f'test-txn-{topic}',
    value_serializer=lambda v: v.encode('utf-8'),
    acks='all',
    retries=3,
    max_in_flight_requests_per_connection=1
)

try:
    producer.init_transactions()
    producer.begin_transaction()
    producer.send(topic, '{"id": 1, "name": "test1"}').get(timeout=10)
    producer.send(topic, '{"id": 2, "name": "test2"}').get(timeout=10)
    producer.commit_transaction()
    print("Transaction committed with COMMIT control message")
except Exception as e:
    print(f"Transaction failed: {e}", file=sys.stderr)
    producer.abort_transaction()
    sys.exit(1)
finally:
    producer.close()
PYTHON_EOF
{"id": 1, "name": "test1"}
{"id": 2, "name": "test2"}
EOF

# Wait for messages to be available
sleep 2s

# Create source
statement ok
CREATE SOURCE issue_24717_source (id int, name varchar)
WITH (
    ${RISEDEV_KAFKA_WITH_OPTIONS_COMMON},
    topic = 'issue_24717_tx_test',
    scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE JSON;

# Test 1: Normal backfill without transactions
# This should work in both old and new code
statement ok
SET BACKGROUND_DDL = false;

statement ok
CREATE MATERIALIZED VIEW issue_24717_mv AS SELECT * FROM issue_24717_source;

# Verify data is correctly ingested
query IT
SELECT id, name FROM issue_24717_mv ORDER BY id;
----
1 test1
2 test2

# Test 2: Simulate EOF handling by creating MV on existing data
# The new code enables partition.eof which will trigger EOF signals
# even without transaction control messages
statement ok
CREATE MATERIALIZED VIEW issue_24717_mv2 AS SELECT * FROM issue_24717_source;

query IT
SELECT id, name FROM issue_24717_mv2 ORDER BY id;
----
1 test1
2 test2

# Test 3: Add more data and verify EOF handling on incremental data
system ok
cat <<EOF | rpk topic produce issue_24717_tx_test -f "%v"
{"id": 3, "name": "test3"}
EOF

sleep 2s

# Create another MV - this will backfill from start to current position
# With partition.eof enabled, it should complete quickly
statement ok
CREATE MATERIALIZED VIEW issue_24717_mv3 AS SELECT * FROM issue_24717_source;

query IT
SELECT id, name FROM issue_24717_mv3 ORDER BY id;
----
1 test1
2 test2
3 test3

# Cleanup
statement ok
DROP MATERIALIZED VIEW issue_24717_mv3;

statement ok
DROP MATERIALIZED VIEW issue_24717_mv2;

statement ok
DROP MATERIALIZED VIEW issue_24717_mv;

statement ok
DROP SOURCE issue_24717_source;

system ok
rpk topic delete issue_24717_tx_test

# Note: To properly test transaction control messages, use the following manual test:
# 1. Run the Rust producer from issue #24717 to create transaction control messages
# 2. Verify with: kafka-run-class kafka.tools.DumpLogSegments --files /tmp/kafka-logs/demo-0/*.log --print-data-log
# 3. Create source and MV as above - should NOT hang
