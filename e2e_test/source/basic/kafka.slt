# We don't support CSV header for Kafka
statement error CSV HEADER is not supported when creating table with Kafka connector
create table s0 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_csv_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format csv DELIMITED BY ',';

statement ok
create table s0 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_csv_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format csv WITHOUT HEADER DELIMITED BY ',';

# expect fail with invalid broker address
statement error
create table s1 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_partition_topic',
  properties.bootstrap.server = '127.0.0.1:9092',
  scan.startup.mode = 'earliest'
) row format json

statement ok
create table s1 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_partition_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format json

statement ok
select * from s1;

statement ok
create table s2 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_2_partition_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format json

statement ok
select * from s2;

statement ok
create table s3 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_3_partition_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format json

statement ok
select * from s3;

statement ok
create table s4 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_4_partition_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format json

statement ok
select * from s4;

statement ok
create table s5 (v1 int, v2 varchar, v3 int[], v4 struct<v5 int, v6 int>) append only with (
  connector = 'kafka',
  topic = 'kafka_4_partition_topic_with_100_message',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest',
) row format json

statement ok
select * from s5;

statement ok
create table s6 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_partition_mv_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format json

statement ok
select * from s6;

statement ok
create sink si
from
  s5 with (
    properties.bootstrap.server = 'message_queue:29092',
    topic = 'sink_target',
    type = 'append-only',
    connector = 'kafka'
  )

query T
show sinks
----
si

statement ok
create table s7 (v1 int, v2 varchar, v3 int[], v4 struct<v5 int, v6 int>) with (
  connector = 'kafka',
  topic = 'sink_target',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format json

statement ok
select * from s7

# we cannot create debezium source without pk
statement error
create table s8 (
  id integer,
  first_name varchar,
  last_name varchar,
  email varchar
) with (
  connector = 'kafka',
  topic = 'debezium_log',
  properties.bootstrap.server = 'message_queue:29092'
) row format debezium_json

statement ok
create table s8 (
  id integer,
  first_name varchar,
  last_name varchar,
  email varchar,
  PRIMARY KEY (id)
) with (
  connector = 'kafka',
  topic = 'debezium_log',
  properties.bootstrap.server = 'message_queue:29092'
) row format debezium_json

statement ok
create table s9 with (
  connector = 'kafka', 
  topic = 'avro_bin',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format avro 
row schema location 'file:///risingwave/avro-simple-schema.avsc'

statement ok
create table s10 with (
  connector = 'kafka', 
  topic = 'avro_c_bin',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format avro 
row schema location 'file:///risingwave/avro-complex-schema.avsc'

statement ok
create table s11 with (
  connector = 'kafka', 
  topic = 'proto_c_bin',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest',
  proto.message = 'test.User'
) row format protobuf message 'test.User' row schema location 'file:///risingwave/proto-complex-schema'

statement ok
CREATE TABLE s12(
    id int,
    code string, 
    timestamp bigint, 
    xfas struct<device_model_id int, device_make_id int, ip string>[],
    contacts struct<emails string[], phones string[]>,
    jsonb jsonb)
WITH (
    connector = 'kafka',
    topic = 'json_c',
    properties.bootstrap.server = 'message_queue:29092',
    scan.startup.mode = 'earliest')
ROW format JSON;

# we cannot create maxwell source without pk
statement error
create table s13 (
  id integer,
  name varchar,
  is_adult integer,
  birthday timestamp
) with (
  connector = 'kafka',
  topic = 'maxwell_json',
  properties.bootstrap.server = 'message_queue:29092'
) row format maxwell;

statement ok
create table s13 (
  id integer,
  name varchar,
  is_adult integer,
  birthday timestamp,
  PRIMARY KEY (id)
) with (
  connector = 'kafka',
  topic = 'maxwell_json',
  properties.bootstrap.server = 'message_queue:29092'
) row format maxwell;

statement ok
create table s14 (
  id integer,
  name varchar,
  is_adult bool,
  reg_time timestamp,
	balance decimal,
	win_rate double,
  PRIMARY KEY (id)
) with (
  connector = 'kafka',
  topic = 'cannal_json',
  properties.bootstrap.server = 'message_queue:29092'
) row format CANAL_JSON;

statement ok
create table s15 (
  v1 int,
  v2 int,
  PRIMARY KEY (v1)
) with (
  connector = 'kafka',
  topic = 'canal_json_double_field',
  properties.bootstrap.server = 'message_queue:29092'
) row format CANAL_JSON;

statement ok
create table s16 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_4_partition_topic_with_100_message',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'latest'
) row format json

statement ok
create source s17 with (
  connector = 'kafka', 
  topic = 'proto_c_bin',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest',
  proto.message = 'test.User'
) row format protobuf message 'test.User' row schema location 'file:///risingwave/proto-complex-schema'

statement ok
create source s18 with (
  connector = 'kafka', 
  topic = 'avro_c_bin',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format avro 
row schema location 'file:///risingwave/avro-complex-schema.avsc'

# we cannot use confluent schema registry when connector is not kafka
statement error
create table s19 
with (
  connector = 'kinesis',
  topic = 'topic',
  properties.bootstrap.server = 'message_queue:29092'
)
row format avro
row schema location confluent schema registry 'http://127.0.0.1:8081'

# we cannot create debezium source with generated column
statement error
create table s20 (
  id integer primary key,
  first_name varchar,
  last_name varchar,
  email varchar,
  gen_id integer as id+1
) with (
  connector = 'kafka',
  topic = 'debezium_log',
  properties.bootstrap.server = 'message_queue:29092'
) row format debezium_json

# create kafka source table with generated column
statement ok
create table s21 (v1 int as v2-1, v2 int, v3 int as v2+1) with (
  connector = 'kafka',
  topic = 'kafka_4_partition_topic_generated_columns',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format json

# create kafka source with generated column
statement ok
create source s22 (v1 int as v2-1, v2 int, v3 int as v2+1, p timestamptz as proctime()) with (
  connector = 'kafka',
  topic = 'kafka_4_partition_topic_generated_columns',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format json

statement ok
create source s23 (bytea_example bytea) with (
  connector = 'kafka',
  topic = 'json_bytea',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format json

statement ok
CREATE TABLE mongo_customers (
	_id BIGINT PRIMARY KEY,
	payload jsonb
)
WITH (
	connector = 'kafka',
  properties.bootstrap.server = 'message_queue:29092',
	topic = 'debezium_mongo_json_customers')
ROW FORMAT DEBEZIUM_MONGO_JSON

# we cannot create upsert_json source without pk
statement error
CREATE TABLE upsert_students (
    "ID" INT,
    "firstName" VARCHAR,
    "lastName" VARCHAR,
    age INT,
    height REAL,
    weight REAL
)
WITH (
	connector = 'kafka',
  properties.bootstrap.server = 'message_queue:29092',
	topic = 'upsert_json')
ROW FORMAT UPSERT_JSON

statement ok
CREATE TABLE upsert_students (
    "ID" INT PRIMARY KEY,
    "firstName" VARCHAR,
    "lastName" VARCHAR,
    age INT,
    height REAL,
    weight REAL
)
WITH (
	connector = 'kafka',
  properties.bootstrap.server = 'message_queue:29092',
	topic = 'upsert_json')
ROW FORMAT UPSERT_JSON


# key schema should be a subset of value schema
statement error
CREATE TABLE upsert_avro_json ()
WITH (
	connector = 'kafka',
  properties.bootstrap.server = 'message_queue:29092',
	topic = 'upsert_avro_json')
ROW FORMAT UPSERT_AVRO 
row schema location confluent schema registry 'http://message_queue:8081'


statement ok
CREATE TABLE upsert_avro_json (
    id TEXT  PRIMARY KEY
)
WITH (
	connector = 'kafka',
  properties.bootstrap.server = 'message_queue:29092',
	topic = 'upsert_avro_json')
ROW FORMAT UPSERT_AVRO 
row schema location confluent schema registry 'http://message_queue:8081'

statement ok
CREATE TABLE upsert_avro_json2 (
    "ID" TEXT  PRIMARY KEY
)
WITH (
	connector = 'kafka',
  properties.bootstrap.server = 'message_queue:29092',
	topic = 'upsert_avro_json')
ROW FORMAT UPSERT_AVRO 
row schema location confluent schema registry 'http://message_queue:8081'

statement ok
flush;

# Wait enough time to ensure SourceExecutor consumes all Kafka data.
sleep 1s

query IT rowsort
select v1, v2 from s0;
----
1 1
2 22
3 333
4 4444

statement ok
drop table s0

query IT rowsort
select * from s1
----
1 1
2 22
3 333
4 4444

statement ok
drop table s1

query IT rowsort
select * from s2
----
1 1
2 22
3 333
4 4444

statement ok
drop table s2

query IT rowsort
select * from s3
----
1 1
2 22
3 333
4 4444

statement ok
drop table s3

query IT rowsort
select * from s4
----
1 1
2 22
3 333
4 4444

statement ok
drop table s4

query I
select count(*) from s5
----
100

# we only support at-least-once semantic, count distinct to make result deterministic in recovery test
query I
select count(distinct (v1,v2)) from s7
----
100

statement ok
drop sink si

statement ok
drop table s5

statement ok
drop table s7

query I
select count(*) from s6
----
20

statement ok
create materialized view source_mv1 as select * from s6;

statement ok
create materialized view source_mv2 as select sum(v1) as sum_v1, count(v2) as count_v2 from s6 where v1 > 3;

# Wait for source
sleep 10s

# Flush into storage
statement ok
flush;

query IT rowsort
select v1, v2 from source_mv1;
----
0 name5
0 name9
1 name0
1 name7
2 name0
2 name3
3 name2
3 name9
4 name6
4 name7
5 name3
5 name8
6 name3
6 name4
7 name0
7 name5
8 name8
8 name9
9 name2
9 name2

query II
select sum_v1, count_v2 from source_mv2;
----
78 12

query ITTT rowsort
select id, first_name, last_name, email from s8;
----
1001 Sally Thomas sally.thomas@acme.com
1002 George Bailey gbailey@foobar.com
1003 Edward Walker ed@walker.com
1004 Anne1 Kretchmar annek@noanswer.org
1005 add add2 add

query IITFFBTT
select id, sequence_id, name, score, avg_score, is_lasted, entrance_date, birthday, passed from s9;
----
32 64 str_value 32 64 t 1970-01-01 1970-01-01 00:00:00+00:00 1 mon 1 day 00:00:01

query ITITT
select id, code, timestamp, xfas, contacts, sex from s10;
----
100 abc 1473305798 {"(0,200,10.0.0.1)","(1,400,10.0.0.2)"} ({1xxx,2xxx},{1xxx,2xxx}) MALE

query ITITT
select id, code, timestamp, xfas, contacts, sex from s11;
----
0 abc 1473305798 {"(0,200,127.0.0.1)","(1,400,127.0.0.2)"} ({1xxx,2xxx},{1xxx,2xxx}) MALE

query ITITT
select id, code, timestamp, xfas, contacts, jsonb from s12;
----
100 abc 1473305798 {"(0,200,10.0.0.1)","(1,400,10.0.0.2)"} ({1xxx,2xxx},{1xxx,2xxx}) {"blockNumber": 16938734}

query ITIT
select * from s13 order by id;
----
1 tom 0 2017-12-31 16:00:01
2 chi 1 1999-12-31 16:00:01

query ITTTTT
select * from s14;
----
1 mike f 2018-01-01 00:00:01 1500.62 0.65

query I
select count(*) from s15
----
0

query I
select count(*) from s16
----
0

query III rowsort
select * from s21;
----
19 20 21
20 21 22
NULL NULL NULL

query III rowsort
select v1, v2, v3, p >= date '2021-01-01' from s22;
----
19 20 21 t
20 21 22 t
NULL NULL NULL t

query T rowsort
select * from s23;
----
\x31324344
\xdeadbeef

query II 
SELECT
	*
FROM
	mongo_customers
ORDER BY
	_id;
----
1001    {"_id": {"$numberLong": "1001"}, "email": "sally.thomas@acme.com", "first_name": "Sally", "last_name": "Thomas"}
1002    {"_id": {"$numberLong": "1002"}, "email": "gbailey@foobar.com", "first_name": "George", "last_name": "Bailey"}
1003    {"_id": {"$numberLong": "1003"}, "email": "ed@walker.com", "first_name": "Edward", "last_name": "Walker"}
1004    {"_id": {"$numberLong": "1004"}, "email": "annek@noanswer.org", "first_name": "Anne", "last_name": "Kretchmar"}

query II 
SELECT
	*
FROM
	upsert_students
ORDER BY
	"ID";
----
1	Ethan	Martinez	18	6.1	180
2	Emily	Jackson	19	5.4	110
3	Noah	Thompson	21	6.3	195
4	Emma	Brown	20	5.3	130
5	Michael	Williams	22	6.2	190
6	Leah	Davis	18	5.7	140
9	Jacob	Anderson	20	5.8	155

query II 
SELECT
	*
FROM
	upsert_avro_json
ORDER BY
	"ID";
----

query II 
SELECT
	*
FROM
	upsert_avro_json2
ORDER BY
	"ID";
----


statement ok
drop materialized view source_mv1

statement ok
drop materialized view source_mv2

statement ok
drop table s6

statement ok
drop table s8

statement ok
drop table s9

statement ok
drop table s10

statement ok
drop table s11

statement ok
drop table s12

statement ok
drop table s13

statement ok
drop table s14

statement ok
drop table s15

statement ok
drop table s16

statement ok
drop source s17

statement ok
drop source s18

statement ok
drop table s21

statement ok
drop source s22

statement ok
drop source s23

statement ok
DROP TABLE mongo_customers;

statement ok
DROP TABLE upsert_students;

statement ok
DROP TABLE upsert_avro_json;

statement ok
DROP TABLE upsert_avro_json2;