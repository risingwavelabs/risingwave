# We don't support CSV header for Kafka
statement error CSV HEADER is not supported when creating table with Kafka connector
create table s0 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_csv_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format csv DELIMITED BY ',';

statement ok
create table s0 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_csv_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format csv WITHOUT HEADER DELIMITED BY ',';

# expect fail with invalid broker address
statement error
create table s1 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_partition_topic',
  properties.bootstrap.server = '127.0.0.1:9092',
  scan.startup.mode = 'earliest'
) ROW FORMAT JSON

statement ok
create table s1 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_partition_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) ROW FORMAT JSON

statement ok
select * from s1;

statement ok
create table s2 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_2_partition_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) ROW FORMAT JSON

statement ok
select * from s2;

statement ok
create table s3 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_3_partition_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) ROW FORMAT JSON

statement ok
select * from s3;

statement ok
create table s4 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_4_partition_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) ROW FORMAT JSON

statement ok
select * from s4;

statement ok
create table s5 (v1 int, v2 varchar, v3 int[], v4 struct<v5 int, v6 int>) append only with (
  connector = 'kafka',
  topic = 'kafka_4_partition_topic_with_100_message',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest',
) ROW FORMAT JSON

statement ok
select * from s5;

statement ok
create table s6 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_partition_mv_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) ROW FORMAT JSON

statement ok
select * from s6;

statement ok
create sink si
from
  s5 with (
    properties.bootstrap.server = 'message_queue:29092',
    topic = 'sink_target',
    type = 'append-only',
    connector = 'kafka'
  )

query T
show sinks
----
public.si

statement ok
create table s7 (v1 int, v2 varchar, v3 int[], v4 struct<v5 int, v6 int>) with (
  connector = 'kafka',
  topic = 'sink_target',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) ROW FORMAT JSON

statement ok
select * from s7

# we cannot create debezium source without pk
statement error
create table s8 (
  id integer,
  first_name varchar,
  last_name varchar,
  email varchar
) with (
  connector = 'kafka',
  topic = 'debezium_log',
  properties.bootstrap.server = 'message_queue:29092'
) ROW FORMAT DEBEZIUM_JSON

statement ok
create table s8 (
  id integer,
  first_name varchar,
  last_name varchar,
  email varchar,
  PRIMARY KEY (id)
) with (
  connector = 'kafka',
  topic = 'debezium_log',
  properties.bootstrap.server = 'message_queue:29092'
) ROW FORMAT DEBEZIUM_JSON

statement ok
create table s8_no_schema_field (
  id integer,
  first_name varchar,
  last_name varchar,
  email varchar,
  PRIMARY KEY (id)
) with (
  connector = 'kafka',
  topic = 'debezium_log_no_schema_field',
  properties.bootstrap.server = 'message_queue:29092'
) ROW FORMAT DEBEZIUM_JSON

statement error without schema registry
create table s9 with (
  connector = 'kafka',
  topic = 'avro_simple_schema_bin',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format avro row schema location 'file:///risingwave/avro-simple-schema.avsc'

statement error without schema registry
create table s10 with (
  connector = 'kafka',
  topic = 'avro_complex_schema_bin',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format avro row schema location 'file:///risingwave/avro-complex-schema.avsc'

statement ok
CREATE TABLE s12(
    id int,
    code string,
    timestamp bigint,
    xfas struct<device_model_id int, device_make_id int, ip string>[],
    contacts struct<emails string[], phones string[]>,
    jsonb jsonb)
WITH (
    connector = 'kafka',
    topic = 'json_c',
    properties.bootstrap.server = 'message_queue:29092',
    scan.startup.mode = 'earliest')
ROW format JSON

# we cannot create maxwell source without pk
statement error
create table s13 (
  id integer,
  name varchar,
  is_adult integer,
  birthday timestamp
) with (
  connector = 'kafka',
  topic = 'maxwell_json',
  properties.bootstrap.server = 'message_queue:29092'
) row format MAXWELL;

statement ok
create table s13 (
  id integer,
  name varchar,
  is_adult integer,
  birthday timestamp,
  PRIMARY KEY (id)
) with (
  connector = 'kafka',
  topic = 'maxwell_json',
  properties.bootstrap.server = 'message_queue:29092'
) row format MAXWELL;

statement ok
create table s14 (
  id integer,
  name varchar,
  is_adult bool,
  reg_time timestamp,
	balance decimal,
	win_rate double,
  PRIMARY KEY (id)
) with (
  connector = 'kafka',
  topic = 'cannal_json',
  properties.bootstrap.server = 'message_queue:29092'
) ROW FORMAT CANAL_JSON;

statement ok
create table s15 (
  v1 int,
  v2 int,
  PRIMARY KEY (v1)
) with (
  connector = 'kafka',
  topic = 'canal_json_double_field',
  properties.bootstrap.server = 'message_queue:29092'
) ROW FORMAT CANAL_JSON;

statement ok
create table s16 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_4_partition_topic_with_100_message',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'latest'
) ROW FORMAT JSON

statement error without schema registry
create source s18 with (
  connector = 'kafka',
  topic = 'avro_complex_schema_bin',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) row format avro row schema location 'file:///risingwave/avro-complex-schema.avsc'

# we cannot use confluent schema registry when connector is not kafka
statement error
create table s19
with (
  connector = 'kinesis',
  topic = 'topic',
  properties.bootstrap.server = 'message_queue:29092'
) row format avro
row schema location confluent schema registry 'http://127.0.0.1:8081'

# create debezium source with generated column
statement ok
create table s20 (
  id integer primary key,
  first_name varchar,
  last_name varchar,
  email varchar,
  gen_id integer as id+1
) with (
  connector = 'kafka',
  topic = 'debezium_log',
  properties.bootstrap.server = 'message_queue:29092'
) ROW FORMAT DEBEZIUM_JSON

# create kafka source table with generated column
statement ok
create table s21 (v1 int as v2-1, v2 int, v3 int as v2+1) with (
  connector = 'kafka',
  topic = 'kafka_4_partition_topic_generated_columns',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) ROW FORMAT JSON

# create kafka source with generated column
statement ok
create source s22 (v1 int as v2-1, v2 int, v3 int as v2+1, p timestamptz as proctime()) with (
  connector = 'kafka',
  topic = 'kafka_4_partition_topic_generated_columns',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) ROW FORMAT JSON

statement ok
create source s23 (bytea_example bytea) with (
  connector = 'kafka',
  topic = 'json_bytea',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) ROW FORMAT JSON

statement ok
create source s24 (id bytea) with (
  connector = 'kafka',
  topic = 'kafka_source_format_bytes',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) ROW FORMAT BYTES

# bytes format only accept one column
statement error
create source s25 (v1 bytea, v2 int) with (
  connector = 'kafka',
  topic = 'kafka_source_format_bytes',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) ROW FORMAT BYTES

# bytes format only accept bytea type
statement error
create source s26 (id int) with (
  connector = 'kafka',
  topic = 'kafka_source_format_bytes',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) ROW FORMAT BYTES

statement ok
CREATE TABLE mongo_customers (
	_id BIGINT PRIMARY KEY,
	payload jsonb
)
WITH (
	connector = 'kafka',
  properties.bootstrap.server = 'message_queue:29092',
	topic = 'debezium_mongo_json_customers')
ROW FORMAT DEBEZIUM_MONGO_JSON

statement ok
CREATE TABLE mongo_customers_no_schema_field (
	_id BIGINT PRIMARY KEY,
	payload jsonb
)
WITH (
	connector = 'kafka',
  properties.bootstrap.server = 'message_queue:29092',
	topic = 'debezium_mongo_json_customers_no_schema_field')
ROW FORMAT DEBEZIUM_MONGO_JSON

statement ok
CREATE TABLE upsert_students (
    "ID" INT,
    "firstName" VARCHAR,
    "lastName" VARCHAR,
    age INT,
    height REAL,
    weight REAL,
    primary key (rw_key)
)
INCLUDE KEY AS rw_key
WITH (
	connector = 'kafka',
    properties.bootstrap.server = 'message_queue:29092',
	topic = 'upsert_json')
ROW FORMAT UPSERT_JSON

statement ok
CREATE TABLE dbz_ignore_case_json (
   L_ORDERKEY BIGINT,
   L_PARTKEY BIGINT,
   L_SUPPKEY BIGINT,
   L_LINENUMBER BIGINT,
   L_QUANTITY DECIMAL,
   L_EXTENDEDPRICE DECIMAL,
   L_DISCOUNT DECIMAL,
   L_TAX DECIMAL,
   L_RETURNFLAG VARCHAR,
   L_LINESTATUS VARCHAR,
   L_SHIPDATE DATE,
   L_COMMITDATE DATE,
   L_RECEIPTDATE DATE,
   L_SHIPINSTRUCT VARCHAR,
   L_SHIPMODE VARCHAR,
   L_COMMENT VARCHAR,
   PRIMARY KEY(L_ORDERKEY, L_LINENUMBER)
) WITH (
    connector = 'kafka',
    properties.bootstrap.server = 'message_queue:29092',
    topic = 'debezium_ignore_case_json'
) ROW FORMAT DEBEZIUM_JSON


statement ok
flush;

# Wait enough time to ensure SourceExecutor consumes all Kafka data.
sleep 1s

query IT rowsort
select v1, v2 from s0;
----
1 1
2 22
3 333
4 4444

statement ok
drop table s0

query IT rowsort
select * from s1
----
1 1
2 22
3 333
4 4444

statement ok
drop table s1

query IT rowsort
select * from s2
----
1 1
2 22
3 333
4 4444

statement ok
drop table s2

query IT rowsort
select * from s3
----
1 1
2 22
3 333
4 4444

statement ok
drop table s3

query IT rowsort
select * from s4
----
1 1
2 22
3 333
4 4444

statement ok
drop table s4

query I
select count(*) from s5
----
100

# we only support at-least-once semantic, count distinct to make result deterministic in recovery test
query I
select count(distinct (v1,v2)) from s7
----
100

statement ok
drop sink si

statement ok
drop table s5

statement ok
drop table s7

query I
select count(*) from s6
----
20

statement ok
create materialized view source_mv1 as select * from s6;

statement ok
create materialized view source_mv2 as select sum(v1) as sum_v1, count(v2) as count_v2 from s6 where v1 > 3;

statement ok
create materialized view source_mv3 as select * from s24;

# Wait for source
sleep 10s

# Flush into storage
statement ok
flush;

query IT rowsort
select v1, v2 from source_mv1;
----
0 name5
0 name9
1 name0
1 name7
2 name0
2 name3
3 name2
3 name9
4 name6
4 name7
5 name3
5 name8
6 name3
6 name4
7 name0
7 name5
8 name8
8 name9
9 name2
9 name2

query II
select sum_v1, count_v2 from source_mv2;
----
78 12

query ITTT rowsort
select id, first_name, last_name, email from s8;
----
1001 Sally Thomas sally.thomas@acme.com
1002 George Bailey gbailey@foobar.com
1003 Edward Walker ed@walker.com
1004 Anne1 Kretchmar annek@noanswer.org
1005 add add2 add

query ITTT rowsort
select id, first_name, last_name, email from s8_no_schema_field;
----
1001 Sally Thomas sally.thomas@acme.com
1002 George Bailey gbailey@foobar.com
1003 Edward Walker ed@walker.com
1004 Anne1 Kretchmar annek@noanswer.org
1005 add add2 add

# query IITFFBTT
# select id, sequence_id, name, score, avg_score, is_lasted, entrance_date, birthday, passed from s9;
# ----
# 32 64 str_value 32 64 t 1970-01-01 1970-01-01 00:00:00+00:00 1 mon 1 day 00:00:01
#
# query ITITT
# select id, code, timestamp, xfas, contacts, sex from s10;
# ----
# 100 abc 1473305798 {"(0,200,10.0.0.1)","(1,400,10.0.0.2)"} ("{1xxx,2xxx}","{1xxx,2xxx}") MALE

query ITITT
select id, code, timestamp, xfas, contacts, jsonb from s12;
----
100 abc 1473305798 {"(0,200,10.0.0.1)","(1,400,10.0.0.2)"} ("{1xxx,2xxx}","{1xxx,2xxx}") {"blockNumber": 16938734}

query ITIT
select * from s13 order by id;
----
1 tom 0 2017-12-31 16:00:01
2 chi 1 1999-12-31 16:00:01

query ITTTTT
select * from s14;
----
1 mike f 2018-01-01 00:00:01 1500.62 0.65

query I
select count(*) from s15
----
2

query I
select count(*) from s16
----
0

query III rowsort
select * from s21;
----
19 20 21
20 21 22
NULL NULL NULL

query III rowsort
select v1, v2, v3, p >= date '2021-01-01' from s22;
----
19 20 21 t
20 21 22 t
NULL NULL NULL t

query T rowsort
select * from s23;
----
\x31324344
\xdeadbeef

query II
SELECT
	*
FROM
	mongo_customers
ORDER BY
	_id;
----
1001    {"_id": {"$numberLong": "1001"}, "email": "sally.thomas@acme.com", "first_name": "Sally", "last_name": "Thomas"}
1002    {"_id": {"$numberLong": "1002"}, "email": "gbailey@foobar.com", "first_name": "George", "last_name": "Bailey"}
1003    {"_id": {"$numberLong": "1003"}, "email": "ed@walker.com", "first_name": "Edward", "last_name": "Walker"}
1004    {"_id": {"$numberLong": "1004"}, "email": "annek@noanswer.org", "first_name": "Anne", "last_name": "Kretchmar"}

query II
SELECT
	*
FROM
	mongo_customers_no_schema_field
ORDER BY
	_id;
----
1001    {"_id": {"$numberLong": "1001"}, "email": "sally.thomas@acme.com", "first_name": "Sally", "last_name": "Thomas"}
1002    {"_id": {"$numberLong": "1002"}, "email": "gbailey@foobar.com", "first_name": "George", "last_name": "Bailey"}
1003    {"_id": {"$numberLong": "1003"}, "email": "ed@walker.com", "first_name": "Edward", "last_name": "Walker"}
1004    {"_id": {"$numberLong": "1004"}, "email": "annek@noanswer.org", "first_name": "Anne", "last_name": "Kretchmar"}


query II
SELECT
	"ID",
  "firstName",
  "lastName",
  "age",
  "height",
  "weight"
FROM
	upsert_students
ORDER BY
	"ID";
----
1	Ethan	Martinez	18	6.1	180
2	Emily	Jackson	19	5.4	110
3	Noah	Thompson	21	6.3	195
4	Emma	Brown	20	5.3	130
5	Michael	Williams	22	6.2	190
6	Leah	Davis	18	5.7	140
9	Jacob	Anderson	20	5.8	155

query II
select
    L_ORDERKEY,
    L_LINENUMBER
from dbz_ignore_case_json
order by
    L_ORDERKEY,
    L_LINENUMBER;
----
56165    1
56166    1
56166    2

query I
SELECT id FROM source_mv3 ORDER BY id;
----
\x6b6b
\x776561776566776566

statement ok
drop materialized view source_mv1

statement ok
drop materialized view source_mv2

statement ok
drop materialized view source_mv3

statement ok
drop table s6

statement ok
drop table s8

statement ok
drop table s8_no_schema_field

# statement ok
# drop table s9
#
# statement ok
# drop table s10

statement ok
drop table s12

statement ok
drop table s13

statement ok
drop table s14

statement ok
drop table s15

statement ok
drop table s16

# statement ok
# drop source s18

statement ok
drop table s20

statement ok
drop table s21

statement ok
drop source s22

statement ok
drop source s23

statement ok
drop source s24

statement ok
DROP TABLE mongo_customers;

statement ok
DROP TABLE mongo_customers_no_schema_field;

statement ok
DROP TABLE upsert_students;

statement ok
drop table dbz_ignore_case_json;
