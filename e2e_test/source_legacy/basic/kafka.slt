statement ok
SET streaming_use_shared_source TO false;

# We don't support CSV header for Kafka
statement error CSV HEADER is not supported when creating table with Kafka connector
create table s0 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_csv_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE CSV (delimiter = ',');

statement ok
create table t0 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_csv_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE CSV (delimiter = ',', without_header = true);

statement ok
create source s0 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_csv_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE CSV (delimiter = ',', without_header = true);

# expect fail with invalid broker address
statement error
create table s1 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_partition_topic',
  properties.bootstrap.server = '127.0.0.1:9092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE JSON

statement ok
create table s1 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_partition_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE JSON

statement ok
select * from s1;

statement ok
create table s2 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_2_partition_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE JSON

statement ok
select * from s2;

statement ok
create table s3 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_3_partition_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE JSON

statement ok
select * from s3;

statement ok
create table s4 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_4_partition_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE JSON

statement ok
select * from s4;

statement ok
create table s5 (v1 int, v2 varchar, v3 int[], v4 struct<v5 int, v6 int>) append only with (
  connector = 'kafka',
  topic = 'kafka_4_partition_topic_with_100_message',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest',
) FORMAT PLAIN ENCODE JSON

statement ok
select * from s5;

statement ok
create table s6 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_partition_mv_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE JSON

statement ok
select * from s6;

statement ok
create sink si
from
  s5 with (
    properties.bootstrap.server = 'message_queue:29092',
    topic = 'sink_target',
    type = 'append-only',
    connector = 'kafka'
  )

query T
show sinks
----
si

statement ok
create table s7 (v1 int, v2 varchar, v3 int[], v4 struct<v5 int, v6 int>) with (
  connector = 'kafka',
  topic = 'sink_target',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE JSON

statement ok
select * from s7

# we cannot create debezium source without pk
statement error
create table s8 (
  id integer,
  first_name varchar,
  last_name varchar,
  email varchar
) with (
  connector = 'kafka',
  topic = 'debezium_log',
  properties.bootstrap.server = 'message_queue:29092'
) FORMAT DEBEZIUM ENCODE JSON

statement ok
create table s8 (
  id integer,
  first_name varchar,
  last_name varchar,
  email varchar,
  PRIMARY KEY (id)
) with (
  connector = 'kafka',
  topic = 'debezium_log',
  properties.bootstrap.server = 'message_queue:29092'
) FORMAT DEBEZIUM ENCODE JSON

statement ok
create table s8_no_schema_field (
  id integer,
  first_name varchar,
  last_name varchar,
  email varchar,
  PRIMARY KEY (id)
) with (
  connector = 'kafka',
  topic = 'debezium_log_no_schema_field',
  properties.bootstrap.server = 'message_queue:29092'
) FORMAT DEBEZIUM ENCODE JSON

statement error without schema registry
create table s9 with (
  connector = 'kafka',
  topic = 'avro_simple_schema_bin',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE AVRO (schema.location = 'file:///risingwave/avro-simple-schema.avsc');

statement ok
create table s9 with (
  connector = 'kafka',
  topic = 'avro_simple_schema_bin',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE AVRO (schema.location = 'file:///risingwave/avro-simple-schema.avsc', with_deprecated_file_header = true);

statement ok
create table s10 with (
  connector = 'kafka',
  topic = 'avro_complex_schema_bin',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE AVRO (schema.location = 'file:///risingwave/avro-complex-schema.avsc', with_deprecated_file_header = true);

statement ok
CREATE TABLE s12(
    id int,
    code string,
    timestamp bigint,
    xfas struct<device_model_id int, device_make_id int, ip string>[],
    contacts struct<emails string[], phones string[]>,
    jsonb jsonb)
WITH (
    connector = 'kafka',
    topic = 'json_c',
    properties.bootstrap.server = 'message_queue:29092',
    scan.startup.mode = 'earliest')
FORMAT PLAIN ENCODE JSON;

# we cannot create maxwell source without pk
statement error
create table s13 (
  id integer,
  name varchar,
  is_adult integer,
  birthday timestamp
) with (
  connector = 'kafka',
  topic = 'maxwell_json',
  properties.bootstrap.server = 'message_queue:29092'
) FORMAT MAXWELL ENCODE JSON;

statement ok
create table s13 (
  id integer,
  name varchar,
  is_adult integer,
  birthday timestamp,
  PRIMARY KEY (id)
) with (
  connector = 'kafka',
  topic = 'maxwell_json',
  properties.bootstrap.server = 'message_queue:29092'
) FORMAT MAXWELL ENCODE JSON;

statement ok
create table s14 (
  id integer,
  name varchar,
  is_adult bool,
  reg_time timestamp,
	balance decimal,
	win_rate double,
  PRIMARY KEY (id)
) with (
  connector = 'kafka',
  topic = 'cannal_json',
  properties.bootstrap.server = 'message_queue:29092'
) FORMAT CANAL ENCODE JSON;

statement ok
create table s15 (
  v1 int,
  v2 int,
  PRIMARY KEY (v1)
) with (
  connector = 'kafka',
  topic = 'canal_json_double_field',
  properties.bootstrap.server = 'message_queue:29092'
) FORMAT CANAL ENCODE JSON;

statement ok
create table s16 (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_4_partition_topic_with_100_message',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'latest'
) FORMAT PLAIN ENCODE JSON

statement ok
create source s18 with (
  connector = 'kafka',
  topic = 'avro_complex_schema_bin',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE AVRO (schema.location = 'file:///risingwave/avro-complex-schema.avsc', with_deprecated_file_header = true);

# we cannot use confluent schema registry when connector is not kafka
statement error
create table s19
with (
  connector = 'kinesis',
  topic = 'topic',
  properties.bootstrap.server = 'message_queue:29092'
) FORMAT PLAIN ENCODE AVRO (schema.registry = 'http://127.0.0.1:8081');

# create debezium source with generated column
statement ok
create table s20 (
  id integer primary key,
  first_name varchar,
  last_name varchar,
  email varchar,
  gen_id integer as id+1
) with (
  connector = 'kafka',
  topic = 'debezium_log',
  properties.bootstrap.server = 'message_queue:29092'
) FORMAT DEBEZIUM ENCODE JSON

# create kafka source table with generated column
statement ok
create table s21 (v1 int as v2-1, v2 int, v3 int as v2+1) with (
  connector = 'kafka',
  topic = 'kafka_4_partition_topic_generated_columns',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE JSON

# create kafka source with generated column
statement ok
create source s22 (v1 int as v2-1, v2 int, v3 int as v2+1, p timestamptz as proctime()) with (
  connector = 'kafka',
  topic = 'kafka_4_partition_topic_generated_columns',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE JSON

statement ok
create source s23 (bytea_example bytea) with (
  connector = 'kafka',
  topic = 'json_bytea',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE JSON

statement ok
create source s24 (id bytea) with (
  connector = 'kafka',
  topic = 'kafka_source_format_bytes',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE BYTES

# bytes format only accept one column
statement error
create source s25 (v1 bytea, v2 int) with (
  connector = 'kafka',
  topic = 'kafka_source_format_bytes',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE BYTES

# bytes format only accept bytea type
statement error
create source s26 (id int) with (
  connector = 'kafka',
  topic = 'kafka_source_format_bytes',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE BYTES

statement ok
create table s27 with (
  connector = 'kafka',
  topic = 'json_schema_without_schema',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE JSON (schema.location = 'file:///risingwave/json-complex-schema')

# currently _rw_key can be set as primary key
# statement ok
# create table s28 (id bytea, PRIMARY KEY(_rw_key)) with (
#   connector = 'kafka',
#   topic = 'kafka_source_format_bytes',
#   properties.bootstrap.server = 'message_queue:29092',
#   scan.startup.mode = 'earliest'
# ) FORMAT PLAIN ENCODE BYTES

# throttle option
statement ok
create table s29 (id bytea) with (
  connector = 'kafka',
  topic = 'kafka_source_format_bytes',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest',
  source_rate_limit = 200
) FORMAT PLAIN ENCODE BYTES

statement ok
CREATE TABLE mongo_customers (
	_id BIGINT PRIMARY KEY,
	payload jsonb
)
WITH (
	connector = 'kafka',
  properties.bootstrap.server = 'message_queue:29092',
	topic = 'debezium_mongo_json_customers')
FORMAT DEBEZIUM_MONGO ENCODE JSON

statement ok
CREATE TABLE mongo_customers_strong_schema (
	_id BIGINT PRIMARY KEY,
	email VARCHAR,
	first_name VARCHAR,
	last_name VARCHAR
)
WITH (
	connector = 'kafka',
	properties.bootstrap.server = 'message_queue:29092',
	topic = 'debezium_mongo_json_customers'
)
FORMAT DEBEZIUM_MONGO ENCODE JSON ( strong_schema = true )

statement ok
CREATE TABLE mongo_customers_no_schema_field (
	_id BIGINT PRIMARY KEY,
	payload jsonb
)
WITH (
	connector = 'kafka',
  properties.bootstrap.server = 'message_queue:29092',
	topic = 'debezium_mongo_json_customers_no_schema_field')
FORMAT DEBEZIUM_MONGO ENCODE JSON

statement ok
CREATE TABLE mongo_customers_no_schema_field_strong_schema (
	_id BIGINT PRIMARY KEY,
	email VARCHAR,
	first_name VARCHAR,
	last_name VARCHAR
)
WITH (
	connector = 'kafka',
	properties.bootstrap.server = 'message_queue:29092',
	topic = 'debezium_mongo_json_customers_no_schema_field'
)
FORMAT DEBEZIUM_MONGO ENCODE JSON ( strong_schema = true )

statement ok
CREATE TABLE upsert_students_default_key (
    "ID" INT,
    "firstName" VARCHAR,
    "lastName" VARCHAR,
    age INT,
    height REAL,
    weight REAL,
    primary key (rw_key)
)
INCLUDE KEY AS rw_key
WITH (
	connector = 'kafka',
  properties.bootstrap.server = 'message_queue:29092',
	topic = 'upsert_json')
FORMAT UPSERT ENCODE JSON

statement ok
CREATE TABLE upsert_students (
    "ID" INT,
    "firstName" VARCHAR,
    "lastName" VARCHAR,
    age INT,
    height REAL,
    weight REAL,
    primary key (rw_key)
)
INCLUDE KEY AS rw_key
WITH (
	connector = 'kafka',
  properties.bootstrap.server = 'message_queue:29092',
	topic = 'upsert_json')
FORMAT UPSERT ENCODE JSON

statement ok
CREATE TABLE dbz_ignore_case_json (
   L_ORDERKEY BIGINT,
   L_PARTKEY BIGINT,
   L_SUPPKEY BIGINT,
   L_LINENUMBER BIGINT,
   L_QUANTITY DECIMAL,
   L_EXTENDEDPRICE DECIMAL,
   L_DISCOUNT DECIMAL,
   L_TAX DECIMAL,
   L_RETURNFLAG VARCHAR,
   L_LINESTATUS VARCHAR,
   L_SHIPDATE DATE,
   L_COMMITDATE DATE,
   L_RECEIPTDATE DATE,
   L_SHIPINSTRUCT VARCHAR,
   L_SHIPMODE VARCHAR,
   L_COMMENT VARCHAR,
   PRIMARY KEY(L_ORDERKEY, L_LINENUMBER)
) WITH (
    connector = 'kafka',
    properties.bootstrap.server = 'message_queue:29092',
    topic = 'debezium_ignore_case_json'
) FORMAT DEBEZIUM ENCODE JSON

# create kafka source with additional rdkafka properties
statement ok
create table source_with_rdkafka_props (v1 int, v2 varchar) with (
  connector = 'kafka',
  topic = 'kafka_1_partition_topic',
  properties.bootstrap.server = 'message_queue:29092',
  scan.startup.mode = 'earliest',
  properties.queued.min.messages = 10000,
  properties.queued.max.messages.kbytes = 65536
) FORMAT PLAIN ENCODE JSON

statement ok
CREATE TABLE debezium_ignore_key (
  order_id int primary key
) with (
  connector = 'kafka',
  topic = 'debezium_mess_key',
  kafka.brokers = 'message_queue:29092',
  kafka.scan.startup.mode = 'earliest')
FORMAT DEBEZIUM ENCODE JSON (
  ignore_key = 'true'
)

statement error INCLUDE payload is only allowed when using ENCODE JSON, but got ENCODE Bytes
CREATE TABLE test_include_payload (a bytea)
INCLUDE payload
WITH (
    connector = 'kafka',
    topic = 'kafka_1_partition_topic',
    properties.bootstrap.server = 'message_queue:29092',
    scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE BYTES

statement ok
CREATE TABLE test_include_payload (v1 int, v2 varchar)
INCLUDE payload
WITH (
    connector = 'kafka',
    topic = 'kafka_1_partition_topic',
    properties.bootstrap.server = 'message_queue:29092',
    scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE JSON

statement ok
CREATE TABLE test_include_payload_only
INCLUDE payload
WITH (
    connector = 'kafka',
    topic = 'kafka_1_partition_topic',
    properties.bootstrap.server = 'message_queue:29092',
    scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE JSON

statement ok
flush;

# Wait enough time to ensure SourceExecutor consumes all Kafka data.
sleep 1s

query IT rowsort
select v1, v2 from t0;
----
1 1
2 22
3 333
4 4444

query ITT rowsort
select v1, v2, _rw_kafka_payload from test_include_payload;
----
1 1 {"v1": 1, "v2": "1"}
2 22 {"v1": 2, "v2": "22"}
3 333 {"v1": 3, "v2": "333"}
4 4444 {"v1": 4, "v2": "4444"}

query IT rowsort
select v1, v2 from s0;
----
1 1
2 22
3 333
4 4444

statement ok
drop source s0

statement ok
drop table t0

query IT rowsort
select * from s1
----
1 1
2 22
3 333
4 4444

statement ok
drop table s1

query IT rowsort
select * from s2
----
1 1
2 22
3 333
4 4444

statement ok
drop table s2

query IT rowsort
select * from s3
----
1 1
2 22
3 333
4 4444

statement ok
drop table s3

query IT rowsort
select * from s4
----
1 1
2 22
3 333
4 4444

statement ok
drop table s4

query I
select count(*) from s5
----
100

# we only support at-least-once semantic, count distinct to make result deterministic in recovery test
query I
select count(distinct (v1,v2)) from s7
----
100

statement ok
drop sink si

statement ok
drop table s5

statement ok
drop table s7

query I
select count(*) from s6
----
20

statement ok
create materialized view source_mv1 as select * from s6;

statement ok
create materialized view source_mv2 as select sum(v1) as sum_v1, count(v2) as count_v2 from s6 where v1 > 3;

statement ok
create materialized view source_mv3 as select * from s24;

# Wait for source
sleep 10s

# Flush into storage
statement ok
flush;

query IT rowsort
select v1, v2 from source_mv1;
----
0 name5
0 name9
1 name0
1 name7
2 name0
2 name3
3 name2
3 name9
4 name6
4 name7
5 name3
5 name8
6 name3
6 name4
7 name0
7 name5
8 name8
8 name9
9 name2
9 name2

query II
select sum_v1, count_v2 from source_mv2;
----
78 12

query ITTT rowsort
select id, first_name, last_name, email from s8;
----
1001 Sally Thomas sally.thomas@acme.com
1002 George Bailey gbailey@foobar.com
1003 Edward Walker ed@walker.com
1004 Anne1 Kretchmar annek@noanswer.org
1005 add add2 add

query ITTT rowsort
select id, first_name, last_name, email from s8_no_schema_field;
----
1001 Sally Thomas sally.thomas@acme.com
1002 George Bailey gbailey@foobar.com
1003 Edward Walker ed@walker.com
1004 Anne1 Kretchmar annek@noanswer.org
1005 add add2 add

query IITFFBTT
select id, sequence_id, name, score, avg_score, is_lasted, entrance_date, birthday, passed from s9;
----
32 64 str_value 32 64 t 1970-01-01 1970-01-01 00:00:00+00:00 1 mon 1 day 00:00:01

query ITITT
select id, code, timestamp, xfas, contacts, sex from s10;
----
100 abc 1473305798 {"(0,200,10.0.0.1)","(1,400,10.0.0.2)"} ("{1xxx,2xxx}","{1xxx,2xxx}") MALE

query ITITT
select id, code, timestamp, xfas, contacts, jsonb from s12;
----
100 abc 1473305798 {"(0,200,10.0.0.1)","(1,400,10.0.0.2)"} ("{1xxx,2xxx}","{1xxx,2xxx}") {"blockNumber": 16938734}

query ITIT
select * from s13 order by id;
----
1 tom 0 2017-12-31 16:00:01
2 chi 1 1999-12-31 16:00:01

query ITTTTT
select * from s14;
----
1 mike f 2018-01-01 00:00:01 1500.62 0.65

query I
select count(*) from s15
----
2

query I
select count(*) from s16
----
0

statement error Not supported: alter source with schema registry
alter source s18 add column v10 int;

query III rowsort
select * from s21;
----
19 20 21
20 21 22
NULL NULL NULL

query III rowsort
select v1, v2, v3, p >= date '2021-01-01' from s22;
----
19 20 21 t
20 21 22 t
NULL NULL NULL t

query T rowsort
select * from s23;
----
\x31324344
\xdeadbeef

query II
SELECT
	*
FROM
	mongo_customers
ORDER BY
	_id;
----
1001    {"_id": {"$numberLong": "1001"}, "email": "sally.thomas@acme.com", "first_name": "Sally", "last_name": "Thomas"}
1002    {"_id": {"$numberLong": "1002"}, "email": "gbailey@foobar.com", "first_name": "George", "last_name": "Bailey"}
1003    {"_id": {"$numberLong": "1003"}, "email": "ed@walker.com", "first_name": "Edward", "last_name": "Walker"}
1004    {"_id": {"$numberLong": "1004"}, "email": "annek@noanswer.org", "first_name": "Anne", "last_name": "Kretchmar"}

query II
SELECT
	*
FROM
	mongo_customers_strong_schema
ORDER BY
	_id;
----
1001 sally.thomas@acme.com Sally Thomas
1002 gbailey@foobar.com George Bailey
1003 ed@walker.com Edward Walker
1004 annek@noanswer.org Anne Kretchmar


query II
SELECT
	*
FROM
	mongo_customers_no_schema_field
ORDER BY
	_id;
----
1001    {"_id": {"$numberLong": "1001"}, "email": "sally.thomas@acme.com", "first_name": "Sally", "last_name": "Thomas"}
1002    {"_id": {"$numberLong": "1002"}, "email": "gbailey@foobar.com", "first_name": "George", "last_name": "Bailey"}
1003    {"_id": {"$numberLong": "1003"}, "email": "ed@walker.com", "first_name": "Edward", "last_name": "Walker"}
1004    {"_id": {"$numberLong": "1004"}, "email": "annek@noanswer.org", "first_name": "Anne", "last_name": "Kretchmar"}


query II
SELECT
	*
FROM
	mongo_customers_no_schema_field_strong_schema
ORDER BY
	_id;
----
1001 sally.thomas@acme.com Sally Thomas
1002 gbailey@foobar.com George Bailey
1003 ed@walker.com Edward Walker
1004 annek@noanswer.org Anne Kretchmar

query II
SELECT
	"ID", "firstName", "lastName", "age", "height", "weight"
FROM
	upsert_students
ORDER BY
	"ID";
----
1	Ethan	Martinez	18	6.1	180
2	Emily	Jackson	19	5.4	110
3	Noah	Thompson	21	6.3	195
4	Emma	Brown	20	5.3	130
5	Michael	Williams	22	6.2	190
6	Leah	Davis	18	5.7	140
9	Jacob	Anderson	20	5.8	155

query II
SELECT
	"ID",
  "firstName",
  "lastName",
  "age",
  "height",
  "weight"
FROM
	upsert_students_default_key
ORDER BY
	"ID";
----
1	Ethan	Martinez	18	6.1	180
2	Emily	Jackson	19	5.4	110
3	Noah	Thompson	21	6.3	195
4	Emma	Brown	20	5.3	130
5	Michael	Williams	22	6.2	190
6	Leah	Davis	18	5.7	140
9	Jacob	Anderson	20	5.8	155

query II
select
    L_ORDERKEY,
    L_LINENUMBER
from dbz_ignore_case_json
order by
    L_ORDERKEY,
    L_LINENUMBER;
----
56165    1
56166    1
56166    2

query I
SELECT id FROM source_mv3 ORDER BY id;
----
\x6b6b
\x776561776566776566

query I
select count(*) from source_with_rdkafka_props
----
4

query TFITT
select * from s27
----
(9.5,7,12) 12.5 1 An ice sculpture {cold,ice}

query I
select count(order_id) from debezium_ignore_key
----
3

statement ok
drop materialized view source_mv1

statement ok
drop materialized view source_mv2

statement ok
drop materialized view source_mv3

statement ok
drop table s6

statement ok
drop table s8

statement ok
drop table s8_no_schema_field

statement ok
drop table s9

statement ok
drop table s10

statement ok
drop table s12

statement ok
drop table s13

statement ok
drop table s14

statement ok
drop table s15

statement ok
drop table s16

statement ok
drop source s18

statement ok
drop table s20

statement ok
drop table s21

statement ok
drop source s22

statement ok
drop source s23

statement ok
drop source s24

statement ok
drop table s27

# statement ok
# drop table s28

statement ok
drop table s29

statement ok
DROP TABLE mongo_customers;

statement ok
DROP TABLE mongo_customers_no_schema_field;

statement ok
DROP TABLE upsert_students;

statement ok
DROP TABLE upsert_students_default_key;

statement ok
drop table dbz_ignore_case_json;

statement ok
drop table source_with_rdkafka_props;

statement ok
drop table debezium_ignore_key;

statement ok
drop table test_include_payload_only;

statement ok
drop table test_include_payload;

statement ok
SET streaming_use_shared_source TO true;
