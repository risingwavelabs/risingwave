# 1. Setup: Create a table and fill with enough data (e.g., 100k rows)
statement ok
CREATE TABLE t1 (v1 int);

statement ok
INSERT INTO t1 SELECT generate_series FROM generate_series(1, 100000);

statement ok
FLUSH;

# 2. Configure: Set sink to run in background
statement ok
SET sink_decouple = true;

statement ok
SET background_ddl = true;

# 3. Execution: Create Sink with specific rate limit (1000 rows/sec)
# Using blackhole connector to isolate backfill speed from network I/O
statement ok
CREATE SINK s1 FROM t1 WITH (connector='blackhole', backfill_rate_limit=1000);

# 4. Verification Loop
# Capture initial progress
query T
SELECT case when processed_rows = 0 then 'ok' else processed_rows::text end FROM rw_catalog.rw_ddl_progress WHERE name = 's1';
----
ok

# Sleep for 5 seconds
sleep 5s

# Check progress again.
# Expected: ~5000 rows (1000 rows/s * 5s). 
# If it returns 100000, the limit failed.
# We check if it is within a reasonable range (1 to 20000)
query T
SELECT case when processed_rows > 0 and processed_rows < 20000 then 'ok' else processed_rows::text end FROM rw_catalog.rw_ddl_progress WHERE name = 's1';
----
ok

# 5. Dynamic Alter Test
statement ok
ALTER SINK s1 SET backfill_rate_limit = 10000;

sleep 2s

# Should have processed significantly more.
# 5000 (initial) + 20000 (10000 * 2s) = 25000 approx.
# We check if it is > 10000 and < 60000
query T
SELECT case when processed_rows > 10000 and processed_rows < 60000 then 'ok' else processed_rows::text end FROM rw_catalog.rw_ddl_progress WHERE name = 's1';
----
ok

statement ok
DROP SINK s1;

statement ok
DROP TABLE t1;