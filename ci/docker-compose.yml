services:
  # TODO: Rename this to `postgres`
  db:
    image: postgres:18-alpine
    environment:
      - POSTGRES_USER=postgres
      # Test with a password containing special characters #21943, for example 0x5c.
      # Note that 0x5c 0x74 are TWO characters. It is NOT 0x09.
      - POSTGRES_PASSWORD=post\tgres
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
    ports:
      - 5432
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    volumes:
      - ./postgres-conf/00-setup-ssl-user.sh:/docker-entrypoint-initdb.d/00-setup-ssl-user.sh
      - ./postgres-conf/entrypoint-wrapper.sh:/entrypoint-wrapper.sh
    entrypoint: ["/bin/sh", "/entrypoint-wrapper.sh"]
    command:
      ["postgres", "-c", "wal_level=logical", "-c", "max_replication_slots=50", "-c", "max_wal_senders=20", "-c", "ssl=on", "-c", "ssl_cert_file=/etc/postgresql/server.crt", "-c", "ssl_key_file=/etc/postgresql/server.key"]

  mysql:
    image: mysql:8.0
    command: --character-set-server=utf8 --collation-server=utf8_general_ci
    ports:
      - 3306
    environment:
      - MYSQL_ROOT_PASSWORD=123456
      - MYSQL_USER=mysqluser
      - MYSQL_PASSWORD=mysqlpw
    healthcheck:
      test: ["CMD-SHELL", "mysqladmin ping -h 127.0.0.1 -u root -p123456"]
      interval: 5s
      timeout: 5s
      retries: 5

  # TODO: reuse the same mysql instance for connector test and meta store
  #       after https://github.com/risingwavelabs/risingwave/issues/19783 addressed
  mysql-meta:
    image: mysql:8.0
    command: --character-set-server=utf8 --collation-server=utf8_general_ci
    ports:
      - 3306
    environment:
      - MYSQL_ROOT_PASSWORD=123456
      - MYSQL_USER=mysqluser
      - MYSQL_PASSWORD=mysqlpw
    healthcheck:
      test: ["CMD-SHELL", "mysqladmin ping -h 127.0.0.1 -u root -p123456"]
      interval: 5s
      timeout: 5s
      retries: 5

  message_queue:
    image: "redpandadata/redpanda:latest"
    command:
      - redpanda
      - start
      - "--smp"
      - "1"
      - "--reserve-memory"
      - 0M
      - "--memory"
      - 4G
      - "--overprovisioned"
      - "--node-id"
      - "0"
      - "--check=false"
      - "--kafka-addr"
      - "PLAINTEXT://0.0.0.0:29092,OUTSIDE://0.0.0.0:9092"
      - "--advertise-kafka-addr"
      - "PLAINTEXT://message_queue:29092,OUTSIDE://localhost:9092"
    expose:
      - "29092"
      - "9092"
      - "9644"
    ports:
      - "29092:29092"
      - "9092:9092"
      - "9644:9644"
      # Don't use Redpanda's schema registry, use the separated service instead
      # - "8081:8081"
    environment: {}
    healthcheck:
      test: curl -f localhost:9644/v1/status/ready
      interval: 1s
      timeout: 5s
      retries: 5

  message_queue_sasl_1:
    image: redpandadata/redpanda:latest
    ports:
      - "19092:19092"
    entrypoint:
      - /bin/sh
      - -c
      - |
        rpk redpanda start --mode dev-container --node-id 0 \
          --smp 1 --memory 1G \
          --set redpanda.kafka_api[0].name=SASL_PLAINTEXT \
          --set redpanda.kafka_api[0].address=0.0.0.0 \
          --set redpanda.kafka_api[0].port=19092 \
          --set redpanda.kafka_api[0].authentication_method=sasl \
          --set redpanda.advertised_kafka_api[0].name=SASL_PLAINTEXT \
          --set redpanda.advertised_kafka_api[0].address=message_queue_sasl_1 \
          --set redpanda.advertised_kafka_api[0].port=19092 \
          --set redpanda.enable_sasl=true \
          --set redpanda.sasl_mechanisms='["SCRAM","PLAIN"]' \
          --set redpanda.superusers='["dev"]' &
        # Wait for the cluster to be ready
        until rpk cluster health --watch --exit-when-healthy; do sleep 1; done
        # Create the user
        rpk acl user create dev -p rw
        # Wait for the redpanda process
        wait
    healthcheck:
      test: curl -f localhost:9644/v1/status/ready
      interval: 1s
      timeout: 5s
      retries: 5

  message_queue_sasl_2:
    image: redpandadata/redpanda:latest
    ports:
      - "19093:19093"
    entrypoint:
      - /bin/sh
      - -c
      - |
        rpk redpanda start --mode dev-container --node-id 0 \
          --smp 1 --memory 1G \
          --set redpanda.kafka_api[0].name=SASL_PLAINTEXT \
          --set redpanda.kafka_api[0].address=0.0.0.0 \
          --set redpanda.kafka_api[0].port=19093 \
          --set redpanda.kafka_api[0].authentication_method=sasl \
          --set redpanda.advertised_kafka_api[0].name=SASL_PLAINTEXT \
          --set redpanda.advertised_kafka_api[0].address=message_queue_sasl_2 \
          --set redpanda.advertised_kafka_api[0].port=19093 \
          --set redpanda.enable_sasl=true \
          --set redpanda.sasl_mechanisms='["SCRAM","PLAIN"]' \
          --set redpanda.superusers='["dev1"]' &
        # Wait for the cluster to be ready
        until rpk cluster health --watch --exit-when-healthy; do sleep 1; done
        # Create the user
        rpk acl user create dev1 -p rw
        # Wait for the redpanda process
        wait
    healthcheck:
      test: curl -f localhost:9644/v1/status/ready
      interval: 1s
      timeout: 5s
      retries: 5

  rabbitmq-server:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"
      - "15672:15672"
      - "1883:1883"
    environment:
      - RABBITMQ_DEFAULT_USER=guest
      - RABBITMQ_DEFAULT_PASS=guest
    volumes:
      - ./rabbitmq-conf/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro
      - ./rabbitmq-conf/enabled_plugins:/etc/rabbitmq/enabled_plugins:ro
      - ./rabbitmq-conf/advanced.config:/etc/rabbitmq/advanced.config:ro
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_port_connectivity"]
      interval: 10s
      timeout: 5s
      retries: 5

  source-test-env:
    image: public.ecr.aws/w1p7b4n3/rw-build-env:${BUILD_ENV_VERSION:?}
    depends_on:
      - mysql
      - mysql-meta
      - sqlserver-server
      - db
      - message_queue
      - schemaregistry
      - pulsar-server
      - mongodb
      - mongodb-setup
      - nats-server
      - vault-server
      - message_queue_sasl_1
      - message_queue_sasl_2
      - rabbitmq-server
    volumes:
      - ..:/risingwave
    stop_grace_period: 30s

  backward-compatibility-test-env:
    image: public.ecr.aws/w1p7b4n3/rw-build-env:${BUILD_ENV_VERSION:?}
    depends_on:
      - mysql
      - mysql-meta
      - sqlserver-server
      - db
      - message_queue
      - schemaregistry
      - pulsar-server
      - mongodb
      - mongodb-setup
    volumes:
      - ..:/risingwave
    stop_grace_period: 30s

  sink-test-env:
    image: public.ecr.aws/w1p7b4n3/rw-build-env:${BUILD_ENV_VERSION:?}
    depends_on:
      - mysql
      - mysql-meta
      - db
      - message_queue
      - schemaregistry
      - elasticsearch
      - clickhouse-server
      - redis-server
      - pulsar-server
      - mqtt-server
      - cassandra-server
      - doris-server
      - starrocks-fe-server
      - starrocks-be-server
      - mongodb
      - mongodb-setup
      - sqlserver-server
    volumes:
      - ..:/risingwave
    stop_grace_period: 30s

  iceberg-test-env:
    image: public.ecr.aws/w1p7b4n3/rw-build-env:${BUILD_ENV_VERSION:?}
    depends_on:
      - mysql
      - db
      - message_queue
      - schemaregistry
    volumes:
      - ..:/risingwave
    stop_grace_period: 30s

  ldap-test-env:
    image: public.ecr.aws/w1p7b4n3/rw-build-env:${BUILD_ENV_VERSION:?}
    depends_on:
      ldap-server:
        condition: service_healthy
    volumes:
      - ..:/risingwave
    stop_grace_period: 30s

  rw-build-env:
    image: public.ecr.aws/w1p7b4n3/rw-build-env:${BUILD_ENV_VERSION:?}
    volumes:
      - ..:/risingwave
    stop_grace_period: 30s

  # Standard environment for CI, including MySQL and Postgres for metadata.
  ci-standard-env:
    image: public.ecr.aws/w1p7b4n3/rw-build-env:${BUILD_ENV_VERSION:?}
    depends_on:
      - mysql-meta
      - db
    volumes:
      - ..:/risingwave
    stop_grace_period: 30s

  iceberg-engine-env:
    image: public.ecr.aws/w1p7b4n3/rw-build-env:${BUILD_ENV_VERSION:?}
    depends_on:
      - db
    volumes:
      - ..:/risingwave
    stop_grace_period: 30s

  ci-flamegraph-env:
    image: public.ecr.aws/w1p7b4n3/rw-build-env:${BUILD_ENV_VERSION:?}
    # NOTE(kwannoel): This is used in order to permit
    # syscalls for `nperf` (perf_event_open),
    # so it can do CPU profiling.
    # These options should NOT be used for other services.
    privileged: true
    userns_mode: host
    volumes:
      - ..:/risingwave

  regress-test-env:
    image: public.ecr.aws/w1p7b4n3/rw-build-env:${BUILD_ENV_VERSION:?}
    depends_on:
      db:
        condition: service_healthy
    volumes:
      - ..:/risingwave

  release-env-x86:
    # Build binaries on a earlier Linux distribution (therefore with earlier version GLIBC)
    # `manylinux_2_28` is based on AlmaLinux 8 with GLIBC 2.28.
    #
    # GLIBC versions on some systems:
    # - Amazon Linux 2023 (AL2023): 2.34
    # - Ubuntu 20.04: 2.31
    #
    # Systems that we don't provide support for:
    # - Ubuntu 18.04: 2.27 (Already EOL 2023-05-31)
    # - Amazon Linux 2: 2.26 (Originally EOL 2023-06-30, superseded by AL2023)
    image: quay.io/pypa/manylinux_2_28_x86_64:2025.03.23-1
    working_dir: /mnt
    volumes:
      - ..:/mnt

  release-env-arm:
    image: quay.io/pypa/manylinux_2_28_aarch64:2025.03.23-1
    working_dir: /mnt
    volumes:
      - ..:/mnt

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.11.0
    environment:
      - xpack.security.enabled=true
      - discovery.type=single-node
      - ELASTIC_PASSWORD=risingwave
    ports:
      - 9200:9200

  clickhouse-server:
    image: clickhouse/clickhouse-server:25.5
    ports:
      - "8123:8123"
      - "9000:9000"
      - "9004:9004"
    expose:
      - 9009
    environment:
      CLICKHOUSE_USER: "default"
      CLICKHOUSE_PASSWORD: "default"

  redis-server:
    image: "redis:latest"
    expose:
      - 6379
    ports:
      - 6378:6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50

  cassandra-server:
    image: cassandra:4.0
    ports:
      - 9042:9042
    environment:
      - CASSANDRA_CLUSTER_NAME=cloudinfra

  doris-server:
    image: apache/doris:doris-all-in-one-2.1.0
    ports:
      - 8030:8030
      - 8040:8040
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9030"]
      interval: 5s
      timeout: 5s
      retries: 30

  sqlserver-server:
    image: mcr.microsoft.com/mssql/server:2022-latest
    ports:
      - 1433:1433
    environment:
      ACCEPT_EULA: "Y"
      SA_PASSWORD: "SomeTestOnly@SA"
      MSSQL_AGENT_ENABLED: "true"

  starrocks-fe-server:
    image: starrocks/fe-ubuntu:3.1.7
    command: /opt/starrocks/fe/bin/start_fe.sh
    ports:
      - 28030:8030
      - 29020:9020
      - 29030:9030
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9030"]
      interval: 5s
      timeout: 5s
      retries: 30

  starrocks-be-server:
    image: starrocks/be-ubuntu:3.1.7
    command:
      - /bin/bash
      - -c
      - |
        sleep 15s; mysql --connect-timeout 2 -h starrocks-fe-server -P9030 -uroot -e "alter system add backend \"starrocks-be-server:9050\";"
        /opt/starrocks/be/bin/start_be.sh
    ports:
      - 28040:8040
      - 29050:9050
    depends_on:
      - starrocks-fe-server

  # # Temporary workaround for json schema registry test since redpanda only supports
  # # protobuf/avro schema registry. Should be removed after the support.
  # # Related tracking issue:
  # # https://github.com/redpanda-data/redpanda/issues/1878
  schemaregistry:
    image: confluentinc/cp-schema-registry:latest
    depends_on:
      - message_queue
    ports:
      - "8082:8082"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schemaregistry
      SCHEMA_REGISTRY_LISTENERS: http://schemaregistry:8082
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: message_queue:29092

  pulsar-server:
    image: apachepulsar/pulsar:latest
    command: bin/pulsar standalone
    ports:
      - "6650:6650"
      - "6651:8080"
    expose:
      - "8080"
      - "6650"
    healthcheck:
      test: ["CMD-SHELL", "bin/pulsar-admin brokers healthcheck"]
      interval: 5s
      timeout: 5s
      retries: 5

  mongodb:
    image: mongo:8
    ports:
      - "27017"
    command: --replSet rs0 --oplogSize 128
    restart: always
    healthcheck:
      test: "echo 'db.runCommand({ping: 1})' | mongosh"
      interval: 5s
      timeout: 10s
      retries: 3

  mongodb-setup:
    image: mongo:8
    depends_on:
      - mongodb
    entrypoint:
      [
        "bash",
        "-c",
        "sleep 10 && mongosh --host mongodb:27017 /config-replica.js && sleep 10",
      ]
    restart: "no"
    volumes:
      - ./mongodb/config-replica.js:/config-replica.js

  mqtt-server:
    image: eclipse-mosquitto
    command:
      - sh
      - -c
      - echo "running command"; printf 'allow_anonymous true\nlistener 1883 0.0.0.0' > /mosquitto/config/mosquitto.conf; echo "starting service..."; cat /mosquitto/config/mosquitto.conf;/docker-entrypoint.sh;/usr/sbin/mosquitto -c /mosquitto/config/mosquitto.conf
    ports:
      - 1883:1883
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "(mosquitto_sub -h localhost -p 1883 -t 'topic' -E -i probe 2>&1 | grep Error) && exit 1 || exit 0",
        ]
      interval: 10s
      timeout: 10s
      retries: 6
  nats-server:
    image: nats:latest
    command: ["-js"]
    ports:
      - "4222:4222"
      - "8222:8222"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8222/healthz"]
      interval: 10s
      timeout: 5s
      retries: 3

  vault-server:
    image: hashicorp/vault:1.20
    ports:
      - "8200:8200"
    environment:
      - VAULT_DEV_ROOT_TOKEN_ID=root-token
      - VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200
      - VAULT_ADDR=http://0.0.0.0:8200
    cap_add:
      - IPC_LOCK
    command: vault server -dev -dev-listen-address=0.0.0.0:8200
    healthcheck:
      test: ["CMD", "vault", "status"]
      interval: 5s
      timeout: 5s
      retries: 5

  ldap-server:
    image: osixia/openldap:1.5.0
    hostname: ldap-server
    environment:
      LDAP_DISABLE_CHOWN: "true"
      LDAP_ORGANISATION: "Example Corp"
      LDAP_DOMAIN: "example.com"
      LDAP_BASE_DN: "dc=example,dc=com"
      LDAP_ADMIN_PASSWORD: "admin123"
      LDAP_CONFIG_PASSWORD: "config123"
      LDAP_READONLY_USER: "false"
      LDAP_RFC2307BIS_SCHEMA: "false"
      LDAP_BACKEND: "mdb"
      LDAP_REMOVE_CONFIG_AFTER_SETUP: "false"
      LDAP_TLS: "true"
      LDAP_TLS_CRT_FILENAME: "server.crt"
      LDAP_TLS_KEY_FILENAME: "server.key"
      LDAP_TLS_CA_CRT_FILENAME: "ca.crt"
      LDAP_TLS_DH_PARAM_FILENAME: "dhparam.pem"
      LDAP_TLS_ENFORCE: "false"
      LDAP_TLS_VERIFY_CLIENT: "try"
      LDAP_TLS_PROTOCOL_MIN: "3.1"
      LDAP_TLS_CIPHER_SUITE: "SECURE256:+SECURE128:-VERS-TLS-ALL:+VERS-TLS1.2:-RSA:-DHE-DSS:-CAMELLIA-128-CBC:-CAMELLIA-256-CBC"
    ports:
      - "389:389"
      - "636:636"
    volumes:
      - ./ldap-test/ldif:/workspace/ldif
      - ./ldap-test/certs:/container/service/slapd/assets/certs
      - ./ldap-test/setup-ldap-certs.sh:/workspace/setup-ldap-certs.sh
    entrypoint:
      - /bin/bash
      - -c
      - |
        set -euo pipefail

        echo "=== Generating certificates ==="
        export CERT_DIR=/container/service/slapd/assets/certs
        bash /workspace/setup-ldap-certs.sh

        echo "=== Importing LDIF files ==="
        mkdir -p /container/service/slapd/assets/config/bootstrap/ldif/custom
        cp -r /workspace/ldif/* /container/service/slapd/assets/config/bootstrap/ldif/custom/

        echo "=== Certificates ready, starting OpenLDAP ==="
        exec /container/tool/run --loglevel debug
    healthcheck:
      test: ["CMD-SHELL", "ldapsearch -x -H ldap://localhost:389 -b 'dc=example,dc=com' -D 'cn=admin,dc=example,dc=com' -w 'admin123' >/dev/null 2>&1"]
      interval: 5s
      timeout: 10s
      retries: 10
      start_period: 10s
