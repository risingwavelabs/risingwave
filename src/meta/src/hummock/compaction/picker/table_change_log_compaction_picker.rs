// Copyright 2026 RisingWave Labs
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

use std::iter;

use risingwave_hummock_sdk::change_log::{EpochNewChangeLog, TableChangeLog};
use risingwave_meta_model::TableId;
use risingwave_pb::hummock::compact_task;

use crate::hummock::compaction::CompactionTask;
use crate::hummock::compaction::picker::CompactionInput;

// Table change log compaction introduction.
// `EpochNewChangeLog` are referred to segment here interchangeably.
//
// The entire table change log is split into two parts: clean part and dirty part.
// The clean part is segments that generated by previous compaction.
// - The segments in clean part are sorted by checkpoint epoch in ascending order.
// - `SStable`s within a clean part's segment are sorted.
// The dirty part is segments that is never compacted.
// - The segments in dirty part are sorted by checkpoint epoch in descending order.
// - `SStable`s within a dirty part's segment are unsorted.
//
// A table change log compaction task selects the most recent segments from the clean part (taken from the end) and the oldest segments from the dirty part (taken from the beginning) as compaction inputs.
// The compaction input includes at least one segment from the dirty part and 0 or more segments from the clean part.
// The compaction input segments are merged and compacted into a single new segment and put back to the clean part.
//

#[derive(Default)]
pub struct TableChangeLogCompactionPicker {
    dirty_ratio: f32,
    /// The minimum total size (in bytes) of the dirty part that must be included in a compaction input.
    min_compaction_size_dirty_part: u64,
    /// The maximum total size (in bytes) of the dirty part that can be included in a compaction input.
    max_compaction_size_dirty_part: u64,
    /// Limits how many unordered `SStable`s from the dirty part can be included in a compaction input.
    max_compaction_sst_count_dirty_part: u64,
    /// Limits the write amplification.
    /// Restricts the maximum size (in bytes) allowed for the most recent segment in the clean part.
    /// When exceeded, next compaction will create a clean part segment instead of merging with the previous one.
    max_compaction_size_clean_part: u64,
}

impl TableChangeLogCompactionPicker {
    pub fn new(
        dirty_ratio: f32,
        min_compaction_size_dirty_part: u64,
        max_compaction_size_dirty_part: u64,
        max_compaction_sst_count_dirty_part: u64,
        max_compaction_size_clean_part: u64,
    ) -> Self {
        Self {
            dirty_ratio,
            min_compaction_size_dirty_part,
            max_compaction_size_dirty_part,
            max_compaction_sst_count_dirty_part,
            max_compaction_size_clean_part,
        }
    }

    pub fn pick_compaction(
        &self,
        table_id: TableId,
        table_change_log: &TableChangeLog,
        compacted_table_change_log: &TableChangeLog,
    ) -> Option<CompactionTask> {
        let table_change_log_compaction_checkpoint_epoch = compacted_table_change_log
            .max_checkpoint_epoch()
            .unwrap_or(0);
        let (_, dirty_part) = table_change_log
            .split_by_checkpoint_epoch(table_change_log_compaction_checkpoint_epoch);
        let clean_part_reversed = compacted_table_change_log.iter_rev();

        let mut dirty_part_subset_file_size = 0;
        let mut dirty_part_subset_sst_count = 0;
        let mut dirty_part_subset_segment_count = 0;
        for segment in dirty_part.clone() {
            let size_delta = part_file_size(iter::once(segment));
            let sst_count_delta = segment.new_value.len() + segment.old_value.len();
            // At least one segment from the dirty part must be selected.
            if dirty_part_subset_segment_count != 0
                && (dirty_part_subset_file_size + size_delta > self.max_compaction_size_dirty_part
                    || dirty_part_subset_sst_count + sst_count_delta
                        > self.max_compaction_sst_count_dirty_part as usize)
            {
                break;
            }
            dirty_part_subset_file_size += size_delta;
            dirty_part_subset_sst_count += sst_count_delta;
            dirty_part_subset_segment_count += 1;
        }
        if dirty_part_subset_segment_count == 0
            || dirty_part_subset_file_size < self.min_compaction_size_dirty_part
        {
            return None;
        }

        let mut clean_part_subset_file_size = 0;
        let mut clean_part_subset_segment_count = 0;
        for segment in clean_part_reversed.clone() {
            let size_delta = part_file_size(iter::once(segment));
            // It's allowed to select 0 segments from the clean part.
            if clean_part_subset_file_size + size_delta > self.max_compaction_size_clean_part {
                break;
            }
            clean_part_subset_file_size += size_delta;
            clean_part_subset_segment_count += 1;
        }
        let clean_part_subset_reversed = clean_part_reversed
            .clone()
            .take(clean_part_subset_segment_count);

        let dirty_part_file_size = part_file_size(dirty_part.clone());
        let clean_part_subset_file_size = part_file_size(clean_part_subset_reversed.clone());
        // Note: We calculate dirty_ratio using the total size of the entire dirty part (dirty_part_file_size)
        // instead of the selected subset (dirty_part_subset_file_size), since the subset is only for enforcing
        // the compaction task size limits; the overall ratio should reflect the proportion of dirty data
        // in the full change log.
        let dirty_ratio = dirty_part_file_size as f32
            / (dirty_part_file_size + clean_part_subset_file_size) as f32;
        if dirty_ratio < self.dirty_ratio {
            return None;
        }

        let mut input_table_change_logs_clean_part =
            clean_part_subset_reversed.cloned().collect::<Vec<_>>();
        input_table_change_logs_clean_part.reverse();
        let input_table_change_logs_dirty_part = dirty_part
            .take(dirty_part_subset_segment_count)
            .cloned()
            .collect();
        Some(CompactionTask {
            input: CompactionInput {
                input_table_change_logs_clean_part,
                input_table_change_logs_dirty_part,
                input_table_change_logs_table_id: table_id,
                ..Default::default()
            },
            compaction_task_type: compact_task::TaskType::TableChangeLog,
            base_level: 0,
            // TODO(ZW): Get compression algorithm from config.
            compression_algorithm: "Zstd".to_owned(),
            // TODO(ZW): Get target file size from config.
            target_file_size: 1 << 30, // 1GB
        })
    }
}

fn part_file_size(part: impl Iterator<Item = &EpochNewChangeLog> + Clone) -> u64 {
    part.map(|log| {
        log.new_value
            .iter()
            .chain(log.old_value.iter())
            .map(|s| s.file_size)
            .sum::<u64>()
    })
    .sum::<u64>()
}

#[cfg(test)]
mod tests {
    use risingwave_common::catalog::TableId;
    use risingwave_hummock_sdk::change_log::TableChangeLog;
    use risingwave_hummock_sdk::key_range::KeyRange;
    use risingwave_hummock_sdk::sstable_info::{SstableInfo, SstableInfoInner};

    use super::*;

    fn build_sstable(id: u64, size: u64, table_id: TableId) -> SstableInfo {
        SstableInfoInner {
            object_id: id.into(),
            sst_id: id.into(),
            key_range: KeyRange::inf(),
            file_size: size,
            table_ids: vec![table_id],
            uncompressed_file_size: size,
            sst_size: size,
            ..Default::default()
        }
        .into()
    }

    fn build_log(
        id: u64,
        checkpoint_epoch: u64,
        size: u64,
        table_id: TableId,
    ) -> EpochNewChangeLog {
        EpochNewChangeLog {
            new_value: vec![build_sstable(id, size, table_id)],
            old_value: vec![],
            non_checkpoint_epochs: vec![],
            checkpoint_epoch,
        }
    }

    #[test]
    fn test_basic() {
        let table_id: TableId = 1.into();
        let compacted = TableChangeLog::new(vec![
            build_log(1, 5, 4, table_id),
            build_log(2, 10, 6, table_id),
        ]);
        let full = TableChangeLog::new(vec![
            build_log(1, 5, 4, table_id),
            build_log(2, 10, 6, table_id),
            build_log(3, 11, 3, table_id),
            build_log(4, 12, 2, table_id),
        ]);

        // Pick from both clean and dirty parts.
        let picker = TableChangeLogCompactionPicker::new(0.3, 1, 20, 10, 20);
        let task = picker
            .pick_compaction(table_id, &full, &compacted)
            .expect("should pick task");
        assert_eq!(task.input.input_table_change_logs_table_id, table_id);
        let clean_part = task.input.input_table_change_logs_clean_part;
        assert_eq!(
            clean_part
                .iter()
                .map(|log| log.checkpoint_epoch)
                .collect::<Vec<_>>(),
            vec![5, 10]
        );
        let dirty_part = task.input.input_table_change_logs_dirty_part;
        assert_eq!(
            dirty_part
                .iter()
                .map(|log| log.checkpoint_epoch)
                .collect::<Vec<_>>(),
            vec![11, 12]
        );

        // Pick no tasks due to min_compaction_size_dirty_part.
        let picker = TableChangeLogCompactionPicker::new(0.3, 1000, 20, 10, 20);
        assert!(
            picker
                .pick_compaction(table_id, &full, &compacted)
                .is_none()
        );

        // Pick only from dirty part due to max_compaction_size_clean_part.
        let picker = TableChangeLogCompactionPicker::new(0.3, 1, 20, 10, 1);
        let task = picker
            .pick_compaction(table_id, &full, &compacted)
            .expect("should pick task");
        assert!(task.input.input_table_change_logs_clean_part.is_empty());
        assert_eq!(task.input.input_table_change_logs_dirty_part.len(), 2);
    }

    #[test]
    fn test_dirty_ratio() {
        let table_id: TableId = 1.into();
        let compacted = TableChangeLog::new(vec![
            build_log(1, 5, 100, table_id),
            build_log(2, 10, 100, table_id),
        ]);
        let full = TableChangeLog::new(vec![
            build_log(1, 5, 100, table_id),
            build_log(2, 10, 100, table_id),
            build_log(3, 11, 1, table_id),
        ]);

        // dirty_ratio = 1 / (1 + 200) which is below 0.005
        let picker = TableChangeLogCompactionPicker::new(0.005, 1, 200, 10, 300);
        assert!(
            picker
                .pick_compaction(table_id, &full, &compacted)
                .is_none()
        );

        // dirty_ratio = 1 / (1 + 200) which is above 0.005
        let picker = TableChangeLogCompactionPicker::new(0.004, 1, 200, 10, 300);
        assert!(
            picker
                .pick_compaction(table_id, &full, &compacted)
                .is_some()
        );
    }

    #[test]
    fn test_limit_dirty_part() {
        let table_id: TableId = 1.into();
        let compacted = TableChangeLog::new(vec![build_log(1, 5, 2, table_id)]);
        let dirty_first = EpochNewChangeLog {
            new_value: vec![build_sstable(11, 4, table_id)],
            old_value: vec![build_sstable(12, 1, table_id)],
            non_checkpoint_epochs: vec![],
            checkpoint_epoch: 6,
        };
        let dirty_second = EpochNewChangeLog {
            new_value: vec![build_sstable(13, 4, table_id)],
            old_value: vec![build_sstable(14, 1, table_id)],
            non_checkpoint_epochs: vec![],
            checkpoint_epoch: 7,
        };
        let full = TableChangeLog::new(vec![
            build_log(1, 5, 2, table_id),
            dirty_first.clone(),
            dirty_second.clone(),
        ]);

        for max_compaction_sst_count_dirty_part in 1..=3 {
            let picker = TableChangeLogCompactionPicker::new(
                0.1,
                1,
                20,
                max_compaction_sst_count_dirty_part,
                20,
            );
            let task = picker
                .pick_compaction(table_id, &full, &compacted)
                .expect("should pick task");
            let dirty_part = task.input.input_table_change_logs_dirty_part;
            assert_eq!(dirty_part.len(), 1);
            assert_eq!(dirty_part[0].checkpoint_epoch, dirty_first.checkpoint_epoch,);
        }

        let picker = TableChangeLogCompactionPicker::new(0.1, 1, 20, 4, 20);
        let task = picker
            .pick_compaction(table_id, &full, &compacted)
            .expect("should pick task");
        let dirty_part = task.input.input_table_change_logs_dirty_part;
        assert_eq!(dirty_part.len(), 2);
        assert_eq!(
            dirty_part[1].checkpoint_epoch,
            dirty_second.checkpoint_epoch,
        );

        for max_compaction_size_dirty_part in [1, 5, 9] {
            let picker =
                TableChangeLogCompactionPicker::new(0.1, 1, max_compaction_size_dirty_part, 4, 20);
            let task = picker
                .pick_compaction(table_id, &full, &compacted)
                .expect("should pick task");
            let dirty_part = task.input.input_table_change_logs_dirty_part;
            assert_eq!(dirty_part.len(), 1);
            assert_eq!(dirty_part[0].checkpoint_epoch, dirty_first.checkpoint_epoch,);
        }
    }
}
