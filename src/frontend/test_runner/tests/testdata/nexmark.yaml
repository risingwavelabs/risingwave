# This file is automatically generated. See `src/frontend/test_runner/README.md` for more information.
- id: create_tables
  sql: |
    CREATE TABLE person (
        id BIGINT,
        name VARCHAR,
        emailAddress VARCHAR,
        creditCard VARCHAR,
        city VARCHAR,
        state VARCHAR,
        dateTime TIMESTAMP
    );

    CREATE TABLE auction (
        id BIGINT,
        itemName VARCHAR,
        description VARCHAR,
        initialBid BIGINT,
        reserve BIGINT,
        dateTime TIMESTAMP,
        expires TIMESTAMP,
        seller BIGINT,
        category BIGINT
    );

    CREATE TABLE bid (
        auction BIGINT,
        bidder BIGINT,
        price BIGINT,
        channel VARCHAR,
        url VARCHAR,
        dateTime TIMESTAMP,
        extra VARCHAR
    );
- id: nexmark_q0
  before:
    - create_tables
  sql: |
    CREATE MATERIALIZED VIEW nexmark_q0
      AS
    SELECT auction, bidder, price, dateTime FROM bid;
- id: nexmark_q0
  before:
    - create_tables
  sql: |
    SELECT auction, bidder, price, dateTime FROM bid;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchScan { table: bid, columns: [auction, bidder, price, dateTime] }
  stream_plan: |
    StreamMaterialize { columns: [auction, bidder, price, dateTime, _row_id(hidden)], pk_columns: [_row_id] }
      StreamTableScan { table: bid, columns: [auction, bidder, price, dateTime, _row_id], pk_indices: [4] }
- id: nexmark_q1
  before:
    - create_tables
  sql: |
    SELECT
      auction,
      bidder,
      0.908 * price as price,
      dateTime
    FROM bid;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1, (0.908:Decimal * $2), $3] }
        BatchScan { table: bid, columns: [auction, bidder, price, dateTime] }
  stream_plan: |
    StreamMaterialize { columns: [auction, bidder, price, dateTime, _row_id(hidden)], pk_columns: [_row_id] }
      StreamProject { exprs: [$0, $1, (0.908:Decimal * $2), $3, $4] }
        StreamTableScan { table: bid, columns: [auction, bidder, price, dateTime, _row_id], pk_indices: [4] }
- id: nexmark_q2
  before:
    - create_tables
  sql: "SELECT auction, price FROM bid \nWHERE auction = 1007 OR auction = 1020 OR auction = 2001 OR auction = 2019 OR auction = 2087;\n"
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchFilter { predicate: ((((($0 = 1007:Int32) OR ($0 = 1020:Int32)) OR ($0 = 2001:Int32)) OR ($0 = 2019:Int32)) OR ($0 = 2087:Int32)) }
        BatchScan { table: bid, columns: [auction, price] }
  stream_plan: |
    StreamMaterialize { columns: [auction, price, _row_id(hidden)], pk_columns: [_row_id] }
      StreamFilter { predicate: ((((($0 = 1007:Int32) OR ($0 = 1020:Int32)) OR ($0 = 2001:Int32)) OR ($0 = 2019:Int32)) OR ($0 = 2087:Int32)) }
        StreamTableScan { table: bid, columns: [auction, price, _row_id], pk_indices: [2] }
- id: nexmark_q3
  before:
    - create_tables
  sql: |
    SELECT
        P.name, P.city, P.state, A.id
    FROM
        auction AS A INNER JOIN person AS P on A.seller = P.id
    WHERE
        A.category = 10 and (P.state = 'or' OR P.state = 'id' OR P.state = 'ca');
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$1, $2, $3, $0] }
        BatchHashJoin { type: Inner, predicate: $1 = $2, output_indices: [0, 3, 4, 5] }
          BatchExchange { order: [], dist: HashShard([1]) }
            BatchProject { exprs: [$0, $1] }
              BatchFilter { predicate: ($2 = 10:Int32) }
                BatchScan { table: auction, columns: [id, seller, category] }
          BatchExchange { order: [], dist: HashShard([0]) }
            BatchFilter { predicate: ((($3 = 'or':Varchar) OR ($3 = 'id':Varchar)) OR ($3 = 'ca':Varchar)) }
              BatchScan { table: person, columns: [id, name, city, state] }
  stream_plan: |
    StreamMaterialize { columns: [name, city, state, id, _row_id(hidden), _row_id#1(hidden)], pk_columns: [_row_id, _row_id#1] }
      StreamExchange { dist: HashShard([4, 5]) }
        StreamProject { exprs: [$1, $2, $3, $0, $4, $5] }
          StreamHashJoin { type: Inner, predicate: $1 = $3, output_indices: [0, 4, 5, 6, 2, 7] }
            StreamExchange { dist: HashShard([1]) }
              StreamProject { exprs: [$0, $1, $2] }
                StreamFilter { predicate: ($3 = 10:Int32) }
                  StreamTableScan { table: auction, columns: [id, seller, _row_id, category], pk_indices: [2] }
            StreamExchange { dist: HashShard([0]) }
              StreamFilter { predicate: ((($3 = 'or':Varchar) OR ($3 = 'id':Varchar)) OR ($3 = 'ca':Varchar)) }
                StreamTableScan { table: person, columns: [id, name, city, state, _row_id], pk_indices: [4] }
- id: nexmark_q4
  before:
    - create_tables
  sql: |
    SELECT
        Q.category,
        AVG(Q.final) as avg
    FROM (
        SELECT MAX(B.price) AS final, A.category
        FROM auction A, bid B
        WHERE A.id = B.auction AND B.dateTime BETWEEN A.dateTime AND A.expires
        GROUP BY A.id, A.category
    ) Q
    GROUP BY Q.category;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, ($1 / $2)] }
        BatchHashAgg { group_key: [$0], aggs: [sum($1), count($1)] }
          BatchExchange { order: [], dist: HashShard([0]) }
            BatchProject { exprs: [$1, $2] }
              BatchHashAgg { group_key: [$0, $1], aggs: [max($2)] }
                BatchProject { exprs: [$0, $3, $5] }
                  BatchFilter { predicate: ($6 >= $1) AND ($6 <= $2) }
                    BatchHashJoin { type: Inner, predicate: $0 = $4, output_indices: all }
                      BatchExchange { order: [], dist: HashShard([0]) }
                        BatchScan { table: auction, columns: [id, dateTime, expires, category] }
                      BatchExchange { order: [], dist: HashShard([0]) }
                        BatchScan { table: bid, columns: [auction, price, dateTime] }
  stream_plan: |
    StreamMaterialize { columns: [category, avg], pk_columns: [category] }
      StreamProject { exprs: [$0, ($2 / $3)] }
        StreamHashAgg { group_key: [$0], aggs: [count, sum($1), count($1)] }
          StreamExchange { dist: HashShard([0]) }
            StreamProject { exprs: [$1, $3, $0] }
              StreamHashAgg { group_key: [$0, $1], aggs: [count, max($2)] }
                StreamProject { exprs: [$0, $3, $6, $4, $8] }
                  StreamFilter { predicate: ($7 >= $1) AND ($7 <= $2) }
                    StreamHashJoin { type: Inner, predicate: $0 = $5, output_indices: all }
                      StreamExchange { dist: HashShard([0]) }
                        StreamTableScan { table: auction, columns: [id, dateTime, expires, category, _row_id], pk_indices: [4] }
                      StreamExchange { dist: HashShard([0]) }
                        StreamTableScan { table: bid, columns: [auction, price, dateTime, _row_id], pk_indices: [3] }
- id: nexmark_q5
  before:
    - create_tables
  sql: "SELECT AuctionBids.auction, AuctionBids.num\nFROM (\n    SELECT\n        bid.auction,\n        count(*) AS num, \n        window_start AS starttime\n    FROM \n        HOP(bid, dateTime, INTERVAL '2' SECOND, INTERVAL '10' SECOND)\n    GROUP BY\n        window_start, \n    bid.auction\n) AS AuctionBids\nJOIN (\n    SELECT\n        max(CountBids.num) AS maxn,\n        CountBids.starttime_c\n    FROM (\n        SELECT\n            count(*) AS num,\n            window_start AS starttime_c\n        FROM HOP(bid, dateTime, INTERVAL '2' SECOND, INTERVAL '10' SECOND)\n        GROUP BY\n            bid.auction,\n            window_start\n        ) AS CountBids\n    GROUP BY \n        CountBids.starttime_c\n    ) AS MaxBids\nON \n  AuctionBids.starttime = MaxBids.starttime_c AND\n  AuctionBids.num >= MaxBids.maxn;\n"
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1] }
        BatchFilter { predicate: ($1 >= $3) }
          BatchHashJoin { type: Inner, predicate: $2 = $4, output_indices: all }
            BatchExchange { order: [], dist: HashShard([2]) }
              BatchProject { exprs: [$1, $2, $0] }
                BatchHashAgg { group_key: [$0, $1], aggs: [count] }
                  BatchExchange { order: [], dist: HashShard([0, 1]) }
                    BatchProject { exprs: [$1, $0] }
                      BatchHopWindow { time_col: $1, slide: 00:00:02, size: 00:00:10, output_indices: [0, 2] }
                        BatchScan { table: bid, columns: [auction, dateTime] }
            BatchProject { exprs: [$1, $0] }
              BatchHashAgg { group_key: [$0], aggs: [max($1)] }
                BatchExchange { order: [], dist: HashShard([0]) }
                  BatchProject { exprs: [$1, $2] }
                    BatchHashAgg { group_key: [$0, $1], aggs: [count] }
                      BatchHopWindow { time_col: $1, slide: 00:00:02, size: 00:00:10, output_indices: [0, 2] }
                        BatchExchange { order: [], dist: HashShard([0]) }
                          BatchScan { table: bid, columns: [auction, dateTime] }
  stream_plan: |
    StreamMaterialize { columns: [auction, num, window_start(hidden), window_start#1(hidden)], pk_columns: [window_start, auction, window_start#1] }
      StreamProject { exprs: [$0, $1, $2, $4] }
        StreamFilter { predicate: ($1 >= $3) }
          StreamHashJoin { type: Inner, predicate: $2 = $4, output_indices: all }
            StreamExchange { dist: HashShard([2]) }
              StreamProject { exprs: [$1, $3, $0] }
                StreamHashAgg { group_key: [$0, $1], aggs: [count, count] }
                  StreamExchange { dist: HashShard([0, 1]) }
                    StreamProject { exprs: [$1, $0, $2] }
                      StreamHopWindow { time_col: $1, slide: 00:00:02, size: 00:00:10, output_indices: [0, 3, 2] }
                        StreamTableScan { table: bid, columns: [auction, dateTime, _row_id], pk_indices: [2] }
            StreamProject { exprs: [$2, $0] }
              StreamHashAgg { group_key: [$0], aggs: [count, max($1)] }
                StreamExchange { dist: HashShard([0]) }
                  StreamProject { exprs: [$1, $3, $0] }
                    StreamHashAgg { group_key: [$0, $1], aggs: [count, count] }
                      StreamExchange { dist: HashShard([0, 1]) }
                        StreamHopWindow { time_col: $1, slide: 00:00:02, size: 00:00:10, output_indices: [0, 3, 2] }
                          StreamTableScan { table: bid, columns: [auction, dateTime, _row_id], pk_indices: [2] }
- id: nexmark_q6
  before:
    - create_tables
  sql: |
    SELECT
        Q.seller,
        AVG(Q.final) OVER
            (PARTITION BY Q.seller ORDER BY Q.dateTime ROWS BETWEEN 10 PRECEDING AND CURRENT ROW)
        as avg
    FROM (
        SELECT MAX(B.price) AS final, A.seller, B.dateTime
        FROM auction AS A, bid AS B
        WHERE A.id = B.auction and B.dateTime between A.dateTime and A.expires
        GROUP BY A.id, A.seller
    ) AS Q;
  planner_error: 'Invalid input syntax: column must appear in the GROUP BY clause or be used in an aggregate function'
- id: nexmark_q7
  before:
    - create_tables
  sql: |
    SELECT
        B.auction,
        B.price,
        B.bidder,
        B.dateTime
    from
        bid B
        JOIN (
            SELECT
                MAX(price) AS maxprice,
                window_end as dateTime
            FROM
                TUMBLE(bid, dateTime, INTERVAL '10' SECOND)
            GROUP BY
                window_end
        ) B1 ON B.price = B1.maxprice
    WHERE
        B.dateTime BETWEEN B1.dateTime - INTERVAL '10' SECOND
        AND B1.dateTime;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $2, $1, $3] }
        BatchFilter { predicate: ($3 >= ($5 - '00:00:10':Interval)) AND ($3 <= $5) }
          BatchHashJoin { type: Inner, predicate: $2 = $4, output_indices: all }
            BatchExchange { order: [], dist: HashShard([2]) }
              BatchScan { table: bid, columns: [auction, bidder, price, dateTime] }
            BatchExchange { order: [], dist: HashShard([0]) }
              BatchProject { exprs: [$1, $0] }
                BatchHashAgg { group_key: [$0], aggs: [max($1)] }
                  BatchExchange { order: [], dist: HashShard([0]) }
                    BatchProject { exprs: [(TumbleStart($1, '00:00:10':Interval) + '00:00:10':Interval), $0] }
                      BatchScan { table: bid, columns: [price, dateTime] }
  stream_plan: |
    StreamMaterialize { columns: [auction, price, bidder, dateTime, _row_id(hidden), expr#0(hidden)], pk_columns: [_row_id, expr#0] }
      StreamExchange { dist: HashShard([4, 5]) }
        StreamProject { exprs: [$0, $2, $1, $3, $4, $6] }
          StreamFilter { predicate: ($3 >= ($6 - '00:00:10':Interval)) AND ($3 <= $6) }
            StreamHashJoin { type: Inner, predicate: $2 = $5, output_indices: all }
              StreamExchange { dist: HashShard([2]) }
                StreamTableScan { table: bid, columns: [auction, bidder, price, dateTime, _row_id], pk_indices: [4] }
              StreamExchange { dist: HashShard([0]) }
                StreamProject { exprs: [$2, $0] }
                  StreamHashAgg { group_key: [$0], aggs: [count, max($1)] }
                    StreamExchange { dist: HashShard([0]) }
                      StreamProject { exprs: [(TumbleStart($1, '00:00:10':Interval) + '00:00:10':Interval), $0, $2] }
                        StreamTableScan { table: bid, columns: [price, dateTime, _row_id], pk_indices: [2] }
- id: nexmark_q8
  before:
    - create_tables
  sql: |
    SELECT
        P.id,
        P.name,
        P.starttime
    FROM
        (
            SELECT
                id,
                name,
                window_start AS starttime,
                window_end AS endtime
            FROM
                TUMBLE(person, dateTime, INTERVAL '10' SECOND)
            GROUP BY
                id,
                name,
                window_start,
                window_end
        ) P
        JOIN (
            SELECT
                seller,
                window_start AS starttime,
                window_end AS endtime
            FROM
                TUMBLE(auction, dateTime, INTERVAL '10' SECOND)
            GROUP BY
                seller,
                window_start,
                window_end
        ) A ON P.id = A.seller
        AND P.starttime = A.starttime
        AND P.endtime = A.endtime;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchHashJoin { type: Inner, predicate: $0 = $4 AND $2 = $5 AND $3 = $6, output_indices: [0, 1, 2] }
        BatchExchange { order: [], dist: HashShard([0, 2, 3]) }
          BatchHashAgg { group_key: [$0, $1, $2, $3], aggs: [] }
            BatchExchange { order: [], dist: HashShard([0, 1, 2, 3]) }
              BatchProject { exprs: [$0, $1, TumbleStart($2, '00:00:10':Interval), (TumbleStart($2, '00:00:10':Interval) + '00:00:10':Interval)] }
                BatchScan { table: person, columns: [id, name, dateTime] }
        BatchHashAgg { group_key: [$0, $1, $2], aggs: [] }
          BatchExchange { order: [], dist: HashShard([0, 1, 2]) }
            BatchProject { exprs: [$1, TumbleStart($0, '00:00:10':Interval), (TumbleStart($0, '00:00:10':Interval) + '00:00:10':Interval)] }
              BatchScan { table: auction, columns: [dateTime, seller] }
  stream_plan: |
    StreamMaterialize { columns: [id, name, starttime, expr#3(hidden), seller(hidden), expr#1(hidden), expr#2(hidden)], pk_columns: [id, name, starttime, expr#3, seller, expr#1, expr#2] }
      StreamHashJoin { type: Inner, predicate: $0 = $5 AND $2 = $6 AND $3 = $7, output_indices: [0, 1, 2, 3, 5, 6, 7] }
        StreamExchange { dist: HashShard([0, 2, 3]) }
          StreamHashAgg { group_key: [$0, $1, $2, $3], aggs: [count] }
            StreamExchange { dist: HashShard([0, 1, 2, 3]) }
              StreamProject { exprs: [$0, $1, TumbleStart($2, '00:00:10':Interval), (TumbleStart($2, '00:00:10':Interval) + '00:00:10':Interval), $3] }
                StreamTableScan { table: person, columns: [id, name, dateTime, _row_id], pk_indices: [3] }
        StreamHashAgg { group_key: [$0, $1, $2], aggs: [count] }
          StreamExchange { dist: HashShard([0, 1, 2]) }
            StreamProject { exprs: [$1, TumbleStart($0, '00:00:10':Interval), (TumbleStart($0, '00:00:10':Interval) + '00:00:10':Interval), $2] }
              StreamTableScan { table: auction, columns: [dateTime, seller, _row_id], pk_indices: [2] }
- id: nexmark_q9
  before:
    - create_tables
  sql: |
    SELECT
        id, itemName, description, initialBid, reserve, dateTime, expires, seller, category,
        auction, bidder, price, bid_dateTime
    FROM (
      SELECT A.*, B.auction, B.bidder, B.price, B.dateTime AS bid_dateTime,
        ROW_NUMBER() OVER (PARTITION BY A.id ORDER BY B.price DESC, B.dateTime ASC) AS rownum
      FROM auction A, bid B
      WHERE A.id = B.auction AND B.dateTime BETWEEN A.dateTime AND A.expires
    )
    WHERE rownum <= 1;
  binder_error: 'Feature is not yet implemented: unsupported function: "row_number", Tracking issue: https://github.com/singularity-data/risingwave/issues/112'
- id: nexmark_q10
  before:
    - create_tables
  sql: |
    SELECT auction, bidder, price, dateTime, TO_CHAR(dateTime, 'YYYY-MM-DD') as date, TO_CHAR(dateTime, 'HH:MI') as time FROM bid;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1, $2, $3, ToChar($3, 'YYYY-MM-DD':Varchar), ToChar($3, 'HH:MI':Varchar)] }
        BatchScan { table: bid, columns: [auction, bidder, price, dateTime] }
  stream_plan: |
    StreamMaterialize { columns: [auction, bidder, price, dateTime, date, time, _row_id(hidden)], pk_columns: [_row_id] }
      StreamProject { exprs: [$0, $1, $2, $3, ToChar($3, 'YYYY-MM-DD':Varchar), ToChar($3, 'HH:MI':Varchar), $4] }
        StreamTableScan { table: bid, columns: [auction, bidder, price, dateTime, _row_id], pk_indices: [4] }
- id: nexmark_q11
  before:
    - create_tables
  sql: |
    SELECT
        B.bidder,
        count(*) as bid_count,
        SESSION_START(B.dateTime, INTERVAL '10' SECOND) as starttime,
        SESSION_END(B.dateTime, INTERVAL '10' SECOND) as endtime
    FROM bid B
    GROUP BY B.bidder, SESSION(B.dateTime, INTERVAL '10' SECOND);
  binder_error: 'Feature is not yet implemented: unsupported function: "session", Tracking issue: https://github.com/singularity-data/risingwave/issues/112'
- id: nexmark_q12
  before:
    - create_tables
  sql: |
    SELECT
        B.bidder,
        count(*) as bid_count,
        TUMBLE_START(B.p_time, INTERVAL '10' SECOND) as starttime,
        TUMBLE_END(B.p_time, INTERVAL '10' SECOND) as endtime
    FROM (SELECT *, PROCTIME() as p_time FROM bid) B
    GROUP BY B.bidder, TUMBLE(B.p_time, INTERVAL '10' SECOND);
  binder_error: 'Feature is not yet implemented: unsupported function: "proctime", Tracking issue: https://github.com/singularity-data/risingwave/issues/112'
- id: nexmark_q13
  before:
    - create_tables
  sql: |
    /* SELECT
        B.auction,
        B.bidder,
        B.price,
        B.dateTime,
        S.value
    FROM (SELECT *, PROCTIME() as p_time FROM bid) B
    JOIN side_input FOR SYSTEM_TIME AS OF B.p_time AS S
    ON mod(B.auction, 10000) = S.key; */
    select 1;
- id: nexmark_q14
  before:
    - create_tables
  sql: |
    SELECT
      auction,
      bidder,
      0.908 * price as price,
      CASE
        WHEN
          extract(hour from dateTime) >= 8 AND
          extract(hour from dateTime) <= 18
        THEN 'dayTime'
        WHEN
          extract(hour from dateTime) <= 6 OR
          extract(hour from dateTime) >= 20
        THEN 'nightTime'
        ELSE 'otherTime'
      END AS bidTimeType,
      dateTime,
      extra
      -- TODO: count_char is an UDF, add it back when we support similar functionality.
      -- https://github.com/nexmark/nexmark/blob/master/nexmark-flink/src/main/java/com/github/nexmark/flink/udf/CountChar.java
      -- count_char(extra, 'c') AS c_counts
    FROM bid
    WHERE 0.908 * price > 1000000 AND 0.908 * price < 50000000;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1, (0.908:Decimal * $2), Case(((Extract('HOUR':Varchar, $3) >= 8:Int32) AND (Extract('HOUR':Varchar, $3) <= 18:Int32)), 'dayTime':Varchar, ((Extract('HOUR':Varchar, $3) <= 6:Int32) OR (Extract('HOUR':Varchar, $3) >= 20:Int32)), 'nightTime':Varchar, 'otherTime':Varchar), $3, $4] }
        BatchFilter { predicate: ((0.908:Decimal * $2) > 1000000:Int32) AND ((0.908:Decimal * $2) < 50000000:Int32) }
          BatchScan { table: bid, columns: [auction, bidder, price, dateTime, extra] }
  stream_plan: |
    StreamMaterialize { columns: [auction, bidder, price, bidTimeType, dateTime, extra, _row_id(hidden)], pk_columns: [_row_id] }
      StreamProject { exprs: [$0, $1, (0.908:Decimal * $2), Case(((Extract('HOUR':Varchar, $3) >= 8:Int32) AND (Extract('HOUR':Varchar, $3) <= 18:Int32)), 'dayTime':Varchar, ((Extract('HOUR':Varchar, $3) <= 6:Int32) OR (Extract('HOUR':Varchar, $3) >= 20:Int32)), 'nightTime':Varchar, 'otherTime':Varchar), $3, $4, $5] }
        StreamFilter { predicate: ((0.908:Decimal * $2) > 1000000:Int32) AND ((0.908:Decimal * $2) < 50000000:Int32) }
          StreamTableScan { table: bid, columns: [auction, bidder, price, dateTime, extra, _row_id], pk_indices: [5] }
- id: nexmark_q15
  before:
    - create_tables
  sql: |
    SELECT
        TO_CHAR(dateTime, 'yyyy-MM-dd') as day,
        count(*) AS total_bids,
        count(*) filter (where price < 10000) AS rank1_bids,
        count(*) filter (where price >= 10000 and price < 1000000) AS rank2_bids,
        count(*) filter (where price >= 1000000) AS rank3_bids,
        count(distinct bidder) AS total_bidders,
        count(distinct bidder) filter (where price < 10000) AS rank1_bidders,
        count(distinct bidder) filter (where price >= 10000 and price < 1000000) AS rank2_bidders,
        count(distinct bidder) filter (where price >= 1000000) AS rank3_bidders,
        count(distinct auction) AS total_auctions,
        count(distinct auction) filter (where price < 10000) AS rank1_auctions,
        count(distinct auction) filter (where price >= 10000 and price < 1000000) AS rank2_auctions,
        count(distinct auction) filter (where price >= 1000000) AS rank3_auctions
    FROM bid
    GROUP BY to_char(dateTime, 'yyyy-MM-dd');
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchHashAgg { group_key: [$0], aggs: [sum($2) filter(($1 = 8:Int64)), sum($3) filter(($1 = 8:Int64)), sum($4) filter(($1 = 8:Int64)), sum($5) filter(($1 = 8:Int64)), sum($6) filter(($1 = 0:Int64)), sum($7) filter(($1 = 1:Int64)), sum($8) filter(($1 = 2:Int64)), sum($9) filter(($1 = 3:Int64)), sum($10) filter(($1 = 4:Int64)), sum($11) filter(($1 = 5:Int64)), sum($12) filter(($1 = 6:Int64)), sum($13) filter(($1 = 7:Int64))] }
        BatchExchange { order: [], dist: HashShard([0]) }
          BatchProject { exprs: [$0, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19, $20, $21] }
            BatchHashAgg { group_key: [$0, $2, $2, $2, $2, $3, $3, $3, $3, $4], aggs: [count, count filter(($1 < 10000:Int32)), count filter((($1 >= 10000:Int32) AND ($1 < 1000000:Int32))), count filter(($1 >= 1000000:Int32)), count($2), count($2) filter(($1 < 10000:Int32)), count($2) filter((($1 >= 10000:Int32) AND ($1 < 1000000:Int32))), count($2) filter(($1 >= 1000000:Int32)), count($3), count($3) filter(($1 < 10000:Int32)), count($3) filter((($1 >= 10000:Int32) AND ($1 < 1000000:Int32))), count($3) filter(($1 >= 1000000:Int32))] }
              BatchExchange { order: [], dist: HashShard([0, 2, 3, 4]) }
                BatchExpand { column_subsets: [[0, 2], [0, 2, 1], [0, 2, 1, 1], [0, 2, 1], [0, 3], [0, 3, 1], [0, 3, 1, 1], [0, 3, 1], [0, 1, 1, 1, 1]] }
                  BatchProject { exprs: [ToChar($3, 'yyyy-MM-dd':Varchar), $2, $1, $0] }
                    BatchScan { table: bid, columns: [auction, bidder, price, dateTime] }
  stream_plan: |
    StreamMaterialize { columns: [day, agg#0(hidden), total_bids, rank1_bids, rank2_bids, rank3_bids, total_bidders, rank1_bidders, rank2_bidders, rank3_bidders, total_auctions, rank1_auctions, rank2_auctions, rank3_auctions], pk_columns: [day] }
      StreamHashAgg { group_key: [$0], aggs: [count, sum($2) filter(($1 = 8:Int64)), sum($3) filter(($1 = 8:Int64)), sum($4) filter(($1 = 8:Int64)), sum($5) filter(($1 = 8:Int64)), sum($6) filter(($1 = 0:Int64)), sum($7) filter(($1 = 1:Int64)), sum($8) filter(($1 = 2:Int64)), sum($9) filter(($1 = 3:Int64)), sum($10) filter(($1 = 4:Int64)), sum($11) filter(($1 = 5:Int64)), sum($12) filter(($1 = 6:Int64)), sum($13) filter(($1 = 7:Int64))] }
        StreamExchange { dist: HashShard([0]) }
          StreamProject { exprs: [$0, $9, $11, $12, $13, $14, $15, $16, $17, $18, $19, $20, $21, $22, $2, $2, $2, $2, $3, $3, $3, $3, $5] }
            StreamHashAgg { group_key: [$0, $2, $2, $2, $2, $3, $3, $3, $3, $5], aggs: [count, count, count filter(($1 < 10000:Int32)), count filter((($1 >= 10000:Int32) AND ($1 < 1000000:Int32))), count filter(($1 >= 1000000:Int32)), count($2), count($2) filter(($1 < 10000:Int32)), count($2) filter((($1 >= 10000:Int32) AND ($1 < 1000000:Int32))), count($2) filter(($1 >= 1000000:Int32)), count($3), count($3) filter(($1 < 10000:Int32)), count($3) filter((($1 >= 10000:Int32) AND ($1 < 1000000:Int32))), count($3) filter(($1 >= 1000000:Int32))] }
              StreamExchange { dist: HashShard([0, 2, 3, 5]) }
                StreamExpand { column_subsets: [[0, 2], [0, 2, 1], [0, 2, 1, 1], [0, 2, 1], [0, 3], [0, 3, 1], [0, 3, 1, 1], [0, 3, 1], [0, 1, 1, 1, 1]] }
                  StreamProject { exprs: [ToChar($3, 'yyyy-MM-dd':Varchar), $2, $1, $0, $4] }
                    StreamTableScan { table: bid, columns: [auction, bidder, price, dateTime, _row_id], pk_indices: [4] }
- id: nexmark_q16
  before:
    - create_tables
  sql: |
    SELECT
      channel,
      to_char(dateTime, 'yyyy-MM-dd') AS day,
      max(to_char(dateTime, 'HH:mm')) AS minute,
      count(*) AS total_bids,
      count(*) filter (where price < 10000) AS rank1_bids,
      count(*) filter (where price >= 10000 and price < 1000000) AS rank2_bids,
      count(*) filter (where price >= 1000000) AS rank3_bids,
      count(distinct bidder) AS total_bidders,
      count(distinct bidder) filter (where price < 10000) AS rank1_bidders,
      count(distinct bidder) filter (where price >= 10000 and price < 1000000) AS rank2_bidders,
      count(distinct bidder) filter (where price >= 1000000) AS rank3_bidders,
      count(distinct auction) AS total_auctions,
      count(distinct auction) filter (where price < 10000) AS rank1_auctions,
      count(distinct auction) filter (where price >= 10000 and price < 1000000) AS rank2_auctions,
      count(distinct auction) filter (where price >= 1000000) AS rank3_auctions
    FROM bid
    GROUP BY channel, to_char(dateTime, 'yyyy-MM-dd');
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchHashAgg { group_key: [$0, $1], aggs: [max($2), count, count filter(($3 < 10000:Int32)), count filter((($3 >= 10000:Int32) AND ($3 < 1000000:Int32))), count filter(($3 >= 1000000:Int32)), count(distinct $4), count(distinct $4) filter(($3 < 10000:Int32)), count(distinct $4) filter((($3 >= 10000:Int32) AND ($3 < 1000000:Int32))), count(distinct $4) filter(($3 >= 1000000:Int32)), count(distinct $5), count(distinct $5) filter(($3 < 10000:Int32)), count(distinct $5) filter((($3 >= 10000:Int32) AND ($3 < 1000000:Int32))), count(distinct $5) filter(($3 >= 1000000:Int32))] }
        BatchExchange { order: [], dist: HashShard([0, 1]) }
          BatchProject { exprs: [$3, ToChar($4, 'yyyy-MM-dd':Varchar), ToChar($4, 'HH:mm':Varchar), $2, $1, $0] }
            BatchScan { table: bid, columns: [auction, bidder, price, channel, dateTime] }
  stream_plan: |
    StreamMaterialize { columns: [channel, day, agg#0(hidden), minute, total_bids, rank1_bids, rank2_bids, rank3_bids, total_bidders, rank1_bidders, rank2_bidders, rank3_bidders, total_auctions, rank1_auctions, rank2_auctions, rank3_auctions], pk_columns: [channel, day] }
      StreamHashAgg { group_key: [$0, $1], aggs: [count, max($2), count, count filter(($3 < 10000:Int32)), count filter((($3 >= 10000:Int32) AND ($3 < 1000000:Int32))), count filter(($3 >= 1000000:Int32)), count(distinct $4), count(distinct $4) filter(($3 < 10000:Int32)), count(distinct $4) filter((($3 >= 10000:Int32) AND ($3 < 1000000:Int32))), count(distinct $4) filter(($3 >= 1000000:Int32)), count(distinct $5), count(distinct $5) filter(($3 < 10000:Int32)), count(distinct $5) filter((($3 >= 10000:Int32) AND ($3 < 1000000:Int32))), count(distinct $5) filter(($3 >= 1000000:Int32))] }
        StreamExchange { dist: HashShard([0, 1]) }
          StreamProject { exprs: [$3, ToChar($4, 'yyyy-MM-dd':Varchar), ToChar($4, 'HH:mm':Varchar), $2, $1, $0, $5] }
            StreamTableScan { table: bid, columns: [auction, bidder, price, channel, dateTime, _row_id], pk_indices: [5] }
- id: nexmark_q17
  before:
    - create_tables
  sql: |
    SELECT
        auction,
        to_char(dateTime, 'yyyy-MM-dd') AS day,
        count(*) AS total_bids,
        count(*) filter (where price < 10000) AS rank1_bids,
        count(*) filter (where price >= 10000 and price < 1000000) AS rank2_bids,
        count(*) filter (where price >= 1000000) AS rank3_bids,
        min(price) AS min_price,
        max(price) AS max_price,
        avg(price) AS avg_price,
        sum(price) AS sum_price
    FROM bid
    GROUP BY auction, to_char(dateTime, 'yyyy-MM-dd');
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1, $2, $3, $4, $5, $6, $7, ($8 / $9), $10] }
        BatchHashAgg { group_key: [$0, $1], aggs: [count, count filter(($2 < 10000:Int32)), count filter((($2 >= 10000:Int32) AND ($2 < 1000000:Int32))), count filter(($2 >= 1000000:Int32)), min($2), max($2), sum($2), count($2), sum($2)] }
          BatchExchange { order: [], dist: HashShard([0, 1]) }
            BatchProject { exprs: [$0, ToChar($2, 'yyyy-MM-dd':Varchar), $1] }
              BatchScan { table: bid, columns: [auction, price, dateTime] }
  stream_plan: |
    StreamMaterialize { columns: [auction, day, total_bids, rank1_bids, rank2_bids, rank3_bids, min_price, max_price, avg_price, sum_price], pk_columns: [auction, day] }
      StreamProject { exprs: [$0, $1, $3, $4, $5, $6, $7, $8, ($9 / $10), $11] }
        StreamHashAgg { group_key: [$0, $1], aggs: [count, count, count filter(($2 < 10000:Int32)), count filter((($2 >= 10000:Int32) AND ($2 < 1000000:Int32))), count filter(($2 >= 1000000:Int32)), min($2), max($2), sum($2), count($2), sum($2)] }
          StreamExchange { dist: HashShard([0, 1]) }
            StreamProject { exprs: [$0, ToChar($2, 'yyyy-MM-dd':Varchar), $1, $3] }
              StreamTableScan { table: bid, columns: [auction, price, dateTime, _row_id], pk_indices: [3] }
- id: nexmark_q18
  before:
    - create_tables
  sql: |
    SELECT auction, bidder, price, channel, url, dateTime, extra
    FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY bidder, auction ORDER BY dateTime DESC) AS rank_number
          FROM bid)
    WHERE rank_number <= 1;
  binder_error: 'Feature is not yet implemented: unsupported function: "row_number", Tracking issue: https://github.com/singularity-data/risingwave/issues/112'
- id: nexmark_q19
  before:
    - create_tables
  sql: |
    SELECT * FROM
    (SELECT *, ROW_NUMBER() OVER (PARTITION BY auction ORDER BY price DESC) AS rank_number FROM bid)
    WHERE rank_number <= 10;
  binder_error: 'Feature is not yet implemented: unsupported function: "row_number", Tracking issue: https://github.com/singularity-data/risingwave/issues/112'
- id: nexmark_q20
  before:
    - create_tables
  sql: |
    SELECT
        auction, bidder, price, channel, url, B.dateTime as dateTimeB,
        itemName, description, initialBid, reserve, A.dateTime as dateTimeA, expires, seller, category
    FROM
        bid B INNER JOIN auction A on B.auction = A.id
    WHERE A.category = 10;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchHashJoin { type: Inner, predicate: $0 = $6, output_indices: [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14] }
        BatchExchange { order: [], dist: HashShard([0]) }
          BatchScan { table: bid, columns: [auction, bidder, price, channel, url, dateTime] }
        BatchExchange { order: [], dist: HashShard([0]) }
          BatchFilter { predicate: ($8 = 10:Int32) }
            BatchScan { table: auction, columns: [id, itemName, description, initialBid, reserve, dateTime, expires, seller, category] }
  stream_plan: |
    StreamMaterialize { columns: [auction, bidder, price, channel, url, dateTimeB, itemName, description, initialBid, reserve, dateTimeA, expires, seller, category, _row_id(hidden), _row_id#1(hidden)], pk_columns: [_row_id, _row_id#1] }
      StreamExchange { dist: HashShard([14, 15]) }
        StreamHashJoin { type: Inner, predicate: $0 = $7, output_indices: [0, 1, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15, 6, 16] }
          StreamExchange { dist: HashShard([0]) }
            StreamTableScan { table: bid, columns: [auction, bidder, price, channel, url, dateTime, _row_id], pk_indices: [6] }
          StreamExchange { dist: HashShard([0]) }
            StreamFilter { predicate: ($8 = 10:Int32) }
              StreamTableScan { table: auction, columns: [id, itemName, description, initialBid, reserve, dateTime, expires, seller, category, _row_id], pk_indices: [9] }
- id: nexmark_q21
  before:
    - create_tables
  sql: |
    SELECT
        auction, bidder, price, channel,
        CASE
            WHEN lower(channel) = 'apple' THEN '0'
            WHEN lower(channel) = 'google' THEN '1'
            WHEN lower(channel) = 'facebook' THEN '2'
            WHEN lower(channel) = 'baidu' THEN '3'
            ELSE REGEXP_EXTRACT(url, '(&|^)channel_id=([^&]*)', 2)
            END
        AS channel_id FROM bid
        where REGEXP_EXTRACT(url, '(&|^)channel_id=([^&]*)', 2) is not null or
              lower(channel) in ('apple', 'google', 'facebook', 'baidu');
  binder_error: 'Feature is not yet implemented: unsupported function: "regexp_extract", Tracking issue: https://github.com/singularity-data/risingwave/issues/112'
- id: nexmark_q22
  before:
    - create_tables
  sql: |
    SELECT
        auction, bidder, price, channel,
        SPLIT_PART(url, '/', 4) as dir1,
        SPLIT_PART(url, '/', 5) as dir2,
        SPLIT_PART(url, '/', 6) as dir3 FROM bid;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1, $2, $3, SplitPart($4, '/':Varchar, 4:Int32), SplitPart($4, '/':Varchar, 5:Int32), SplitPart($4, '/':Varchar, 6:Int32)] }
        BatchScan { table: bid, columns: [auction, bidder, price, channel, url] }
  stream_plan: |
    StreamMaterialize { columns: [auction, bidder, price, channel, dir1, dir2, dir3, _row_id(hidden)], pk_columns: [_row_id] }
      StreamProject { exprs: [$0, $1, $2, $3, SplitPart($4, '/':Varchar, 4:Int32), SplitPart($4, '/':Varchar, 5:Int32), SplitPart($4, '/':Varchar, 6:Int32), $5] }
        StreamTableScan { table: bid, columns: [auction, bidder, price, channel, url, _row_id], pk_indices: [5] }
