# This file is automatically generated. See `src/frontend/test_runner/README.md` for more information.
- id: create_tables
  sql: |
    CREATE TABLE person (
        id BIGINT,
        name VARCHAR,
        email_address VARCHAR,
        credit_card VARCHAR,
        city VARCHAR,
        state VARCHAR,
        date_time TIMESTAMP
    );

    CREATE TABLE auction (
        id BIGINT,
        "item_name" VARCHAR,
        "description" VARCHAR,
        "initial_bid" BIGINT,
        "reserve" BIGINT,
        "date_time" TIMESTAMP,
        "expires" TIMESTAMP,
        "seller" BIGINT,
        "category" BIGINT
    );

    CREATE TABLE bid (
        "auction" BIGINT,
        "bidder" BIGINT,
        "price" BIGINT,
        "channel" VARCHAR,
        "url" VARCHAR,
        "date_time" TIMESTAMP,
        "extra" VARCHAR
    );
- id: nexmark_q0
  before:
    - create_tables
  sql: |
    CREATE MATERIALIZED VIEW nexmark_q0
      AS
    SELECT auction, bidder, price, date_time FROM bid;
- id: nexmark_q0
  before:
    - create_tables
  sql: |
    SELECT auction, bidder, price, date_time FROM bid;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchScan { table: bid, columns: [auction, bidder, price, date_time] }
  stream_plan: |
    StreamMaterialize { columns: [auction, bidder, price, date_time, bid._row_id(hidden)], pk_columns: [bid._row_id] }
      StreamTableScan { table: bid, columns: [auction, bidder, price, date_time, _row_id], pk_indices: [4] }
- id: nexmark_q1
  before:
    - create_tables
  sql: |
    SELECT
      auction,
      bidder,
      0.908 * price as price,
      date_time
    FROM bid;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1, (0.908:Decimal * $2), $3] }
        BatchScan { table: bid, columns: [auction, bidder, price, date_time] }
  stream_plan: |
    StreamMaterialize { columns: [auction, bidder, price, date_time, bid._row_id(hidden)], pk_columns: [bid._row_id] }
      StreamProject { exprs: [$0, $1, (0.908:Decimal * $2), $3, $4] }
        StreamTableScan { table: bid, columns: [auction, bidder, price, date_time, _row_id], pk_indices: [4] }
- id: nexmark_q2
  before:
    - create_tables
  sql: SELECT auction, price FROM bid WHERE auction = 1007 OR auction = 1020 OR auction = 2001 OR auction = 2019 OR auction = 2087;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchFilter { predicate: ((((($0 = 1007:Int32) OR ($0 = 1020:Int32)) OR ($0 = 2001:Int32)) OR ($0 = 2019:Int32)) OR ($0 = 2087:Int32)) }
        BatchScan { table: bid, columns: [auction, price] }
  stream_plan: |
    StreamMaterialize { columns: [auction, price, bid._row_id(hidden)], pk_columns: [bid._row_id] }
      StreamFilter { predicate: ((((($0 = 1007:Int32) OR ($0 = 1020:Int32)) OR ($0 = 2001:Int32)) OR ($0 = 2019:Int32)) OR ($0 = 2087:Int32)) }
        StreamTableScan { table: bid, columns: [auction, price, _row_id], pk_indices: [2] }
- id: nexmark_q3
  before:
    - create_tables
  sql: |
    SELECT
        P.name, P.city, P.state, A.id
    FROM
        auction AS A INNER JOIN person AS P on A.seller = P.id
    WHERE
        A.category = 10 and (P.state = 'or' OR P.state = 'id' OR P.state = 'ca');
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$1, $2, $3, $0] }
        BatchHashJoin { type: Inner, predicate: $1 = $2, output_indices: [0, 3, 4, 5] }
          BatchExchange { order: [], dist: HashShard([1]) }
            BatchProject { exprs: [$0, $1] }
              BatchFilter { predicate: ($2 = 10:Int32) }
                BatchScan { table: auction, columns: [id, seller, category] }
          BatchExchange { order: [], dist: HashShard([0]) }
            BatchFilter { predicate: ((($3 = 'or':Varchar) OR ($3 = 'id':Varchar)) OR ($3 = 'ca':Varchar)) }
              BatchScan { table: person, columns: [id, name, city, state] }
  stream_plan: |
    StreamMaterialize { columns: [name, city, state, id, auction._row_id(hidden), person._row_id(hidden)], pk_columns: [auction._row_id, person._row_id] }
      StreamExchange { dist: HashShard([4, 5]) }
        StreamProject { exprs: [$1, $2, $3, $0, $4, $5] }
          StreamHashJoin { type: Inner, predicate: $1 = $3, output_indices: [0, 4, 5, 6, 2, 7] }
            StreamExchange { dist: HashShard([1]) }
              StreamProject { exprs: [$0, $1, $2] }
                StreamFilter { predicate: ($3 = 10:Int32) }
                  StreamTableScan { table: auction, columns: [id, seller, _row_id, category], pk_indices: [2] }
            StreamExchange { dist: HashShard([0]) }
              StreamFilter { predicate: ((($3 = 'or':Varchar) OR ($3 = 'id':Varchar)) OR ($3 = 'ca':Varchar)) }
                StreamTableScan { table: person, columns: [id, name, city, state, _row_id], pk_indices: [4] }
- id: nexmark_q4
  before:
    - create_tables
  sql: |
    SELECT
        Q.category,
        AVG(Q.final) as avg
    FROM (
        SELECT MAX(B.price) AS final, A.category
        FROM auction A, bid B
        WHERE A.id = B.auction AND B.date_time BETWEEN A.date_time AND A.expires
        GROUP BY A.id, A.category
    ) Q
    GROUP BY Q.category;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, ($1 / $2)] }
        BatchHashAgg { group_key: [$0], aggs: [sum($1), count($1)] }
          BatchExchange { order: [], dist: HashShard([0]) }
            BatchProject { exprs: [$1, $2] }
              BatchHashAgg { group_key: [$0, $1], aggs: [max($2)] }
                BatchProject { exprs: [$0, $3, $5] }
                  BatchFilter { predicate: ($6 >= $1) AND ($6 <= $2) }
                    BatchHashJoin { type: Inner, predicate: $0 = $4, output_indices: all }
                      BatchExchange { order: [], dist: HashShard([0]) }
                        BatchScan { table: auction, columns: [id, date_time, expires, category] }
                      BatchExchange { order: [], dist: HashShard([0]) }
                        BatchScan { table: bid, columns: [auction, price, date_time] }
  stream_plan: |
    StreamMaterialize { columns: [category, avg], pk_columns: [category] }
      StreamProject { exprs: [$0, ($2 / $3)] }
        StreamHashAgg { group_key: [$0], aggs: [count, sum($1), count($1)] }
          StreamExchange { dist: HashShard([0]) }
            StreamProject { exprs: [$1, $3, $0] }
              StreamHashAgg { group_key: [$0, $1], aggs: [count, max($2)] }
                StreamProject { exprs: [$0, $3, $6, $4, $8] }
                  StreamFilter { predicate: ($7 >= $1) AND ($7 <= $2) }
                    StreamHashJoin { type: Inner, predicate: $0 = $5, output_indices: all }
                      StreamExchange { dist: HashShard([0]) }
                        StreamTableScan { table: auction, columns: [id, date_time, expires, category, _row_id], pk_indices: [4] }
                      StreamExchange { dist: HashShard([0]) }
                        StreamTableScan { table: bid, columns: [auction, price, date_time, _row_id], pk_indices: [3] }
- id: nexmark_q5
  before:
    - create_tables
  sql: |
    SELECT AuctionBids.auction, AuctionBids.num FROM (
      SELECT
        bid.auction,
        count(*) AS num,
        window_start AS starttime
      FROM
        HOP(bid, date_time, INTERVAL '2' SECOND, INTERVAL '10' SECOND)
      GROUP BY
        window_start,
        bid.auction
    ) AS AuctionBids
    JOIN (
      SELECT
        max(CountBids.num) AS maxn,
        CountBids.starttime_c
      FROM (
        SELECT
          count(*) AS num,
          window_start AS starttime_c
        FROM HOP(bid, date_time, INTERVAL '2' SECOND, INTERVAL '10' SECOND)
        GROUP BY
          bid.auction,
          window_start
      ) AS CountBids
      GROUP BY
        CountBids.starttime_c
    ) AS MaxBids
    ON AuctionBids.starttime = MaxBids.starttime_c AND AuctionBids.num >= MaxBids.maxn;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1] }
        BatchFilter { predicate: ($1 >= $3) }
          BatchHashJoin { type: Inner, predicate: $2 = $4, output_indices: all }
            BatchExchange { order: [], dist: HashShard([2]) }
              BatchProject { exprs: [$1, $2, $0] }
                BatchHashAgg { group_key: [$0, $1], aggs: [count] }
                  BatchExchange { order: [], dist: HashShard([0, 1]) }
                    BatchProject { exprs: [$1, $0] }
                      BatchHopWindow { time_col: $1, slide: 00:00:02, size: 00:00:10, output_indices: [0, 2] }
                        BatchScan { table: bid, columns: [auction, date_time] }
            BatchProject { exprs: [$1, $0] }
              BatchHashAgg { group_key: [$0], aggs: [max($1)] }
                BatchExchange { order: [], dist: HashShard([0]) }
                  BatchProject { exprs: [$1, $2] }
                    BatchHashAgg { group_key: [$0, $1], aggs: [count] }
                      BatchHopWindow { time_col: $1, slide: 00:00:02, size: 00:00:10, output_indices: [0, 2] }
                        BatchExchange { order: [], dist: HashShard([0]) }
                          BatchScan { table: bid, columns: [auction, date_time] }
  stream_plan: |
    StreamMaterialize { columns: [auction, num, window_start(hidden), window_start#1(hidden)], pk_columns: [window_start, auction, window_start#1] }
      StreamProject { exprs: [$0, $1, $2, $4] }
        StreamFilter { predicate: ($1 >= $3) }
          StreamHashJoin { type: Inner, predicate: $2 = $4, output_indices: all }
            StreamExchange { dist: HashShard([2]) }
              StreamProject { exprs: [$1, $3, $0] }
                StreamHashAgg { group_key: [$0, $1], aggs: [count, count] }
                  StreamExchange { dist: HashShard([0, 1]) }
                    StreamProject { exprs: [$1, $0, $2] }
                      StreamHopWindow { time_col: $1, slide: 00:00:02, size: 00:00:10, output_indices: [0, 3, 2] }
                        StreamTableScan { table: bid, columns: [auction, date_time, _row_id], pk_indices: [2] }
            StreamProject { exprs: [$2, $0] }
              StreamHashAgg { group_key: [$0], aggs: [count, max($1)] }
                StreamExchange { dist: HashShard([0]) }
                  StreamProject { exprs: [$1, $3, $0] }
                    StreamHashAgg { group_key: [$0, $1], aggs: [count, count] }
                      StreamExchange { dist: HashShard([0, 1]) }
                        StreamHopWindow { time_col: $1, slide: 00:00:02, size: 00:00:10, output_indices: [0, 3, 2] }
                          StreamTableScan { table: bid, columns: [auction, date_time, _row_id], pk_indices: [2] }
- id: nexmark_q6
  before:
    - create_tables
  sql: |
    SELECT
        Q.seller,
        AVG(Q.final) OVER
            (PARTITION BY Q.seller ORDER BY Q.date_time ROWS BETWEEN 10 PRECEDING AND CURRENT ROW)
        as avg
    FROM (
        SELECT MAX(B.price) AS final, A.seller, B.date_time
        FROM auction AS A, bid AS B
        WHERE A.id = B.auction and B.date_time between A.date_time and A.expires
        GROUP BY A.id, A.seller
    ) AS Q;
  planner_error: 'Invalid input syntax: column must appear in the GROUP BY clause or be used in an aggregate function'
- id: nexmark_q7
  before:
    - create_tables
  sql: |
    SELECT
      B.auction,
      B.price,
      B.bidder,
      B.date_time
    FROM
      bid B
    JOIN (
      SELECT
        MAX(price) AS maxprice,
        window_end as date_time
      FROM
        TUMBLE(bid, date_time, INTERVAL '10' SECOND)
      GROUP BY
        window_end
    ) B1 ON B.price = B1.maxprice
    WHERE
      B.date_time BETWEEN B1.date_time - INTERVAL '10' SECOND
      AND B1.date_time;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $2, $1, $3] }
        BatchFilter { predicate: ($3 >= ($5 - '00:00:10':Interval)) AND ($3 <= $5) }
          BatchHashJoin { type: Inner, predicate: $2 = $4, output_indices: all }
            BatchExchange { order: [], dist: HashShard([2]) }
              BatchScan { table: bid, columns: [auction, bidder, price, date_time] }
            BatchExchange { order: [], dist: HashShard([0]) }
              BatchProject { exprs: [$1, $0] }
                BatchHashAgg { group_key: [$0], aggs: [max($1)] }
                  BatchExchange { order: [], dist: HashShard([0]) }
                    BatchProject { exprs: [(TumbleStart($1, '00:00:10':Interval) + '00:00:10':Interval), $0] }
                      BatchScan { table: bid, columns: [price, date_time] }
  stream_plan: |
    StreamMaterialize { columns: [auction, price, bidder, date_time, bid._row_id(hidden), (TumbleStart(bid.date_time, '00:00:10':Interval) + '00:00:10':Interval)(hidden)], pk_columns: [bid._row_id, (TumbleStart(bid.date_time, '00:00:10':Interval) + '00:00:10':Interval)] }
      StreamExchange { dist: HashShard([4, 5]) }
        StreamProject { exprs: [$0, $2, $1, $3, $4, $6] }
          StreamFilter { predicate: ($3 >= ($6 - '00:00:10':Interval)) AND ($3 <= $6) }
            StreamHashJoin { type: Inner, predicate: $2 = $5, output_indices: all }
              StreamExchange { dist: HashShard([2]) }
                StreamTableScan { table: bid, columns: [auction, bidder, price, date_time, _row_id], pk_indices: [4] }
              StreamExchange { dist: HashShard([0]) }
                StreamProject { exprs: [$2, $0] }
                  StreamHashAgg { group_key: [$0], aggs: [count, max($1)] }
                    StreamExchange { dist: HashShard([0]) }
                      StreamProject { exprs: [(TumbleStart($1, '00:00:10':Interval) + '00:00:10':Interval), $0, $2] }
                        StreamTableScan { table: bid, columns: [price, date_time, _row_id], pk_indices: [2] }
- id: nexmark_q8
  before:
    - create_tables
  sql: |
    SELECT
      P.id,
      P.name,
      P.starttime
    FROM (
      SELECT
        id,
        name,
        window_start AS starttime,
        window_end AS endtime
      FROM
        TUMBLE(person, date_time, INTERVAL '10' SECOND)
      GROUP BY
        id,
        name,
        window_start,
        window_end
    ) P
    JOIN (
      SELECT
        seller,
        window_start AS starttime,
        window_end AS endtime
      FROM
        TUMBLE(auction, date_time, INTERVAL '10' SECOND)
      GROUP BY
        seller,
        window_start,
        window_end
    ) A ON P.id = A.seller
      AND P.starttime = A.starttime
      AND P.endtime = A.endtime;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchHashJoin { type: Inner, predicate: $0 = $4 AND $2 = $5 AND $3 = $6, output_indices: [0, 1, 2] }
        BatchExchange { order: [], dist: HashShard([0, 2, 3]) }
          BatchHashAgg { group_key: [$0, $1, $2, $3], aggs: [] }
            BatchExchange { order: [], dist: HashShard([0, 1, 2, 3]) }
              BatchProject { exprs: [$0, $1, TumbleStart($2, '00:00:10':Interval), (TumbleStart($2, '00:00:10':Interval) + '00:00:10':Interval)] }
                BatchScan { table: person, columns: [id, name, date_time] }
        BatchHashAgg { group_key: [$0, $1, $2], aggs: [] }
          BatchExchange { order: [], dist: HashShard([0, 1, 2]) }
            BatchProject { exprs: [$1, TumbleStart($0, '00:00:10':Interval), (TumbleStart($0, '00:00:10':Interval) + '00:00:10':Interval)] }
              BatchScan { table: auction, columns: [date_time, seller] }
  stream_plan: |
    StreamMaterialize { columns: [id, name, starttime, (TumbleStart(person.date_time, '00:00:10':Interval) + '00:00:10':Interval)(hidden), auction.seller(hidden), TumbleStart(auction.date_time, '00:00:10':Interval)(hidden), (TumbleStart(auction.date_time, '00:00:10':Interval) + '00:00:10':Interval)(hidden)], pk_columns: [id, name, starttime, (TumbleStart(person.date_time, '00:00:10':Interval) + '00:00:10':Interval), auction.seller, TumbleStart(auction.date_time, '00:00:10':Interval), (TumbleStart(auction.date_time, '00:00:10':Interval) + '00:00:10':Interval)] }
      StreamHashJoin { type: Inner, predicate: $0 = $5 AND $2 = $6 AND $3 = $7, output_indices: [0, 1, 2, 3, 5, 6, 7] }
        StreamExchange { dist: HashShard([0, 2, 3]) }
          StreamHashAgg { group_key: [$0, $1, $2, $3], aggs: [count] }
            StreamExchange { dist: HashShard([0, 1, 2, 3]) }
              StreamProject { exprs: [$0, $1, TumbleStart($2, '00:00:10':Interval), (TumbleStart($2, '00:00:10':Interval) + '00:00:10':Interval), $3] }
                StreamTableScan { table: person, columns: [id, name, date_time, _row_id], pk_indices: [3] }
        StreamHashAgg { group_key: [$0, $1, $2], aggs: [count] }
          StreamExchange { dist: HashShard([0, 1, 2]) }
            StreamProject { exprs: [$1, TumbleStart($0, '00:00:10':Interval), (TumbleStart($0, '00:00:10':Interval) + '00:00:10':Interval), $2] }
              StreamTableScan { table: auction, columns: [date_time, seller, _row_id], pk_indices: [2] }
- id: nexmark_q9
  before:
    - create_tables
  sql: |
    SELECT
      id, item_name, description, initial_bid, reserve, date_time, expires, seller, category,
      auction, bidder, price, bid_date_time
    FROM (
      SELECT A.*, B.auction, B.bidder, B.price, B.date_time AS bid_date_time,
        ROW_NUMBER() OVER (PARTITION BY A.id ORDER BY B.price DESC, B.date_time ASC) AS rownum
      FROM auction A, bid B
      WHERE A.id = B.auction AND B.date_time BETWEEN A.date_time AND A.expires
    )
    WHERE rownum <= 1;
  binder_error: 'Feature is not yet implemented: unsupported function: "row_number", Tracking issue: https://github.com/singularity-data/risingwave/issues/112'
- id: nexmark_q10
  before:
    - create_tables
  sql: |
    SELECT auction, bidder, price, date_time, TO_CHAR(date_time, 'YYYY-MM-DD') as date, TO_CHAR(date_time, 'HH:MI') as time FROM bid;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1, $2, $3, ToChar($3, 'YYYY-MM-DD':Varchar), ToChar($3, 'HH:MI':Varchar)] }
        BatchScan { table: bid, columns: [auction, bidder, price, date_time] }
  stream_plan: |
    StreamMaterialize { columns: [auction, bidder, price, date_time, date, time, bid._row_id(hidden)], pk_columns: [bid._row_id] }
      StreamProject { exprs: [$0, $1, $2, $3, ToChar($3, 'YYYY-MM-DD':Varchar), ToChar($3, 'HH:MI':Varchar), $4] }
        StreamTableScan { table: bid, columns: [auction, bidder, price, date_time, _row_id], pk_indices: [4] }
- id: nexmark_q11
  before:
    - create_tables
  sql: |
    SELECT
      B.bidder,
      count(*) as bid_count,
      SESSION_START(B.date_time, INTERVAL '10' SECOND) as starttime,
      SESSION_END(B.date_time, INTERVAL '10' SECOND) as endtime
    FROM bid B
    GROUP BY B.bidder, SESSION(B.date_time, INTERVAL '10' SECOND);
  binder_error: 'Feature is not yet implemented: unsupported function: "session", Tracking issue: https://github.com/singularity-data/risingwave/issues/112'
- id: nexmark_q12
  before:
    - create_tables
  sql: |
    SELECT
        B.bidder,
        count(*) as bid_count,
        TUMBLE_START(B.p_time, INTERVAL '10' SECOND) as starttime,
        TUMBLE_END(B.p_time, INTERVAL '10' SECOND) as endtime
    FROM (SELECT *, PROCTIME() as p_time FROM bid) B
    GROUP BY B.bidder, TUMBLE(B.p_time, INTERVAL '10' SECOND);
  binder_error: 'Feature is not yet implemented: unsupported function: "proctime", Tracking issue: https://github.com/singularity-data/risingwave/issues/112'
- id: nexmark_q13
  before:
    - create_tables
  sql: |
    /* SELECT
        B.auction,
        B.bidder,
        B.price,
        B.date_time,
        S.value
    FROM (SELECT *, PROCTIME() as p_time FROM bid) B
    JOIN side_input FOR SYSTEM_TIME AS OF B.p_time AS S
    ON mod(B.auction, 10000) = S.key; */
    select 1;
- id: nexmark_q14
  before:
    - create_tables
  sql: |
    SELECT
      auction,
      bidder,
      0.908 * price as price,
      CASE
        WHEN
          extract(hour from date_time) >= 8 AND
          extract(hour from date_time) <= 18
        THEN 'dayTime'
        WHEN
          extract(hour from date_time) <= 6 OR
          extract(hour from date_time) >= 20
        THEN 'nightTime'
        ELSE 'otherTime'
      END AS bidTimeType,
      date_time,
      extra
      -- TODO: count_char is an UDF, add it back when we support similar functionality.
      -- https://github.com/nexmark/nexmark/blob/master/nexmark-flink/src/main/java/com/github/nexmark/flink/udf/CountChar.java
      -- count_char(extra, 'c') AS c_counts
    FROM bid
    WHERE 0.908 * price > 1000000 AND 0.908 * price < 50000000;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1, (0.908:Decimal * $2), Case(((Extract('HOUR':Varchar, $3) >= 8:Int32) AND (Extract('HOUR':Varchar, $3) <= 18:Int32)), 'dayTime':Varchar, ((Extract('HOUR':Varchar, $3) <= 6:Int32) OR (Extract('HOUR':Varchar, $3) >= 20:Int32)), 'nightTime':Varchar, 'otherTime':Varchar), $3, $4] }
        BatchFilter { predicate: ((0.908:Decimal * $2) > 1000000:Int32) AND ((0.908:Decimal * $2) < 50000000:Int32) }
          BatchScan { table: bid, columns: [auction, bidder, price, date_time, extra] }
  stream_plan: |
    StreamMaterialize { columns: [auction, bidder, price, bidtimetype, date_time, extra, bid._row_id(hidden)], pk_columns: [bid._row_id] }
      StreamProject { exprs: [$0, $1, (0.908:Decimal * $2), Case(((Extract('HOUR':Varchar, $3) >= 8:Int32) AND (Extract('HOUR':Varchar, $3) <= 18:Int32)), 'dayTime':Varchar, ((Extract('HOUR':Varchar, $3) <= 6:Int32) OR (Extract('HOUR':Varchar, $3) >= 20:Int32)), 'nightTime':Varchar, 'otherTime':Varchar), $3, $4, $5] }
        StreamFilter { predicate: ((0.908:Decimal * $2) > 1000000:Int32) AND ((0.908:Decimal * $2) < 50000000:Int32) }
          StreamTableScan { table: bid, columns: [auction, bidder, price, date_time, extra, _row_id], pk_indices: [5] }
- id: nexmark_q15
  before:
    - create_tables
  sql: |
    SELECT
        TO_CHAR(date_time, 'yyyy-MM-dd') as day,
        count(*) AS total_bids,
        count(*) filter (where price < 10000) AS rank1_bids,
        count(*) filter (where price >= 10000 and price < 1000000) AS rank2_bids,
        count(*) filter (where price >= 1000000) AS rank3_bids,
        count(distinct bidder) AS total_bidders,
        count(distinct bidder) filter (where price < 10000) AS rank1_bidders,
        count(distinct bidder) filter (where price >= 10000 and price < 1000000) AS rank2_bidders,
        count(distinct bidder) filter (where price >= 1000000) AS rank3_bidders,
        count(distinct auction) AS total_auctions,
        count(distinct auction) filter (where price < 10000) AS rank1_auctions,
        count(distinct auction) filter (where price >= 10000 and price < 1000000) AS rank2_auctions,
        count(distinct auction) filter (where price >= 1000000) AS rank3_auctions
    FROM bid
    GROUP BY to_char(date_time, 'yyyy-MM-dd');
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, Coalesce($1, 0:Int64), Coalesce($2, 0:Int64), Coalesce($3, 0:Int64), Coalesce($4, 0:Int64), $5, $6, $7, $8, $9, $10, $11, $12] }
        BatchHashAgg { group_key: [$0], aggs: [sum($10) filter(($9 = 0:Int64)), sum($11) filter(($9 = 0:Int64)), sum($12) filter(($9 = 0:Int64)), sum($13) filter(($9 = 0:Int64)), count($1) filter(($9 = 1:Int64)), count($2) filter((($14 > 0:Int64) AND ($9 = 2:Int64))), count($3) filter((($15 > 0:Int64) AND ($9 = 3:Int64))), count($4) filter((($16 > 0:Int64) AND ($9 = 4:Int64))), count($5) filter(($9 = 5:Int64)), count($6) filter((($17 > 0:Int64) AND ($9 = 6:Int64))), count($7) filter((($18 > 0:Int64) AND ($9 = 7:Int64))), count($8) filter((($19 > 0:Int64) AND ($9 = 8:Int64)))] }
          BatchExchange { order: [], dist: HashShard([0]) }
            BatchHashAgg { group_key: [$0, $2, $2, $2, $2, $3, $3, $3, $3, $4], aggs: [count, count filter(($1 < 10000:Int32)), count filter((($1 >= 10000:Int32) AND ($1 < 1000000:Int32))), count filter(($1 >= 1000000:Int32)), count filter(($1 < 10000:Int32)), count filter((($1 >= 10000:Int32) AND ($1 < 1000000:Int32))), count filter(($1 >= 1000000:Int32)), count filter(($1 < 10000:Int32)), count filter((($1 >= 10000:Int32) AND ($1 < 1000000:Int32))), count filter(($1 >= 1000000:Int32))] }
              BatchExchange { order: [], dist: HashShard([0, 2, 3, 4]) }
                BatchExpand { column_subsets: [[0, 1], [0, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3]] }
                  BatchProject { exprs: [ToChar($3, 'yyyy-MM-dd':Varchar), $2, $1, $0] }
                    BatchScan { table: bid, columns: [auction, bidder, price, date_time] }
  stream_plan: |
    StreamMaterialize { columns: [day, total_bids, rank1_bids, rank2_bids, rank3_bids, total_bidders, rank1_bidders, rank2_bidders, rank3_bidders, total_auctions, rank1_auctions, rank2_auctions, rank3_auctions], pk_columns: [day] }
      StreamProject { exprs: [$0, Coalesce($2, 0:Int64), Coalesce($3, 0:Int64), Coalesce($4, 0:Int64), Coalesce($5, 0:Int64), $6, $7, $8, $9, $10, $11, $12, $13] }
        StreamHashAgg { group_key: [$0], aggs: [count, sum($11) filter(($9 = 0:Int64)), sum($12) filter(($9 = 0:Int64)), sum($13) filter(($9 = 0:Int64)), sum($14) filter(($9 = 0:Int64)), count($1) filter(($9 = 1:Int64)), count($2) filter((($15 > 0:Int64) AND ($9 = 2:Int64))), count($3) filter((($16 > 0:Int64) AND ($9 = 3:Int64))), count($4) filter((($17 > 0:Int64) AND ($9 = 4:Int64))), count($5) filter(($9 = 5:Int64)), count($6) filter((($18 > 0:Int64) AND ($9 = 6:Int64))), count($7) filter((($19 > 0:Int64) AND ($9 = 7:Int64))), count($8) filter((($20 > 0:Int64) AND ($9 = 8:Int64)))] }
          StreamExchange { dist: HashShard([0]) }
            StreamHashAgg { group_key: [$0, $2, $2, $2, $2, $3, $3, $3, $3, $5], aggs: [count, count, count filter(($1 < 10000:Int32)), count filter((($1 >= 10000:Int32) AND ($1 < 1000000:Int32))), count filter(($1 >= 1000000:Int32)), count filter(($1 < 10000:Int32)), count filter((($1 >= 10000:Int32) AND ($1 < 1000000:Int32))), count filter(($1 >= 1000000:Int32)), count filter(($1 < 10000:Int32)), count filter((($1 >= 10000:Int32) AND ($1 < 1000000:Int32))), count filter(($1 >= 1000000:Int32))] }
              StreamExchange { dist: HashShard([0, 2, 3, 5]) }
                StreamExpand { column_subsets: [[0, 1], [0, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3]] }
                  StreamProject { exprs: [ToChar($3, 'yyyy-MM-dd':Varchar), $2, $1, $0, $4] }
                    StreamTableScan { table: bid, columns: [auction, bidder, price, date_time, _row_id], pk_indices: [4] }
- id: nexmark_q16
  before:
    - create_tables
  sql: |
    SELECT
      channel,
      to_char(date_time, 'yyyy-MM-dd') AS day,
      max(to_char(date_time, 'HH:mm')) AS minute,
      count(*) AS total_bids,
      count(*) filter (where price < 10000) AS rank1_bids,
      count(*) filter (where price >= 10000 and price < 1000000) AS rank2_bids,
      count(*) filter (where price >= 1000000) AS rank3_bids,
      count(distinct bidder) AS total_bidders,
      count(distinct bidder) filter (where price < 10000) AS rank1_bidders,
      count(distinct bidder) filter (where price >= 10000 and price < 1000000) AS rank2_bidders,
      count(distinct bidder) filter (where price >= 1000000) AS rank3_bidders,
      count(distinct auction) AS total_auctions,
      count(distinct auction) filter (where price < 10000) AS rank1_auctions,
      count(distinct auction) filter (where price >= 10000 and price < 1000000) AS rank2_auctions,
      count(distinct auction) filter (where price >= 1000000) AS rank3_auctions
    FROM bid
    GROUP BY channel, to_char(date_time, 'yyyy-MM-dd');
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1, $2, Coalesce($3, 0:Int64), Coalesce($4, 0:Int64), Coalesce($5, 0:Int64), Coalesce($6, 0:Int64), $7, $8, $9, $10, $11, $12, $13, $14] }
        BatchHashAgg { group_key: [$0, $1], aggs: [max($11) filter(($10 = 0:Int64)), sum($12) filter(($10 = 0:Int64)), sum($13) filter(($10 = 0:Int64)), sum($14) filter(($10 = 0:Int64)), sum($15) filter(($10 = 0:Int64)), count($2) filter(($10 = 1:Int64)), count($3) filter((($16 > 0:Int64) AND ($10 = 2:Int64))), count($4) filter((($17 > 0:Int64) AND ($10 = 3:Int64))), count($5) filter((($18 > 0:Int64) AND ($10 = 4:Int64))), count($6) filter(($10 = 5:Int64)), count($7) filter((($19 > 0:Int64) AND ($10 = 6:Int64))), count($8) filter((($20 > 0:Int64) AND ($10 = 7:Int64))), count($9) filter((($21 > 0:Int64) AND ($10 = 8:Int64)))] }
          BatchExchange { order: [], dist: HashShard([0, 1]) }
            BatchHashAgg { group_key: [$0, $1, $4, $4, $4, $4, $5, $5, $5, $5, $6], aggs: [max($2), count, count filter(($3 < 10000:Int32)), count filter((($3 >= 10000:Int32) AND ($3 < 1000000:Int32))), count filter(($3 >= 1000000:Int32)), count filter(($3 < 10000:Int32)), count filter((($3 >= 10000:Int32) AND ($3 < 1000000:Int32))), count filter(($3 >= 1000000:Int32)), count filter(($3 < 10000:Int32)), count filter((($3 >= 10000:Int32) AND ($3 < 1000000:Int32))), count filter(($3 >= 1000000:Int32))] }
              BatchExchange { order: [], dist: HashShard([0, 1, 4, 5, 6]) }
                BatchExpand { column_subsets: [[0, 1, 2, 3], [0, 1, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 5], [0, 1, 3, 5], [0, 1, 3, 5], [0, 1, 3, 5]] }
                  BatchProject { exprs: [$3, ToChar($4, 'yyyy-MM-dd':Varchar), ToChar($4, 'HH:mm':Varchar), $2, $1, $0] }
                    BatchScan { table: bid, columns: [auction, bidder, price, channel, date_time] }
  stream_plan: |
    StreamMaterialize { columns: [channel, day, minute, total_bids, rank1_bids, rank2_bids, rank3_bids, total_bidders, rank1_bidders, rank2_bidders, rank3_bidders, total_auctions, rank1_auctions, rank2_auctions, rank3_auctions], pk_columns: [channel, day] }
      StreamProject { exprs: [$0, $1, $3, Coalesce($4, 0:Int64), Coalesce($5, 0:Int64), Coalesce($6, 0:Int64), Coalesce($7, 0:Int64), $8, $9, $10, $11, $12, $13, $14, $15] }
        StreamHashAgg { group_key: [$0, $1], aggs: [count, max($12) filter(($10 = 0:Int64)), sum($13) filter(($10 = 0:Int64)), sum($14) filter(($10 = 0:Int64)), sum($15) filter(($10 = 0:Int64)), sum($16) filter(($10 = 0:Int64)), count($2) filter(($10 = 1:Int64)), count($3) filter((($17 > 0:Int64) AND ($10 = 2:Int64))), count($4) filter((($18 > 0:Int64) AND ($10 = 3:Int64))), count($5) filter((($19 > 0:Int64) AND ($10 = 4:Int64))), count($6) filter(($10 = 5:Int64)), count($7) filter((($20 > 0:Int64) AND ($10 = 6:Int64))), count($8) filter((($21 > 0:Int64) AND ($10 = 7:Int64))), count($9) filter((($22 > 0:Int64) AND ($10 = 8:Int64)))] }
          StreamExchange { dist: HashShard([0, 1]) }
            StreamHashAgg { group_key: [$0, $1, $4, $4, $4, $4, $5, $5, $5, $5, $7], aggs: [count, max($2), count, count filter(($3 < 10000:Int32)), count filter((($3 >= 10000:Int32) AND ($3 < 1000000:Int32))), count filter(($3 >= 1000000:Int32)), count filter(($3 < 10000:Int32)), count filter((($3 >= 10000:Int32) AND ($3 < 1000000:Int32))), count filter(($3 >= 1000000:Int32)), count filter(($3 < 10000:Int32)), count filter((($3 >= 10000:Int32) AND ($3 < 1000000:Int32))), count filter(($3 >= 1000000:Int32))] }
              StreamExchange { dist: HashShard([0, 1, 4, 5, 7]) }
                StreamExpand { column_subsets: [[0, 1, 2, 3], [0, 1, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 5], [0, 1, 3, 5], [0, 1, 3, 5], [0, 1, 3, 5]] }
                  StreamProject { exprs: [$3, ToChar($4, 'yyyy-MM-dd':Varchar), ToChar($4, 'HH:mm':Varchar), $2, $1, $0, $5] }
                    StreamTableScan { table: bid, columns: [auction, bidder, price, channel, date_time, _row_id], pk_indices: [5] }
- id: nexmark_q17
  before:
    - create_tables
  sql: |
    SELECT
        auction,
        to_char(date_time, 'yyyy-MM-dd') AS day,
        count(*) AS total_bids,
        count(*) filter (where price < 10000) AS rank1_bids,
        count(*) filter (where price >= 10000 and price < 1000000) AS rank2_bids,
        count(*) filter (where price >= 1000000) AS rank3_bids,
        min(price) AS min_price,
        max(price) AS max_price,
        avg(price) AS avg_price,
        sum(price) AS sum_price
    FROM bid
    GROUP BY auction, to_char(date_time, 'yyyy-MM-dd');
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1, $2, $3, $4, $5, $6, $7, ($8 / $9), $10] }
        BatchHashAgg { group_key: [$0, $1], aggs: [count, count filter(($2 < 10000:Int32)), count filter((($2 >= 10000:Int32) AND ($2 < 1000000:Int32))), count filter(($2 >= 1000000:Int32)), min($2), max($2), sum($2), count($2), sum($2)] }
          BatchExchange { order: [], dist: HashShard([0, 1]) }
            BatchProject { exprs: [$0, ToChar($2, 'yyyy-MM-dd':Varchar), $1] }
              BatchScan { table: bid, columns: [auction, price, date_time] }
  stream_plan: |
    StreamMaterialize { columns: [auction, day, total_bids, rank1_bids, rank2_bids, rank3_bids, min_price, max_price, avg_price, sum_price], pk_columns: [auction, day] }
      StreamProject { exprs: [$0, $1, $3, $4, $5, $6, $7, $8, ($9 / $10), $11] }
        StreamHashAgg { group_key: [$0, $1], aggs: [count, count, count filter(($2 < 10000:Int32)), count filter((($2 >= 10000:Int32) AND ($2 < 1000000:Int32))), count filter(($2 >= 1000000:Int32)), min($2), max($2), sum($2), count($2), sum($2)] }
          StreamExchange { dist: HashShard([0, 1]) }
            StreamProject { exprs: [$0, ToChar($2, 'yyyy-MM-dd':Varchar), $1, $3] }
              StreamTableScan { table: bid, columns: [auction, price, date_time, _row_id], pk_indices: [3] }
- id: nexmark_q18
  before:
    - create_tables
  sql: |
    SELECT auction, bidder, price, channel, url, date_time, extra
    FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY bidder, auction ORDER BY date_time DESC) AS rank_number
          FROM bid)
    WHERE rank_number <= 1;
  binder_error: 'Feature is not yet implemented: unsupported function: "row_number", Tracking issue: https://github.com/singularity-data/risingwave/issues/112'
- id: nexmark_q19
  before:
    - create_tables
  sql: |
    SELECT * FROM
    (SELECT *, ROW_NUMBER() OVER (PARTITION BY auction ORDER BY price DESC) AS rank_number FROM bid)
    WHERE rank_number <= 10;
  binder_error: 'Feature is not yet implemented: unsupported function: "row_number", Tracking issue: https://github.com/singularity-data/risingwave/issues/112'
- id: nexmark_q20
  before:
    - create_tables
  sql: |
    SELECT
        auction, bidder, price, channel, url, B.date_time as date_timeB,
        item_name, description, initial_bid, reserve, A.date_time as date_timeA, expires, seller, category
    FROM
        bid B INNER JOIN auction A on B.auction = A.id
    WHERE A.category = 10;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchHashJoin { type: Inner, predicate: $0 = $6, output_indices: [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14] }
        BatchExchange { order: [], dist: HashShard([0]) }
          BatchScan { table: bid, columns: [auction, bidder, price, channel, url, date_time] }
        BatchExchange { order: [], dist: HashShard([0]) }
          BatchFilter { predicate: ($8 = 10:Int32) }
            BatchScan { table: auction, columns: [id, item_name, description, initial_bid, reserve, date_time, expires, seller, category] }
  stream_plan: |
    StreamMaterialize { columns: [auction, bidder, price, channel, url, date_timeb, item_name, description, initial_bid, reserve, date_timea, expires, seller, category, bid._row_id(hidden), auction._row_id(hidden)], pk_columns: [bid._row_id, auction._row_id] }
      StreamExchange { dist: HashShard([14, 15]) }
        StreamHashJoin { type: Inner, predicate: $0 = $7, output_indices: [0, 1, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15, 6, 16] }
          StreamExchange { dist: HashShard([0]) }
            StreamTableScan { table: bid, columns: [auction, bidder, price, channel, url, date_time, _row_id], pk_indices: [6] }
          StreamExchange { dist: HashShard([0]) }
            StreamFilter { predicate: ($8 = 10:Int32) }
              StreamTableScan { table: auction, columns: [id, item_name, description, initial_bid, reserve, date_time, expires, seller, category, _row_id], pk_indices: [9] }
- id: nexmark_q21
  before:
    - create_tables
  sql: |
    SELECT
        auction, bidder, price, channel,
        CASE
            WHEN lower(channel) = 'apple' THEN '0'
            WHEN lower(channel) = 'google' THEN '1'
            WHEN lower(channel) = 'facebook' THEN '2'
            WHEN lower(channel) = 'baidu' THEN '3'
            ELSE REGEXP_EXTRACT(url, '(&|^)channel_id=([^&]*)', 2)
            END
        AS channel_id FROM bid
        where REGEXP_EXTRACT(url, '(&|^)channel_id=([^&]*)', 2) is not null or
              lower(channel) in ('apple', 'google', 'facebook', 'baidu');
  binder_error: 'Feature is not yet implemented: unsupported function: "regexp_extract", Tracking issue: https://github.com/singularity-data/risingwave/issues/112'
- id: nexmark_q22
  before:
    - create_tables
  sql: |
    SELECT
        auction, bidder, price, channel,
        SPLIT_PART(url, '/', 4) as dir1,
        SPLIT_PART(url, '/', 5) as dir2,
        SPLIT_PART(url, '/', 6) as dir3 FROM bid;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1, $2, $3, SplitPart($4, '/':Varchar, 4:Int32), SplitPart($4, '/':Varchar, 5:Int32), SplitPart($4, '/':Varchar, 6:Int32)] }
        BatchScan { table: bid, columns: [auction, bidder, price, channel, url] }
  stream_plan: |
    StreamMaterialize { columns: [auction, bidder, price, channel, dir1, dir2, dir3, bid._row_id(hidden)], pk_columns: [bid._row_id] }
      StreamProject { exprs: [$0, $1, $2, $3, SplitPart($4, '/':Varchar, 4:Int32), SplitPart($4, '/':Varchar, 5:Int32), SplitPart($4, '/':Varchar, 6:Int32), $5] }
        StreamTableScan { table: bid, columns: [auction, bidder, price, channel, url, _row_id], pk_indices: [5] }
