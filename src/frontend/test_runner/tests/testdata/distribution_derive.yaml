# This file is automatically generated. See `src/frontend/test_runner/README.md` for more information.
- sql: |
    create table t(a int, b int);
    select
      max(num) as max_num, a
    from (
      select
        count(*) as num, a, b
      from t
      group by a, b
    )
    group by a;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [max(count), t.a] }
        BatchHashAgg { group_key: [t.a], aggs: [max(count)] }
          BatchExchange { order: [], dist: HashShard(t.a) }
            BatchProject { exprs: [t.a, count] }
              BatchHashAgg { group_key: [t.a, t.b], aggs: [count] }
                BatchExchange { order: [], dist: HashShard(t.a, t.b) }
                  BatchScan { table: t, columns: [t.a, t.b], distribution: SomeShard }
  stream_plan: |
    StreamMaterialize { columns: [max_num, a], pk_columns: [a] }
      StreamProject { exprs: [max(max(count)), t.a] }
        StreamHashAgg { group_key: [t.a], aggs: [sum(count), max(max(count))] }
          StreamExchange { dist: HashShard(t.a) }
            StreamHashAgg { group_key: [t.a, Vnode(t.a, t.b)], aggs: [count, max(count)] }
              StreamProject { exprs: [t.a, count, t.b, Vnode(t.a, t.b)] }
                StreamProject { exprs: [t.a, count, t.b] }
                  StreamHashAgg { group_key: [t.a, t.b], aggs: [count, count] }
                    StreamExchange { dist: HashShard(t.a, t.b) }
                      StreamTableScan { table: t, columns: [t.a, t.b, t._row_id], pk: [t._row_id], distribution: SomeShard }
- sql: |
    create table t(a int, b int);
    select
      max(a) as max_a
    from (
      select
        a, a + b as ab
      from t
    )
    group by ab;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [max(t.a)] }
        BatchHashAgg { group_key: [(t.a + t.b)], aggs: [max(t.a)] }
          BatchExchange { order: [], dist: HashShard((t.a + t.b)) }
            BatchProject { exprs: [(t.a + t.b), t.a] }
              BatchScan { table: t, columns: [t.a, t.b], distribution: SomeShard }
  stream_plan: |
    StreamMaterialize { columns: [max_a, (t.a + t.b)(hidden)], pk_columns: [(t.a + t.b)] }
      StreamProject { exprs: [max(t.a), (t.a + t.b)] }
        StreamHashAgg { group_key: [(t.a + t.b)], aggs: [count, max(t.a)] }
          StreamExchange { dist: HashShard((t.a + t.b)) }
            StreamProject { exprs: [(t.a + t.b), t.a, t._row_id] }
              StreamTableScan { table: t, columns: [t.a, t.b, t._row_id], pk: [t._row_id], distribution: SomeShard }
- sql: |
    create table t1 (row_id int, uid int, v int, created_at timestamp);
    select * from hop(t1, created_at, interval '15' minute, interval '30' minute);
  logical_plan: |
    LogicalProject { exprs: [t1.row_id, t1.uid, t1.v, t1.created_at, window_start, window_end] }
      LogicalHopWindow { time_col: t1.created_at, slide: 00:15:00, size: 00:30:00, output: all }
        LogicalScan { table: t1, columns: [t1._row_id, t1.row_id, t1.uid, t1.v, t1.created_at] }
  optimized_logical_plan: |
    LogicalHopWindow { time_col: t1.created_at, slide: 00:15:00, size: 00:30:00, output: all }
      LogicalScan { table: t1, columns: [t1.row_id, t1.uid, t1.v, t1.created_at] }
  batch_plan: |
    BatchHopWindow { time_col: t1.created_at, slide: 00:15:00, size: 00:30:00, output: all }
      BatchExchange { order: [], dist: Single }
        BatchScan { table: t1, columns: [t1.row_id, t1.uid, t1.v, t1.created_at], distribution: SomeShard }
  stream_plan: |
    StreamMaterialize { columns: [row_id, uid, v, created_at, window_start, window_end, t1._row_id(hidden)], pk_columns: [t1._row_id, window_start] }
      StreamExchange { dist: HashShard(window_start, t1._row_id) }
        StreamHopWindow { time_col: t1.created_at, slide: 00:15:00, size: 00:30:00, output: [t1.row_id, t1.uid, t1.v, t1.created_at, window_start, window_end, t1._row_id] }
          StreamTableScan { table: t1, columns: [t1.row_id, t1.uid, t1.v, t1.created_at, t1._row_id], pk: [t1._row_id], distribution: SomeShard }
