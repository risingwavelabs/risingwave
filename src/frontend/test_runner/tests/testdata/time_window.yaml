# This file is automatically generated. See `src/frontend/test_runner/README.md` for more information.
- sql: |
    create table t1 (id int, created_at date);
    select * from tumble(t1, created_at, interval '3' day);
  logical_plan: |
    LogicalProject { exprs: [$1, $2, $3, $4] }
      LogicalProject { exprs: [$0, $1, $2, TumbleStart($2, '3 days 00:00:00':Interval), (TumbleStart($2, '3 days 00:00:00':Interval) + '3 days 00:00:00':Interval)] }
        LogicalScan { table: t1, columns: [_row_id, id, created_at] }
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1, TumbleStart($1, '3 days 00:00:00':Interval), (TumbleStart($1, '3 days 00:00:00':Interval) + '3 days 00:00:00':Interval)] }
        BatchScan { table: t1, columns: [id, created_at] }
- sql: |
    create materialized view t as select * from s;
    select * from tumble(t, (country).created_at, interval '3' day);
  binder_error: 'Bind error: the 2st arg of window table function should be time_col'
  create_source:
    row_format: protobuf
    name: s
    file: |
        syntax = "proto3";
        package test;
        message TestRecord {
          int32 id = 1;
          Country country = 3;
          int64 zipcode = 4;
          float rate = 5;
        }
        message Country {
          string address = 1;
          City city = 2;
          string zipcode = 3;
          string created_at = 4;
        }
        message City {
          string address = 1;
          string zipcode = 2;
        }
- sql: |
    create table t1 (id int, created_at date);
    select * from hop(t1, created_at, interval '1' day, interval '3' day);
  logical_plan: |
    LogicalProject { exprs: [$1, $2, $3, $4] }
      LogicalHopWindow { time_col: $2, slide: 1 day 00:00:00, size: 3 days 00:00:00, output_indices: all }
        LogicalScan { table: t1, columns: [_row_id, id, created_at] }
  stream_plan: |
    StreamMaterialize { columns: [id, created_at, window_start, window_end, t1._row_id(hidden)], pk_columns: [t1._row_id, window_start] }
      StreamHopWindow { time_col: $1, slide: 1 day 00:00:00, size: 3 days 00:00:00, output_indices: [0, 1, 3, 4, 2] }
        StreamTableScan { table: t1, columns: [id, created_at, _row_id], pk_indices: [2] }
- sql: |
    create table t1 (id int, created_at date);
    select id, created_at, window_start from hop(t1, created_at, interval '1' day, interval '3' day);
  logical_plan: |
    LogicalProject { exprs: [$1, $2, $3] }
      LogicalHopWindow { time_col: $2, slide: 1 day 00:00:00, size: 3 days 00:00:00, output_indices: all }
        LogicalScan { table: t1, columns: [_row_id, id, created_at] }
  stream_plan: |
    StreamMaterialize { columns: [id, created_at, window_start, t1._row_id(hidden)], pk_columns: [t1._row_id, window_start] }
      StreamHopWindow { time_col: $1, slide: 1 day 00:00:00, size: 3 days 00:00:00, output_indices: [0, 1, 3, 2] }
        StreamTableScan { table: t1, columns: [id, created_at, _row_id], pk_indices: [2] }
- sql: |
    create table t1 (id int, created_at date);
    select id, created_at, window_end from hop(t1, created_at, interval '1' day, interval '3' day);
  logical_plan: |
    LogicalProject { exprs: [$1, $2, $4] }
      LogicalHopWindow { time_col: $2, slide: 1 day 00:00:00, size: 3 days 00:00:00, output_indices: all }
        LogicalScan { table: t1, columns: [_row_id, id, created_at] }
  stream_plan: |
    StreamMaterialize { columns: [id, created_at, window_end, t1._row_id(hidden)], pk_columns: [t1._row_id, window_end] }
      StreamHopWindow { time_col: $1, slide: 1 day 00:00:00, size: 3 days 00:00:00, output_indices: [0, 1, 4, 2] }
        StreamTableScan { table: t1, columns: [id, created_at, _row_id], pk_indices: [2] }
- sql: |
    create table t1 (id int, created_at date);
    select id, created_at from hop(t1, created_at, interval '1' day, interval '3' day);
  logical_plan: |
    LogicalProject { exprs: [$1, $2] }
      LogicalHopWindow { time_col: $2, slide: 1 day 00:00:00, size: 3 days 00:00:00, output_indices: all }
        LogicalScan { table: t1, columns: [_row_id, id, created_at] }
  batch_plan: |
    BatchHopWindow { time_col: $1, slide: 1 day 00:00:00, size: 3 days 00:00:00, output_indices: [0, 1] }
      BatchExchange { order: [], dist: Single }
        BatchScan { table: t1, columns: [id, created_at] }
  stream_plan: |
    StreamMaterialize { columns: [id, created_at, window_start(hidden), t1._row_id(hidden)], pk_columns: [t1._row_id, window_start] }
      StreamHopWindow { time_col: $1, slide: 1 day 00:00:00, size: 3 days 00:00:00, output_indices: [0, 1, 3, 2] }
        StreamTableScan { table: t1, columns: [id, created_at, _row_id], pk_indices: [2] }
- sql: |
    create table t1 (id int, created_at date);
    select t_hop.id, t_hop.created_at from hop(t1, created_at, interval '1' day, interval '3' day) as t_hop;
  logical_plan: |
    LogicalProject { exprs: [$1, $2] }
      LogicalHopWindow { time_col: $2, slide: 1 day 00:00:00, size: 3 days 00:00:00, output_indices: all }
        LogicalScan { table: t1, columns: [_row_id, id, created_at] }
  batch_plan: |
    BatchHopWindow { time_col: $1, slide: 1 day 00:00:00, size: 3 days 00:00:00, output_indices: [0, 1] }
      BatchExchange { order: [], dist: Single }
        BatchScan { table: t1, columns: [id, created_at] }
  stream_plan: |
    StreamMaterialize { columns: [id, created_at, window_start(hidden), t1._row_id(hidden)], pk_columns: [t1._row_id, window_start] }
      StreamHopWindow { time_col: $1, slide: 1 day 00:00:00, size: 3 days 00:00:00, output_indices: [0, 1, 3, 2] }
        StreamTableScan { table: t1, columns: [id, created_at, _row_id], pk_indices: [2] }
- sql: |
    create table t (v1 varchar, v2 timestamp, v3 float);
    select v1, window_end, avg(v3) as avg from hop( t, v2, interval '1' minute, interval '10' minute) group by v1, window_end;
  logical_plan: |
    LogicalProject { exprs: [$0, $1, ($2 / $3)] }
      LogicalAgg { group_key: [0, 1], agg_calls: [sum($2), count($2)] }
        LogicalProject { exprs: [$1, $5, $3] }
          LogicalHopWindow { time_col: $2, slide: 00:01:00, size: 00:10:00, output_indices: all }
            LogicalScan { table: t, columns: [_row_id, v1, v2, v3] }
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1, ($2 / $3)] }
        BatchHashAgg { group_key: [$0, $1], aggs: [sum($2), count($2)] }
          BatchExchange { order: [], dist: HashShard([0, 1]) }
            BatchProject { exprs: [$0, $2, $1] }
              BatchHopWindow { time_col: $1, slide: 00:01:00, size: 00:10:00, output_indices: [0, 2, 4] }
                BatchScan { table: t, columns: [v1, v2, v3] }
  stream_plan: |
    StreamMaterialize { columns: [v1, window_end, avg], pk_columns: [v1, window_end] }
      StreamProject { exprs: [$0, $1, ($3 / $4)] }
        StreamHashAgg { group_key: [$0, $1], aggs: [count, sum($2), count($2)] }
          StreamExchange { dist: HashShard([0, 1]) }
            StreamProject { exprs: [$0, $2, $1, $3] }
              StreamHopWindow { time_col: $1, slide: 00:01:00, size: 00:10:00, output_indices: [0, 2, 5, 3] }
                StreamTableScan { table: t, columns: [v1, v2, v3, _row_id], pk_indices: [3] }
- sql: |
    create table t1 (id int, v1 int, created_at date);
    with t2 as (select * from t1 where v1 >= 10)
    select * from tumble(t1, created_at, interval '3' day);
  logical_plan: |
    LogicalProject { exprs: [$1, $2, $3, $4, $5] }
      LogicalProject { exprs: [$0, $1, $2, $3, TumbleStart($3, '3 days 00:00:00':Interval), (TumbleStart($3, '3 days 00:00:00':Interval) + '3 days 00:00:00':Interval)] }
        LogicalScan { table: t1, columns: [_row_id, id, v1, created_at] }
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1, $2, TumbleStart($2, '3 days 00:00:00':Interval), (TumbleStart($2, '3 days 00:00:00':Interval) + '3 days 00:00:00':Interval)] }
        BatchScan { table: t1, columns: [id, v1, created_at] }
  stream_plan: |
    StreamMaterialize { columns: [id, v1, created_at, window_start, window_end, t1._row_id(hidden)], pk_columns: [t1._row_id] }
      StreamProject { exprs: [$0, $1, $2, TumbleStart($2, '3 days 00:00:00':Interval), (TumbleStart($2, '3 days 00:00:00':Interval) + '3 days 00:00:00':Interval), $3] }
        StreamTableScan { table: t1, columns: [id, v1, created_at, _row_id], pk_indices: [3] }
- sql: |
    create table t1 (id int, v1 int, created_at date);
    with t2 as (select * from t1 where v1 >= 10)
    select * from hop(t2, created_at, interval '1' day, interval '3' day);
  logical_plan: |
    LogicalProject { exprs: [$0, $1, $2, $3, $4] }
      LogicalHopWindow { time_col: $2, slide: 1 day 00:00:00, size: 3 days 00:00:00, output_indices: all }
        LogicalProject { exprs: [$1, $2, $3] }
          LogicalFilter { predicate: ($2 >= 10:Int32) }
            LogicalScan { table: t1, columns: [_row_id, id, v1, created_at] }
  batch_plan: |
    BatchHopWindow { time_col: $2, slide: 1 day 00:00:00, size: 3 days 00:00:00, output_indices: all }
      BatchExchange { order: [], dist: Single }
        BatchFilter { predicate: ($1 >= 10:Int32) }
          BatchScan { table: t1, columns: [id, v1, created_at] }
  stream_plan: |
    StreamMaterialize { columns: [id, v1, created_at, window_start, window_end, t1._row_id(hidden)], pk_columns: [t1._row_id, window_start] }
      StreamHopWindow { time_col: $2, slide: 1 day 00:00:00, size: 3 days 00:00:00, output_indices: [0, 1, 2, 4, 5, 3] }
        StreamFilter { predicate: ($1 >= 10:Int32) }
          StreamTableScan { table: t1, columns: [id, v1, created_at, _row_id], pk_indices: [3] }
