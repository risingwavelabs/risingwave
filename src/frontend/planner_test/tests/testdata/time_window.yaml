# This file is automatically generated. See `src/frontend/planner_test/README.md` for more information.
- sql: |
    create table t1 (id int, created_at date);
    select * from tumble(t1, created_at, interval '3' day);
  logical_plan: |
    LogicalProject { exprs: [t1.id, t1.created_at, TumbleStart(t1.created_at, '3 days 00:00:00':Interval), (TumbleStart(t1.created_at, '3 days 00:00:00':Interval) + '3 days 00:00:00':Interval)] }
    └─LogicalProject { exprs: [t1.id, t1.created_at, t1._row_id, TumbleStart(t1.created_at, '3 days 00:00:00':Interval), (TumbleStart(t1.created_at, '3 days 00:00:00':Interval) + '3 days 00:00:00':Interval)] }
      └─LogicalScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id] }
  batch_plan: |
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t1.id, t1.created_at, TumbleStart(t1.created_at, '3 days 00:00:00':Interval), (TumbleStart(t1.created_at, '3 days 00:00:00':Interval) + '3 days 00:00:00':Interval)] }
      └─BatchScan { table: t1, columns: [t1.id, t1.created_at], distribution: SomeShard }
- sql: |
    create materialized view t as select * from s;
    select * from tumble(t, (country).created_at, interval '3' day);
  binder_error: 'Bind error: the 2st arg of window table function should be a timestamp
    or date column'
  create_source:
    row_format: protobuf
    name: s
    file: |
      syntax = "proto3";
      package test;
      message TestRecord {
        int32 id = 1;
        Country country = 3;
        int64 zipcode = 4;
        float rate = 5;
      }
      message Country {
        string address = 1;
        City city = 2;
        string zipcode = 3;
        string created_at = 4;
      }
      message City {
        string address = 1;
        string zipcode = 2;
      }
- sql: |
    create table t1 (id int, created_at varchar);
    select * from hop(t1, created_at, interval '2' day, interval '4' day);
  binder_error: 'Bind error: the 2st arg of window table function should be a timestamp
    or date column'
- sql: |
    create table t1 (id int, created_at date);
    select * from hop(t1, created_at, interval '2' day, interval '3' day);
  planner_error: 'Bind error: Invalid arguments for HOP window function: window_size
    3 days 00:00:00 cannot be divided by window_slide 2 days 00:00:00'
- sql: |
    create table t1 (id int, created_at date);
    select * from hop(t1, created_at, interval '-1' day, interval '1' day);
  planner_error: 'Bind error: window_size 1 day 00:00:00 and window_slide -1 days
    00:00:00 must be positive'
- sql: |
    create table t1 (id int, created_at date);
    select * from hop(t1, created_at, interval '0' day, interval '1' day);
  planner_error: 'Bind error: window_size 1 day 00:00:00 and window_slide 00:00:00
    must be positive'
- sql: |
    create table t1 (id int, created_at date);
    select * from hop(t1, created_at, interval '-1' day, interval '-1' day);
  planner_error: 'Bind error: window_size -1 days 00:00:00 and window_slide -1 days
    00:00:00 must be positive'
- sql: |
    create table t1 (id int, created_at date);
    select * from hop(t1, created_at, interval '1' day, interval '3' day);
  logical_plan: |
    LogicalProject { exprs: [t1.id, t1.created_at, window_start, window_end] }
    └─LogicalHopWindow { time_col: t1.created_at, slide: 1 day 00:00:00, size: 3 days 00:00:00, output: all }
      └─LogicalScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id] }
  stream_plan: |
    StreamMaterialize { columns: [id, created_at, window_start, window_end, t1._row_id(hidden)], pk_columns: [t1._row_id, window_start, window_end] }
    └─StreamHopWindow { time_col: t1.created_at, slide: 1 day 00:00:00, size: 3 days 00:00:00, output: [t1.id, t1.created_at, window_start, window_end, t1._row_id] }
      └─StreamTableScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id], pk: [t1._row_id], dist: UpstreamHashShard(t1._row_id) }
- sql: |
    create table t1 (id int, created_at date);
    select id, created_at, window_start from hop(t1, created_at, interval '1' day, interval '3' day);
  logical_plan: |
    LogicalProject { exprs: [t1.id, t1.created_at, window_start] }
    └─LogicalHopWindow { time_col: t1.created_at, slide: 1 day 00:00:00, size: 3 days 00:00:00, output: all }
      └─LogicalScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id] }
  stream_plan: |
    StreamMaterialize { columns: [id, created_at, window_start, t1._row_id(hidden)], pk_columns: [t1._row_id, window_start] }
    └─StreamHopWindow { time_col: t1.created_at, slide: 1 day 00:00:00, size: 3 days 00:00:00, output: [t1.id, t1.created_at, window_start, t1._row_id] }
      └─StreamTableScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id], pk: [t1._row_id], dist: UpstreamHashShard(t1._row_id) }
- sql: |
    create table t1 (id int, created_at date);
    select id, created_at, window_end from hop(t1, created_at, interval '1' day, interval '3' day);
  logical_plan: |
    LogicalProject { exprs: [t1.id, t1.created_at, window_end] }
    └─LogicalHopWindow { time_col: t1.created_at, slide: 1 day 00:00:00, size: 3 days 00:00:00, output: all }
      └─LogicalScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id] }
  stream_plan: |
    StreamMaterialize { columns: [id, created_at, window_end, t1._row_id(hidden)], pk_columns: [t1._row_id, window_end] }
    └─StreamHopWindow { time_col: t1.created_at, slide: 1 day 00:00:00, size: 3 days 00:00:00, output: [t1.id, t1.created_at, window_end, t1._row_id] }
      └─StreamTableScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id], pk: [t1._row_id], dist: UpstreamHashShard(t1._row_id) }
- sql: |
    create table t1 (id int, created_at date);
    select id, created_at from hop(t1, created_at, interval '1' day, interval '3' day);
  logical_plan: |
    LogicalProject { exprs: [t1.id, t1.created_at] }
    └─LogicalHopWindow { time_col: t1.created_at, slide: 1 day 00:00:00, size: 3 days 00:00:00, output: all }
      └─LogicalScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id] }
  batch_plan: |
    BatchHopWindow { time_col: t1.created_at, slide: 1 day 00:00:00, size: 3 days 00:00:00, output: [t1.id, t1.created_at] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchScan { table: t1, columns: [t1.id, t1.created_at], distribution: SomeShard }
  stream_plan: |
    StreamMaterialize { columns: [id, created_at, window_start(hidden), t1._row_id(hidden)], pk_columns: [t1._row_id, window_start] }
    └─StreamHopWindow { time_col: t1.created_at, slide: 1 day 00:00:00, size: 3 days 00:00:00, output: [t1.id, t1.created_at, window_start, t1._row_id] }
      └─StreamTableScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id], pk: [t1._row_id], dist: UpstreamHashShard(t1._row_id) }
- sql: |
    create table t1 (id int, created_at date);
    select t_hop.id, t_hop.created_at from hop(t1, created_at, interval '1' day, interval '3' day) as t_hop;
  logical_plan: |
    LogicalProject { exprs: [t1.id, t1.created_at] }
    └─LogicalHopWindow { time_col: t1.created_at, slide: 1 day 00:00:00, size: 3 days 00:00:00, output: all }
      └─LogicalScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id] }
  batch_plan: |
    BatchHopWindow { time_col: t1.created_at, slide: 1 day 00:00:00, size: 3 days 00:00:00, output: [t1.id, t1.created_at] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchScan { table: t1, columns: [t1.id, t1.created_at], distribution: SomeShard }
  stream_plan: |
    StreamMaterialize { columns: [id, created_at, window_start(hidden), t1._row_id(hidden)], pk_columns: [t1._row_id, window_start] }
    └─StreamHopWindow { time_col: t1.created_at, slide: 1 day 00:00:00, size: 3 days 00:00:00, output: [t1.id, t1.created_at, window_start, t1._row_id] }
      └─StreamTableScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id], pk: [t1._row_id], dist: UpstreamHashShard(t1._row_id) }
- sql: |
    create table t (v1 varchar, v2 timestamp, v3 float);
    select v1, window_end, avg(v3) as avg from hop( t, v2, interval '1' minute, interval '10' minute) group by v1, window_end;
  logical_plan: |
    LogicalProject { exprs: [t.v1, window_end, (sum(t.v3) / count(t.v3))] }
    └─LogicalAgg { group_key: [t.v1, window_end], aggs: [sum(t.v3), count(t.v3)] }
      └─LogicalProject { exprs: [t.v1, window_end, t.v3] }
        └─LogicalHopWindow { time_col: t.v2, slide: 00:01:00, size: 00:10:00, output: all }
          └─LogicalScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id] }
  batch_plan: |
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.v1, window_end, (sum(t.v3) / count(t.v3))] }
      └─BatchHashAgg { group_key: [t.v1, window_end], aggs: [sum(t.v3), count(t.v3)] }
        └─BatchExchange { order: [], dist: HashShard(t.v1, window_end) }
          └─BatchProject { exprs: [t.v1, window_end, t.v3] }
            └─BatchHopWindow { time_col: t.v2, slide: 00:01:00, size: 00:10:00, output: [t.v1, t.v3, window_end] }
              └─BatchScan { table: t, columns: [t.v1, t.v2, t.v3], distribution: SomeShard }
  stream_plan: |
    StreamMaterialize { columns: [v1, window_end, avg], pk_columns: [v1, window_end] }
    └─StreamProject { exprs: [t.v1, window_end, (sum(t.v3) / count(t.v3))] }
      └─StreamHashAgg { group_key: [t.v1, window_end], aggs: [count, sum(t.v3), count(t.v3)] }
        └─StreamExchange { dist: HashShard(t.v1, window_end) }
          └─StreamProject { exprs: [t.v1, window_end, t.v3, t._row_id] }
            └─StreamHopWindow { time_col: t.v2, slide: 00:01:00, size: 00:10:00, output: [t.v1, t.v3, window_end, t._row_id] }
              └─StreamTableScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id], pk: [t._row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t1 (id int, v1 int, created_at date);
    with t2 as (select * from t1 where v1 >= 10)
    select * from tumble(t2, created_at, interval '3' day);
  logical_plan: |
    LogicalProject { exprs: [t1.id, t1.v1, t1.created_at, TumbleStart(t1.created_at, '3 days 00:00:00':Interval), (TumbleStart(t1.created_at, '3 days 00:00:00':Interval) + '3 days 00:00:00':Interval)] }
    └─LogicalProject { exprs: [t1.id, t1.v1, t1.created_at, TumbleStart(t1.created_at, '3 days 00:00:00':Interval), (TumbleStart(t1.created_at, '3 days 00:00:00':Interval) + '3 days 00:00:00':Interval)] }
      └─LogicalProject { exprs: [t1.id, t1.v1, t1.created_at] }
        └─LogicalFilter { predicate: (t1.v1 >= 10:Int32) }
          └─LogicalScan { table: t1, columns: [t1.id, t1.v1, t1.created_at, t1._row_id] }
  batch_plan: |
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t1.id, t1.v1, t1.created_at, TumbleStart(t1.created_at, '3 days 00:00:00':Interval), (TumbleStart(t1.created_at, '3 days 00:00:00':Interval) + '3 days 00:00:00':Interval)] }
      └─BatchFilter { predicate: (t1.v1 >= 10:Int32) }
        └─BatchScan { table: t1, columns: [t1.id, t1.v1, t1.created_at], distribution: SomeShard }
  stream_plan: |
    StreamMaterialize { columns: [id, v1, created_at, window_start, window_end, t1._row_id(hidden)], pk_columns: [t1._row_id] }
    └─StreamProject { exprs: [t1.id, t1.v1, t1.created_at, TumbleStart(t1.created_at, '3 days 00:00:00':Interval), (TumbleStart(t1.created_at, '3 days 00:00:00':Interval) + '3 days 00:00:00':Interval), t1._row_id] }
      └─StreamFilter { predicate: (t1.v1 >= 10:Int32) }
        └─StreamTableScan { table: t1, columns: [t1.id, t1.v1, t1.created_at, t1._row_id], pk: [t1._row_id], dist: UpstreamHashShard(t1._row_id) }
- sql: |
    create table t1 (id int, v1 int, created_at date);
    with t2 as (select * from t1 where v1 >= 10)
    select * from hop(t2, created_at, interval '1' day, interval '3' day);
  logical_plan: |
    LogicalProject { exprs: [t1.id, t1.v1, t1.created_at, window_start, window_end] }
    └─LogicalHopWindow { time_col: t1.created_at, slide: 1 day 00:00:00, size: 3 days 00:00:00, output: all }
      └─LogicalProject { exprs: [t1.id, t1.v1, t1.created_at] }
        └─LogicalFilter { predicate: (t1.v1 >= 10:Int32) }
          └─LogicalScan { table: t1, columns: [t1.id, t1.v1, t1.created_at, t1._row_id] }
  batch_plan: |
    BatchHopWindow { time_col: t1.created_at, slide: 1 day 00:00:00, size: 3 days 00:00:00, output: all }
    └─BatchExchange { order: [], dist: Single }
      └─BatchFilter { predicate: (t1.v1 >= 10:Int32) }
        └─BatchScan { table: t1, columns: [t1.id, t1.v1, t1.created_at], distribution: SomeShard }
  stream_plan: |
    StreamMaterialize { columns: [id, v1, created_at, window_start, window_end, t1._row_id(hidden)], pk_columns: [t1._row_id, window_start, window_end] }
    └─StreamHopWindow { time_col: t1.created_at, slide: 1 day 00:00:00, size: 3 days 00:00:00, output: [t1.id, t1.v1, t1.created_at, window_start, window_end, t1._row_id] }
      └─StreamFilter { predicate: (t1.v1 >= 10:Int32) }
        └─StreamTableScan { table: t1, columns: [t1.id, t1.v1, t1.created_at, t1._row_id], pk: [t1._row_id], dist: UpstreamHashShard(t1._row_id) }
- sql: |
    with t(ts) as (values ('2020-01-01 12:00:00'::timestamp)) select * from tumble(t, ts, interval '10' second) as z;
  logical_plan: |
    LogicalProject { exprs: [*VALUES*_0.column_0, TumbleStart(*VALUES*_0.column_0, '00:00:10':Interval), (TumbleStart(*VALUES*_0.column_0, '00:00:10':Interval) + '00:00:10':Interval)] }
    └─LogicalProject { exprs: [*VALUES*_0.column_0, TumbleStart(*VALUES*_0.column_0, '00:00:10':Interval), (TumbleStart(*VALUES*_0.column_0, '00:00:10':Interval) + '00:00:10':Interval)] }
      └─LogicalValues { rows: [['2020-01-01 12:00:00':Varchar::Timestamp]], schema: Schema { fields: [*VALUES*_0.column_0:Timestamp] } }
  batch_plan: |
    BatchProject { exprs: [*VALUES*_0.column_0, TumbleStart(*VALUES*_0.column_0, '00:00:10':Interval), (TumbleStart(*VALUES*_0.column_0, '00:00:10':Interval) + '00:00:10':Interval)] }
    └─BatchValues { rows: [['2020-01-01 12:00:00':Varchar::Timestamp]] }
