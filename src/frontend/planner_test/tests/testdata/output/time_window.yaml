# This file is automatically generated. See `src/frontend/planner_test/README.md` for more information.
- sql: |
    create table t1 (id int, created_at date);
    select * from tumble(t1, created_at, interval '3' day);
  logical_plan: |-
    LogicalProject { exprs: [t1.id, t1.created_at, $expr1, $expr2] }
    └──LogicalProject { exprs: [t1.id, t1.created_at, t1._row_id, TumbleStart(t1.created_at, '3 days':Interval) as $expr1, (TumbleStart(t1.created_at, '3 days':Interval) + '3 days':Interval) as $expr2] }
       └──LogicalScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id] }
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └──BatchProject { exprs: [t1.id, t1.created_at, TumbleStart(t1.created_at, '3 days':Interval) as $expr1, (TumbleStart(t1.created_at, '3 days':Interval) + '3 days':Interval) as $expr2] }
       └──BatchScan { table: t1, columns: [t1.id, t1.created_at], distribution: SomeShard }
- sql: |
    create materialized view t as select * from s;
    select * from tumble(t, (country).created_at, interval '3' day);
  binder_error: 'Bind error: The 2st arg of window table function should be a column name but not complex expression. Consider using an intermediate CTE or view as workaround.'
  create_source:
    row_format: protobuf
    name: s
    file: |
      syntax = "proto3";
      package test;
      message TestRecord {
        int32 id = 1;
        Country country = 3;
        int64 zipcode = 4;
        float rate = 5;
      }
      message Country {
        string address = 1;
        City city = 2;
        string zipcode = 3;
        string created_at = 4;
      }
      message City {
        string address = 1;
        string zipcode = 2;
      }
- sql: |
    create table t1 (id int, created_at varchar);
    select * from hop(t1, created_at, interval '2' day, interval '4' day);
  binder_error: 'Bind error: The 2st arg of window table function should be a column of type timestamp with time zone, timestamp or date.'
- sql: |
    create table t1 (id int, created_at date);
    select * from hop(t1, created_at, interval '2' day, interval '3' day);
  planner_error: 'Bind error: Invalid arguments for HOP window function: window_size 3 days cannot be divided by window_slide 2 days'
- sql: |
    create table t1 (id int, created_at date);
    select * from hop(t1, created_at, interval '-1' day, interval '1' day);
  planner_error: 'Bind error: window_size 1 day and window_slide -1 days must be positive'
- sql: |
    create table t1 (id int, created_at date);
    select * from hop(t1, created_at, interval '0' day, interval '1' day);
  planner_error: 'Bind error: window_size 1 day and window_slide 00:00:00 must be positive'
- sql: |
    create table t1 (id int, created_at date);
    select * from hop(t1, created_at, interval '-1' day, interval '-1' day);
  planner_error: 'Bind error: window_size -1 days and window_slide -1 days must be positive'
- sql: |
    create table t1 (id int, created_at date);
    select * from hop(t1, created_at, interval '1' day, interval '3' day);
  logical_plan: |-
    LogicalProject { exprs: [t1.id, t1.created_at, window_start, window_end] }
    └──LogicalHopWindow { time_col: t1.created_at, slide: Interval { months: 0, days: 1, usecs: 0 }, size: Interval { months: 0, days: 3, usecs: 0 }, output: all }
       └──LogicalFilter { predicate: IsNotNull(t1.created_at) }
          └──LogicalScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id] }
  stream_plan: |-
    StreamMaterialize { columns: [id, created_at, window_start, window_end, t1._row_id(hidden)], stream_key: [t1._row_id, window_start, window_end], pk_columns: [t1._row_id, window_start, window_end], pk_conflict: NoCheck }
    └──StreamHopWindow { time_col: t1.created_at, slide: Interval { months: 0, days: 1, usecs: 0 }, size: Interval { months: 0, days: 3, usecs: 0 }, output: [t1.id, t1.created_at, window_start, window_end, t1._row_id] }
       └──StreamFilter { predicate: IsNotNull(t1.created_at) }
          └──StreamTableScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id], pk: [t1._row_id], dist: UpstreamHashShard(t1._row_id) }
- sql: |
    create table t1 (id int, created_at date);
    select id, created_at, window_start from hop(t1, created_at, interval '1' day, interval '3' day);
  logical_plan: |-
    LogicalProject { exprs: [t1.id, t1.created_at, window_start] }
    └──LogicalHopWindow { time_col: t1.created_at, slide: Interval { months: 0, days: 1, usecs: 0 }, size: Interval { months: 0, days: 3, usecs: 0 }, output: all }
       └──LogicalFilter { predicate: IsNotNull(t1.created_at) }
          └──LogicalScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id] }
  stream_plan: |-
    StreamMaterialize { columns: [id, created_at, window_start, t1._row_id(hidden)], stream_key: [t1._row_id, window_start], pk_columns: [t1._row_id, window_start], pk_conflict: NoCheck }
    └──StreamHopWindow { time_col: t1.created_at, slide: Interval { months: 0, days: 1, usecs: 0 }, size: Interval { months: 0, days: 3, usecs: 0 }, output: [t1.id, t1.created_at, window_start, t1._row_id] }
       └──StreamFilter { predicate: IsNotNull(t1.created_at) }
          └──StreamTableScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id], pk: [t1._row_id], dist: UpstreamHashShard(t1._row_id) }
- sql: |
    create table t1 (id int, created_at date);
    select id, created_at, window_end from hop(t1, created_at, interval '1' day, interval '3' day);
  logical_plan: |-
    LogicalProject { exprs: [t1.id, t1.created_at, window_end] }
    └──LogicalHopWindow { time_col: t1.created_at, slide: Interval { months: 0, days: 1, usecs: 0 }, size: Interval { months: 0, days: 3, usecs: 0 }, output: all }
       └──LogicalFilter { predicate: IsNotNull(t1.created_at) }
          └──LogicalScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id] }
  stream_plan: |-
    StreamMaterialize { columns: [id, created_at, window_end, t1._row_id(hidden)], stream_key: [t1._row_id, window_end], pk_columns: [t1._row_id, window_end], pk_conflict: NoCheck }
    └──StreamHopWindow { time_col: t1.created_at, slide: Interval { months: 0, days: 1, usecs: 0 }, size: Interval { months: 0, days: 3, usecs: 0 }, output: [t1.id, t1.created_at, window_end, t1._row_id] }
       └──StreamFilter { predicate: IsNotNull(t1.created_at) }
          └──StreamTableScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id], pk: [t1._row_id], dist: UpstreamHashShard(t1._row_id) }
- sql: |
    create table t1 (id int, created_at date);
    select id, created_at from hop(t1, created_at, interval '1' day, interval '3' day);
  logical_plan: |-
    LogicalProject { exprs: [t1.id, t1.created_at] }
    └──LogicalHopWindow { time_col: t1.created_at, slide: Interval { months: 0, days: 1, usecs: 0 }, size: Interval { months: 0, days: 3, usecs: 0 }, output: all }
       └──LogicalFilter { predicate: IsNotNull(t1.created_at) }
          └──LogicalScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id] }
  batch_plan: |-
    BatchHopWindow { time_col: t1.created_at, slide: Interval { months: 0, days: 1, usecs: 0 }, size: Interval { months: 0, days: 3, usecs: 0 }, output: [t1.id, t1.created_at] }
    └──BatchExchange { order: [], dist: Single }
       └──BatchFilter { predicate: IsNotNull(t1.created_at) }
          └──BatchScan { table: t1, columns: [t1.id, t1.created_at], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [id, created_at, window_start(hidden), t1._row_id(hidden)], stream_key: [t1._row_id, window_start], pk_columns: [t1._row_id, window_start], pk_conflict: NoCheck }
    └──StreamHopWindow { time_col: t1.created_at, slide: Interval { months: 0, days: 1, usecs: 0 }, size: Interval { months: 0, days: 3, usecs: 0 }, output: [t1.id, t1.created_at, window_start, t1._row_id] }
       └──StreamFilter { predicate: IsNotNull(t1.created_at) }
          └──StreamTableScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id], pk: [t1._row_id], dist: UpstreamHashShard(t1._row_id) }
- sql: |
    create table t1 (id int, created_at date);
    select t_hop.id, t_hop.created_at from hop(t1, created_at, interval '1' day, interval '3' day) as t_hop;
  logical_plan: |-
    LogicalProject { exprs: [t1.id, t1.created_at] }
    └──LogicalHopWindow { time_col: t1.created_at, slide: Interval { months: 0, days: 1, usecs: 0 }, size: Interval { months: 0, days: 3, usecs: 0 }, output: all }
       └──LogicalFilter { predicate: IsNotNull(t1.created_at) }
          └──LogicalScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id] }
  batch_plan: |-
    BatchHopWindow { time_col: t1.created_at, slide: Interval { months: 0, days: 1, usecs: 0 }, size: Interval { months: 0, days: 3, usecs: 0 }, output: [t1.id, t1.created_at] }
    └──BatchExchange { order: [], dist: Single }
       └──BatchFilter { predicate: IsNotNull(t1.created_at) }
          └──BatchScan { table: t1, columns: [t1.id, t1.created_at], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [id, created_at, window_start(hidden), t1._row_id(hidden)], stream_key: [t1._row_id, window_start], pk_columns: [t1._row_id, window_start], pk_conflict: NoCheck }
    └──StreamHopWindow { time_col: t1.created_at, slide: Interval { months: 0, days: 1, usecs: 0 }, size: Interval { months: 0, days: 3, usecs: 0 }, output: [t1.id, t1.created_at, window_start, t1._row_id] }
       └──StreamFilter { predicate: IsNotNull(t1.created_at) }
          └──StreamTableScan { table: t1, columns: [t1.id, t1.created_at, t1._row_id], pk: [t1._row_id], dist: UpstreamHashShard(t1._row_id) }
- sql: |
    create table t (v1 varchar, v2 timestamp, v3 float);
    select v1, window_end, avg(v3) as avg from hop( t, v2, interval '1' minute, interval '10' minute) group by v1, window_end;
  logical_plan: |-
    LogicalProject { exprs: [t.v1, window_end, (sum(t.v3) / count(t.v3)::Float64) as $expr1] }
    └──LogicalAgg { group_key: [t.v1, window_end], aggs: [sum(t.v3), count(t.v3)] }
       └──LogicalProject { exprs: [t.v1, window_end, t.v3] }
          └──LogicalHopWindow { time_col: t.v2, slide: Interval { months: 0, days: 0, usecs: 60000000 }, size: Interval { months: 0, days: 0, usecs: 600000000 }, output: all }
             └──LogicalFilter { predicate: IsNotNull(t.v2) }
                └──LogicalScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id] }
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └──BatchProject { exprs: [t.v1, window_end, (sum(t.v3) / count(t.v3)::Float64) as $expr1] }
       └──BatchHashAgg { group_key: [t.v1, window_end], aggs: [sum(t.v3), count(t.v3)] }
          └──BatchHopWindow { time_col: t.v2, slide: Interval { months: 0, days: 0, usecs: 60000000 }, size: Interval { months: 0, days: 0, usecs: 600000000 }, output: [t.v1, t.v3, window_end] }
             └──BatchExchange { order: [], dist: HashShard(t.v1) }
                └──BatchFilter { predicate: IsNotNull(t.v2) }
                   └──BatchScan { table: t, columns: [t.v1, t.v2, t.v3], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [v1, window_end, avg], stream_key: [v1, window_end], pk_columns: [v1, window_end], pk_conflict: NoCheck }
    └──StreamProject { exprs: [t.v1, window_end, (sum(t.v3) / count(t.v3)::Float64) as $expr1] }
       └──StreamHashAgg { group_key: [t.v1, window_end], aggs: [sum(t.v3), count(t.v3), count] }
          └──StreamExchange { dist: HashShard(t.v1, window_end) }
             └──StreamHopWindow { time_col: t.v2, slide: Interval { months: 0, days: 0, usecs: 60000000 }, size: Interval { months: 0, days: 0, usecs: 600000000 }, output: [t.v1, t.v3, window_end, t._row_id] }
                └──StreamFilter { predicate: IsNotNull(t.v2) }
                   └──StreamTableScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id], pk: [t._row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t1 (id int, v1 int, created_at date);
    with t2 as (select * from t1 where v1 >= 10)
    select * from tumble(t2, created_at, interval '3' day);
  logical_plan: |-
    LogicalProject { exprs: [t1.id, t1.v1, t1.created_at, $expr1, $expr2] }
    └──LogicalProject { exprs: [t1.id, t1.v1, t1.created_at, TumbleStart(t1.created_at, '3 days':Interval) as $expr1, (TumbleStart(t1.created_at, '3 days':Interval) + '3 days':Interval) as $expr2] }
       └──LogicalShare { id: 3 }
          └──LogicalProject { exprs: [t1.id, t1.v1, t1.created_at] }
             └──LogicalFilter { predicate: (t1.v1 >= 10:Int32) }
                └──LogicalScan { table: t1, columns: [t1.id, t1.v1, t1.created_at, t1._row_id] }
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └──BatchProject { exprs: [t1.id, t1.v1, t1.created_at, TumbleStart(t1.created_at, '3 days':Interval) as $expr1, (TumbleStart(t1.created_at, '3 days':Interval) + '3 days':Interval) as $expr2] }
       └──BatchFilter { predicate: (t1.v1 >= 10:Int32) }
          └──BatchScan { table: t1, columns: [t1.id, t1.v1, t1.created_at], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [id, v1, created_at, window_start, window_end, t1._row_id(hidden)], stream_key: [t1._row_id], pk_columns: [t1._row_id], pk_conflict: NoCheck }
    └──StreamProject { exprs: [t1.id, t1.v1, t1.created_at, TumbleStart(t1.created_at, '3 days':Interval) as $expr1, (TumbleStart(t1.created_at, '3 days':Interval) + '3 days':Interval) as $expr2, t1._row_id] }
       └──StreamFilter { predicate: (t1.v1 >= 10:Int32) }
          └──StreamTableScan { table: t1, columns: [t1.id, t1.v1, t1.created_at, t1._row_id], pk: [t1._row_id], dist: UpstreamHashShard(t1._row_id) }
- sql: |
    create table t1 (id int, v1 int, created_at date);
    with t2 as (select * from t1 where v1 >= 10)
    select * from hop(t2, created_at, interval '1' day, interval '3' day);
  logical_plan: |-
    LogicalProject { exprs: [t1.id, t1.v1, t1.created_at, window_start, window_end] }
    └──LogicalHopWindow { time_col: t1.created_at, slide: Interval { months: 0, days: 1, usecs: 0 }, size: Interval { months: 0, days: 3, usecs: 0 }, output: all }
       └──LogicalFilter { predicate: IsNotNull(t1.created_at) }
          └──LogicalShare { id: 3 }
             └──LogicalProject { exprs: [t1.id, t1.v1, t1.created_at] }
                └──LogicalFilter { predicate: (t1.v1 >= 10:Int32) }
                   └──LogicalScan { table: t1, columns: [t1.id, t1.v1, t1.created_at, t1._row_id] }
  batch_plan: |-
    BatchHopWindow { time_col: t1.created_at, slide: Interval { months: 0, days: 1, usecs: 0 }, size: Interval { months: 0, days: 3, usecs: 0 }, output: all }
    └──BatchExchange { order: [], dist: Single }
       └──BatchFilter { predicate: IsNotNull(t1.created_at) AND (t1.v1 >= 10:Int32) }
          └──BatchScan { table: t1, columns: [t1.id, t1.v1, t1.created_at], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [id, v1, created_at, window_start, window_end, t1._row_id(hidden)], stream_key: [t1._row_id, window_start, window_end], pk_columns: [t1._row_id, window_start, window_end], pk_conflict: NoCheck }
    └──StreamHopWindow { time_col: t1.created_at, slide: Interval { months: 0, days: 1, usecs: 0 }, size: Interval { months: 0, days: 3, usecs: 0 }, output: [t1.id, t1.v1, t1.created_at, window_start, window_end, t1._row_id] }
       └──StreamFilter { predicate: IsNotNull(t1.created_at) AND (t1.v1 >= 10:Int32) }
          └──StreamTableScan { table: t1, columns: [t1.id, t1.v1, t1.created_at, t1._row_id], pk: [t1._row_id], dist: UpstreamHashShard(t1._row_id) }
- sql: |
    with t(ts) as (values ('2020-01-01 12:00:00'::timestamp)) select * from tumble(t, ts, interval '10' second) as z;
  logical_plan: |-
    LogicalProject { exprs: [*VALUES*_0.column_0, $expr1, $expr2] }
    └──LogicalProject { exprs: [*VALUES*_0.column_0, TumbleStart(*VALUES*_0.column_0, '00:00:10':Interval) as $expr1, (TumbleStart(*VALUES*_0.column_0, '00:00:10':Interval) + '00:00:10':Interval) as $expr2] }
       └──LogicalShare { id: 1 }
          └──LogicalValues { rows: [['2020-01-01 12:00:00':Timestamp]], schema: Schema { fields: [*VALUES*_0.column_0:Timestamp] } }
  batch_plan: |-
    BatchProject { exprs: [*VALUES*_0.column_0, TumbleStart(*VALUES*_0.column_0, '00:00:10':Interval) as $expr1, (TumbleStart(*VALUES*_0.column_0, '00:00:10':Interval) + '00:00:10':Interval) as $expr2] }
    └──BatchValues { rows: [['2020-01-01 12:00:00':Timestamp]] }
- sql: |
    create table t1 ( c1 varchar, c2 int, c3 timestamp);
    create table t2 ( d1 int, d2 varchar, d3 timestamp);
    create index idx_t1 on t1(c2) include (c1, c2, c3);
    select * from t2 inner join hop(t1,  c3, interval '10 minute',interval '20 minute')  on t2.d1 = t1.c2;
  logical_plan: |-
    LogicalProject { exprs: [t2.d1, t2.d2, t2.d3, t1.c1, t1.c2, t1.c3, window_start, window_end] }
    └──LogicalJoin { type: Inner, on: (t2.d1 = t1.c2), output: all }
       ├──LogicalScan { table: t2, columns: [t2.d1, t2.d2, t2.d3, t2._row_id] }
       └──LogicalHopWindow { time_col: t1.c3, slide: Interval { months: 0, days: 0, usecs: 600000000 }, size: Interval { months: 0, days: 0, usecs: 1200000000 }, output: all }
          └──LogicalFilter { predicate: IsNotNull(t1.c3) }
             └──LogicalScan { table: t1, columns: [t1.c1, t1.c2, t1.c3, t1._row_id] }
  optimized_logical_plan_for_batch: |-
    LogicalHopWindow { time_col: t1.c3, slide: Interval { months: 0, days: 0, usecs: 600000000 }, size: Interval { months: 0, days: 0, usecs: 1200000000 }, output: all }
    └──LogicalFilter { predicate: IsNotNull(t1.c3) }
       └──LogicalJoin { type: Inner, on: (t2.d1 = t1.c2), output: all }
          ├──LogicalScan { table: t2, columns: [t2.d1, t2.d2, t2.d3] }
          └──LogicalScan { table: t1, columns: [t1.c1, t1.c2, t1.c3], predicate: IsNotNull(t1.c3) }
  batch_plan: |-
    BatchHopWindow { time_col: idx_t1.c3, slide: Interval { months: 0, days: 0, usecs: 600000000 }, size: Interval { months: 0, days: 0, usecs: 1200000000 }, output: all }
    └──BatchExchange { order: [], dist: Single }
       └──BatchFilter { predicate: IsNotNull(idx_t1.c3) }
          └──BatchLookupJoin { type: Inner, predicate: t2.d1 = idx_t1.c2 AND IsNotNull(idx_t1.c3), output: all }
             └──BatchExchange { order: [], dist: UpstreamHashShard(t2.d1) }
                └──BatchScan { table: t2, columns: [t2.d1, t2.d2, t2.d3], distribution: SomeShard }
- sql: |
    create table t1 ( c1 varchar, c2 int, c3 timestamp);
    create table t2 ( d1 int, d2 varchar, d3 timestamp);
    create index idx_t1 on t1(c2) include (c1, c2, c3);
    select * from hop(t2,  d3, interval '10 minute',interval '20 minute') inner join hop(t1,  c3, interval '10 minute',interval '20 minute')  on t2.d1 = t1.c2;
  logical_plan: |-
    LogicalProject { exprs: [t2.d1, t2.d2, t2.d3, window_start, window_end, t1.c1, t1.c2, t1.c3, window_start, window_end] }
    └──LogicalJoin { type: Inner, on: (t2.d1 = t1.c2), output: all }
       ├──LogicalHopWindow { time_col: t2.d3, slide: Interval { months: 0, days: 0, usecs: 600000000 }, size: Interval { months: 0, days: 0, usecs: 1200000000 }, output: all }
       │  └──LogicalFilter { predicate: IsNotNull(t2.d3) }
       │     └──LogicalScan { table: t2, columns: [t2.d1, t2.d2, t2.d3, t2._row_id] }
       └──LogicalHopWindow { time_col: t1.c3, slide: Interval { months: 0, days: 0, usecs: 600000000 }, size: Interval { months: 0, days: 0, usecs: 1200000000 }, output: all }
          └──LogicalFilter { predicate: IsNotNull(t1.c3) }
             └──LogicalScan { table: t1, columns: [t1.c1, t1.c2, t1.c3, t1._row_id] }
  optimized_logical_plan_for_batch: |-
    LogicalHopWindow { time_col: t1.c3, slide: Interval { months: 0, days: 0, usecs: 600000000 }, size: Interval { months: 0, days: 0, usecs: 1200000000 }, output: [t2.d1, t2.d2, t2.d3, window_start, window_end, t1.c1, t1.c2, t1.c3, window_start, window_end] }
    └──LogicalFilter { predicate: IsNotNull(t1.c3) }
       └──LogicalHopWindow { time_col: t2.d3, slide: Interval { months: 0, days: 0, usecs: 600000000 }, size: Interval { months: 0, days: 0, usecs: 1200000000 }, output: all }
          └──LogicalFilter { predicate: IsNotNull(t2.d3) }
             └──LogicalJoin { type: Inner, on: (t2.d1 = t1.c2), output: all }
                ├──LogicalScan { table: t2, columns: [t2.d1, t2.d2, t2.d3], predicate: IsNotNull(t2.d3) }
                └──LogicalScan { table: t1, columns: [t1.c1, t1.c2, t1.c3], predicate: IsNotNull(t1.c3) }
  batch_plan: |-
    BatchHopWindow { time_col: idx_t1.c3, slide: Interval { months: 0, days: 0, usecs: 600000000 }, size: Interval { months: 0, days: 0, usecs: 1200000000 }, output: [t2.d1, t2.d2, t2.d3, window_start, window_end, idx_t1.c1, idx_t1.c2, idx_t1.c3, window_start, window_end] }
    └──BatchExchange { order: [], dist: Single }
       └──BatchFilter { predicate: IsNotNull(idx_t1.c3) }
          └──BatchHopWindow { time_col: t2.d3, slide: Interval { months: 0, days: 0, usecs: 600000000 }, size: Interval { months: 0, days: 0, usecs: 1200000000 }, output: all }
             └──BatchFilter { predicate: IsNotNull(t2.d3) }
                └──BatchLookupJoin { type: Inner, predicate: t2.d1 = idx_t1.c2 AND IsNotNull(idx_t1.c3), output: all }
                   └──BatchExchange { order: [], dist: UpstreamHashShard(t2.d1) }
                      └──BatchFilter { predicate: IsNotNull(t2.d3) }
                         └──BatchScan { table: t2, columns: [t2.d1, t2.d2, t2.d3], distribution: SomeShard }
- sql: |
    CREATE TABLE auction (id BIGINT, item_name CHARACTER VARYING, description CHARACTER VARYING, initial_bid BIGINT, reserve BIGINT, date_time TIMESTAMP, expires TIMESTAMP, seller BIGINT, category BIGINT, extra CHARACTER VARYING, PRIMARY KEY (id));
    CREATE TABLE nation (n_nationkey INT, n_name CHARACTER VARYING, n_regionkey INT, n_comment CHARACTER VARYING, PRIMARY KEY (n_nationkey));
    CREATE TABLE alltypes2 (c1 BOOLEAN, c2 SMALLINT, c3 INT, c4 BIGINT, c5 REAL, c6 DOUBLE, c7 NUMERIC, c8 DATE, c9 CHARACTER VARYING, c10 TIME, c11 TIMESTAMP, c13 INTERVAL, c14 STRUCT<a INT>, c15 INT[], c16 CHARACTER VARYING[]);
    SELECT ((CASE WHEN (hop_0.c2 = (32)) THEN TIMESTAMP '2022-07-29 15:06:36' WHEN hop_0.c1 THEN t_2.expires WHEN false THEN (t_2.expires + (INTERVAL '-18')) ELSE t_2.expires END) + (INTERVAL '87')) AS col_0, ((INT '618') % ((311))) AS col_1, (SMALLINT '35') AS col_2, ((coalesce(NULL, NULL, NULL, NULL, NULL, (INT '0'), NULL, NULL, NULL, NULL)) - hop_0.c7) AS col_3 FROM hop(alltypes2, alltypes2.c11, INTERVAL '3600', INTERVAL '144000') AS hop_0, nation AS t_1 JOIN auction AS t_2 ON t_1.n_name = t_2.item_name AND true WHERE (t_2.seller > ((CAST(((SMALLINT '62')) IN (hop_0.c2, hop_0.c2, hop_0.c2, (- hop_0.c2), hop_0.c2, hop_0.c2, hop_0.c2, (SMALLINT '-32768')) AS INT) | t_1.n_nationkey) + hop_0.c7)) GROUP BY t_2.description, hop_0.c7, t_2.category, t_2.item_name, hop_0.c15, hop_0.c2, hop_0.c1, hop_0.c9, t_2.expires, hop_0.c6;
  logical_plan: |-
    LogicalProject
    ├──exprs:
    │  ┌──(Case((alltypes2.c2 = 32:Int32), '2022-07-29 15:06:36':Timestamp, alltypes2.c1, auction.expires, false:Boolean, (auction.expires + '-00:00:18':Interval), auction.expires) + '00:01:27':Interval) as $expr1
    │  ├──(618:Int32 % 311:Int32) as $expr2
    │  ├──35:Int16
    │  └──(Coalesce(null:Int32, null:Int32, null:Int32, null:Int32, null:Int32, 0:Int32, null:Int32, null:Int32, null:Int32, null:Int32)::Decimal - alltypes2.c7) as $expr3
    └──LogicalAgg { group_key: [auction.description, alltypes2.c7, auction.category, auction.item_name, alltypes2.c15, alltypes2.c2, alltypes2.c1, alltypes2.c9, auction.expires, alltypes2.c6], aggs: [] }
       └──LogicalProject { exprs: [auction.description, alltypes2.c7, auction.category, auction.item_name, alltypes2.c15, alltypes2.c2, alltypes2.c1, alltypes2.c9, auction.expires, alltypes2.c6] }
          └──LogicalFilter
             ├──predicate: (auction.seller::Decimal > (((((((((In(62:Int16, -32768:Int16) OR (62:Int16 = alltypes2.c2)) OR (62:Int16 = alltypes2.c2)) OR (62:Int16 = alltypes2.c2)) OR (62:Int16 = Neg(alltypes2.c2))) OR (62:Int16 = alltypes2.c2)) OR (62:Int16 = alltypes2.c2)) OR (62:Int16 = alltypes2.c2))::Int32 | nation.n_nationkey)::Decimal + alltypes2.c7))
             └──LogicalJoin { type: Inner, on: true, output: all }
                ├──LogicalHopWindow { time_col: alltypes2.c11, slide: Interval { months: 0, days: 0, usecs: 3600000000 }, size: Interval { months: 0, days: 0, usecs: 144000000000 }, output: all }
                │  └──LogicalFilter { predicate: IsNotNull(alltypes2.c11) }
                │     └──LogicalScan { table: alltypes2, columns: [alltypes2.c1, alltypes2.c2, alltypes2.c3, alltypes2.c4, alltypes2.c5, alltypes2.c6, alltypes2.c7, alltypes2.c8, alltypes2.c9, alltypes2.c10, alltypes2.c11, alltypes2.c13, alltypes2.c14, alltypes2.c15, alltypes2.c16, alltypes2._row_id] }
                └──LogicalJoin { type: Inner, on: (nation.n_name = auction.item_name), output: all }
                   ├──LogicalScan { table: nation, columns: [nation.n_nationkey, nation.n_name, nation.n_regionkey, nation.n_comment] }
                   └──LogicalScan { table: auction, columns: [auction.id, auction.item_name, auction.description, auction.initial_bid, auction.reserve, auction.date_time, auction.expires, auction.seller, auction.category, auction.extra] }
  optimized_logical_plan_for_batch: |-
    LogicalProject
    ├──exprs:
    │  ┌──(Case((alltypes2.c2 = 32:Int32), '2022-07-29 15:06:36':Timestamp, alltypes2.c1, auction.expires, false:Boolean, (auction.expires + '-00:00:18':Interval), auction.expires) + '00:01:27':Interval) as $expr1
    │  ├──(618:Int32 % 311:Int32) as $expr2
    │  ├──35:Int16
    │  └──(Coalesce(null:Int32, null:Int32, null:Int32, null:Int32, null:Int32, 0:Int32, null:Int32, null:Int32, null:Int32, null:Int32)::Decimal - alltypes2.c7) as $expr3
    └──LogicalAgg { group_key: [auction.description, alltypes2.c7, auction.category, auction.item_name, alltypes2.c15, alltypes2.c2, alltypes2.c1, alltypes2.c9, auction.expires, alltypes2.c6], aggs: [] }
       └──LogicalHopWindow { time_col: alltypes2.c11, slide: Interval { months: 0, days: 0, usecs: 3600000000 }, size: Interval { months: 0, days: 0, usecs: 144000000000 }, output: [auction.description, alltypes2.c7, auction.category, auction.item_name, alltypes2.c15, alltypes2.c2, alltypes2.c1, alltypes2.c9, auction.expires, alltypes2.c6] }
          └──LogicalFilter { predicate: IsNotNull(alltypes2.c11) }
             └──LogicalJoin
                ├──type: Inner
                ├──on: (auction.seller::Decimal > (((((((((In(62:Int16, -32768:Int16) OR (62:Int16 = alltypes2.c2)) OR (62:Int16 = alltypes2.c2)) OR (62:Int16 = alltypes2.c2)) OR (62:Int16 = Neg(alltypes2.c2))) OR (62:Int16 = alltypes2.c2)) OR (62:Int16 = alltypes2.c2)) OR (62:Int16 = alltypes2.c2))::Int32 | nation.n_nationkey)::Decimal + alltypes2.c7))
                ├──output: all
                ├──LogicalJoin { type: Inner, on: (nation.n_name = auction.item_name), output: [nation.n_nationkey, auction.item_name, auction.description, auction.expires, auction.seller, auction.category] }
                │  ├──LogicalScan { table: nation, columns: [nation.n_nationkey, nation.n_name] }
                │  └──LogicalScan { table: auction, columns: [auction.item_name, auction.description, auction.expires, auction.seller, auction.category] }
                └──LogicalScan { table: alltypes2, columns: [alltypes2.c1, alltypes2.c2, alltypes2.c6, alltypes2.c7, alltypes2.c9, alltypes2.c11, alltypes2.c15], predicate: IsNotNull(alltypes2.c11) }
