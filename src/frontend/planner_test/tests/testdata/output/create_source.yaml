# This file is automatically generated. See `src/frontend/planner_test/README.md` for more information.
- id: create_source_without_with_clause
  sql: |
    create source s() FORMAT PLAIN ENCODE JSON;
  planner_error: 'Invalid input syntax: missing WITH clause'
- id: create_source_without_connector
  sql: |
    create source s() with(a=1) FORMAT PLAIN ENCODE JSON;
  planner_error: 'Protocol error: missing field ''connector'''
- id: create_source_without_schema_in_json
  sql: |
    create source s with(connector='kafka') FORMAT PLAIN ENCODE JSON;
  planner_error: 'Protocol error: Schema definition is required, either from SQL or schema registry.'
- id: csv_delimiter_comma
  sql: |
    explain create table s0 (v1 int, v2 varchar) with (
      connector = 'kafka',
      topic = 'kafka_1_csv_topic',
      properties.bootstrap.server = 'message_queue:29092',
      scan.startup.mode = 'earliest'
    ) FORMAT PLAIN ENCODE CSV (delimiter = ',', without_header = true);
  explain_output: |
    StreamMaterialize { columns: [v1, v2, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: Overwrite }
    └─StreamRowIdGen { row_id_index: 2 }
      └─StreamUnion { all: true }
        ├─StreamExchange [no_shuffle] { dist: SomeShard }
        │ └─StreamSource { source: s0, columns: [v1, v2, _row_id] }
        ├─StreamExchange { dist: HashShard(_row_id) }
        │ └─StreamDml { columns: [v1, v2, _row_id] }
        │   └─StreamSource
        └─StreamUpstreamSinkUnion
- id: csv_delimiter_semicolon
  sql: |
    explain create table s0 (v1 int, v2 varchar) with (
      connector = 'kafka',
      topic = 'kafka_1_csv_topic',
      properties.bootstrap.server = 'message_queue:29092',
      scan.startup.mode = 'earliest'
    ) FORMAT PLAIN ENCODE CSV (delimiter = ';', without_header = true);
  explain_output: |
    StreamMaterialize { columns: [v1, v2, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: Overwrite }
    └─StreamRowIdGen { row_id_index: 2 }
      └─StreamUnion { all: true }
        ├─StreamExchange [no_shuffle] { dist: SomeShard }
        │ └─StreamSource { source: s0, columns: [v1, v2, _row_id] }
        ├─StreamExchange { dist: HashShard(_row_id) }
        │ └─StreamDml { columns: [v1, v2, _row_id] }
        │   └─StreamSource
        └─StreamUpstreamSinkUnion
- id: csv_delimiter_tab
  sql: |
    explain create table s0 (v1 int, v2 varchar) with (
      connector = 'kafka',
      topic = 'kafka_1_csv_topic',
      properties.bootstrap.server = 'message_queue:29092',
      scan.startup.mode = 'earliest'
    ) FORMAT PLAIN ENCODE CSV (delimiter = E'\t', without_header = true);
  explain_output: |
    StreamMaterialize { columns: [v1, v2, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: Overwrite }
    └─StreamRowIdGen { row_id_index: 2 }
      └─StreamUnion { all: true }
        ├─StreamExchange [no_shuffle] { dist: SomeShard }
        │ └─StreamSource { source: s0, columns: [v1, v2, _row_id] }
        ├─StreamExchange { dist: HashShard(_row_id) }
        │ └─StreamDml { columns: [v1, v2, _row_id] }
        │   └─StreamSource
        └─StreamUpstreamSinkUnion
