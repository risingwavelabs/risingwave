# This file is automatically generated. See `src/frontend/planner_test/README.md` for more information.
- sql: |
    select * from unnest(array[1,2,3]) WITH ORDINALITY;
  batch_plan: |-
    BatchProject { exprs: [Unnest(ARRAY[1, 2, 3]:List(Int32)), (_rw_projected_row_id + 1:Int64) as $expr1] }
    └─BatchProjectSet { select_list: [Unnest(ARRAY[1, 2, 3]:List(Int32))] }
      └─BatchValues { rows: [[]] }
  stream_plan: |-
    StreamMaterialize { columns: [unnest, ordinality, _row_id(hidden), _rw_projected_row_id(hidden)], stream_key: [_row_id, _rw_projected_row_id], pk_columns: [_row_id, _rw_projected_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [Unnest(ARRAY[1, 2, 3]:List(Int32)), (_rw_projected_row_id + 1:Int64) as $expr1, _row_id, _rw_projected_row_id] }
      └─StreamProjectSet { select_list: [Unnest(ARRAY[1, 2, 3]:List(Int32)), $0] }
        └─StreamValues { rows: [[0:Int64]] }
- sql: |
    create table t(x int , arr int[]);
    select * from t cross join unnest(arr) WITH ORDINALITY;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1] }
      └─BatchProjectSet { select_list: [$0, $1, Unnest($1)] }
        └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, unnest, ordinality, t._row_id(hidden), _rw_projected_row_id(hidden)], stream_key: [t._row_id, _rw_projected_row_id], pk_columns: [t._row_id, _rw_projected_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1, t._row_id, _rw_projected_row_id] }
      └─StreamProjectSet { select_list: [$0, $1, Unnest($1), $2] }
        └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: SnapshotBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t(x int , arr int[]);
    select * from t cross join unnest(arr) WITH ORDINALITY as foo;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1] }
      └─BatchProjectSet { select_list: [$0, $1, Unnest($1)] }
        └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, foo, ordinality, t._row_id(hidden), _rw_projected_row_id(hidden)], stream_key: [t._row_id, _rw_projected_row_id], pk_columns: [t._row_id, _rw_projected_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1, t._row_id, _rw_projected_row_id] }
      └─StreamProjectSet { select_list: [$0, $1, Unnest($1), $2] }
        └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: SnapshotBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t(x int , arr int[]);
    select * from t cross join unnest(arr) WITH ORDINALITY as foo(a);
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1] }
      └─BatchProjectSet { select_list: [$0, $1, Unnest($1)] }
        └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, a, ordinality, t._row_id(hidden), _rw_projected_row_id(hidden)], stream_key: [t._row_id, _rw_projected_row_id], pk_columns: [t._row_id, _rw_projected_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1, t._row_id, _rw_projected_row_id] }
      └─StreamProjectSet { select_list: [$0, $1, Unnest($1), $2] }
        └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: SnapshotBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t(x int , arr int[]);
    select * from t cross join unnest(arr) WITH ORDINALITY as foo(a,ord);
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1] }
      └─BatchProjectSet { select_list: [$0, $1, Unnest($1)] }
        └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, a, ord, t._row_id(hidden), _rw_projected_row_id(hidden)], stream_key: [t._row_id, _rw_projected_row_id], pk_columns: [t._row_id, _rw_projected_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1, t._row_id, _rw_projected_row_id] }
      └─StreamProjectSet { select_list: [$0, $1, Unnest($1), $2] }
        └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: SnapshotBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: use alias columns explicitlity
  sql: |
    create table t(x int , arr int[]);
    select x, arr, a, ord from t cross join unnest(arr) WITH ORDINALITY as foo(a,ord);
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1] }
      └─BatchProjectSet { select_list: [$0, $1, Unnest($1)] }
        └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, a, ord, t._row_id(hidden), _rw_projected_row_id(hidden)], stream_key: [t._row_id, _rw_projected_row_id], pk_columns: [t._row_id, _rw_projected_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1, t._row_id, _rw_projected_row_id] }
      └─StreamProjectSet { select_list: [$0, $1, Unnest($1), $2] }
        └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: SnapshotBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t(x int , arr int[]);
    select * from t cross join unnest(arr) WITH ORDINALITY as foo(a,ord,bar);
  binder_error: 'Bind error: table "foo" has 2 columns available but 3 column aliases specified'
- name: filter on ordinality column
  sql: |
    create table t(x int , arr int[]);
    select * from t
    cross join unnest(arr) WITH ORDINALITY as u(val, ord)
    where ord > 1;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1] }
      └─BatchFilter { predicate: ((_rw_projected_row_id + 1:Int64) > 1:Int32) }
        └─BatchProjectSet { select_list: [$0, $1, Unnest($1)] }
          └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, val, ord, t._row_id(hidden), _rw_projected_row_id(hidden)], stream_key: [t._row_id, _rw_projected_row_id], pk_columns: [t._row_id, _rw_projected_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1, t._row_id, _rw_projected_row_id] }
      └─StreamFilter { predicate: ((_rw_projected_row_id + 1:Int64) > 1:Int32) }
        └─StreamProjectSet { select_list: [$0, $1, Unnest($1), $2] }
          └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: SnapshotBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: correlated arg uses array concat (int[] || int)
  sql: |
    create table t(x int , arr int[]);
    select * from t
    cross join unnest(arr || x) WITH ORDINALITY as u(val, ord);
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Unnest(ArrayAppend($1, $0)), (_rw_projected_row_id + 1:Int64) as $expr1] }
      └─BatchProjectSet { select_list: [$0, $1, Unnest(ArrayAppend($1, $0))] }
        └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, val, ord, t._row_id(hidden), _rw_projected_row_id(hidden)], stream_key: [t._row_id, _rw_projected_row_id], pk_columns: [t._row_id, _rw_projected_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.x, t.arr, Unnest(ArrayAppend($1, $0)), (_rw_projected_row_id + 1:Int64) as $expr1, t._row_id, _rw_projected_row_id] }
      └─StreamProjectSet { select_list: [$0, $1, Unnest(ArrayAppend($1, $0)), $2] }
        └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: SnapshotBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: multiple correlated table functions with ordinality
  sql: |
    create table t(x int , arr int[]);
    select *
    from t
    cross join unnest(arr) WITH ORDINALITY as u(a, ord1)
    cross join unnest(arr || x) WITH ORDINALITY as v(b, ord2);
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Unnest($1), $expr1, Unnest(ArrayAppend($1, $0)), (_rw_projected_row_id + 1:Int64) as $expr2] }
      └─BatchProjectSet { select_list: [$0, $1, $2, $3, Unnest(ArrayAppend($1, $0))] }
        └─BatchProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1] }
          └─BatchProjectSet { select_list: [$0, $1, Unnest($1)] }
            └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, a, ord1, b, ord2, t._row_id(hidden), _rw_projected_row_id(hidden), _rw_projected_row_id#1(hidden)], stream_key: [t._row_id, _rw_projected_row_id, _rw_projected_row_id#1], pk_columns: [t._row_id, _rw_projected_row_id, _rw_projected_row_id#1], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.x, t.arr, Unnest($1), $expr1, Unnest(ArrayAppend($1, $0)), (_rw_projected_row_id + 1:Int64) as $expr2, t._row_id, _rw_projected_row_id, _rw_projected_row_id] }
      └─StreamProjectSet { select_list: [$0, $1, $2, $3, Unnest(ArrayAppend($1, $0)), $4, $5] }
        └─StreamProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1, t._row_id, _rw_projected_row_id] }
          └─StreamProjectSet { select_list: [$0, $1, Unnest($1), $2] }
            └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: SnapshotBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t(x int , arr int[]);
    select * from t cross join unnest(arr) WITH ORDINALITY, unnest(arr) WITH ORDINALITY AS unnest_2(arr_2,ordinality_2);
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Unnest($1), $expr1, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr2] }
      └─BatchProjectSet { select_list: [$0, $1, $2, $3, Unnest($1)] }
        └─BatchProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1] }
          └─BatchProjectSet { select_list: [$0, $1, Unnest($1)] }
            └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, unnest, ordinality, arr_2, ordinality_2, t._row_id(hidden), _rw_projected_row_id(hidden), _rw_projected_row_id#1(hidden)], stream_key: [t._row_id, _rw_projected_row_id, _rw_projected_row_id#1], pk_columns: [t._row_id, _rw_projected_row_id, _rw_projected_row_id#1], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.x, t.arr, Unnest($1), $expr1, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr2, t._row_id, _rw_projected_row_id, _rw_projected_row_id] }
      └─StreamProjectSet { select_list: [$0, $1, $2, $3, Unnest($1), $4, $5] }
        └─StreamProject { exprs: [t.x, t.arr, Unnest($1), (_rw_projected_row_id + 1:Int64) as $expr1, t._row_id, _rw_projected_row_id] }
          └─StreamProjectSet { select_list: [$0, $1, Unnest($1), $2] }
            └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: SnapshotBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    select * from abs(1) WITH ORDINALITY;
  batch_plan: |-
    BatchProject { exprs: [, 1:Int64] }
    └─BatchValues { rows: [[1:Int32]] }
  stream_plan: |-
    StreamMaterialize { columns: [abs, ordinality, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [, 1:Int64, _row_id] }
      └─StreamValues { rows: [[1:Int32, 0:Int64]] }
- sql: |
    create table t(x int , arr int[]);
    select * from t, abs(x) WITH ORDINALITY;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Abs(t.x) as $expr1, 1:Int64] }
      └─BatchHashJoin { type: Inner, predicate: t.x IS NOT DISTINCT FROM t.x, output: all }
        ├─BatchExchange { order: [], dist: HashShard(t.x) }
        │ └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
        └─BatchExchange { order: [], dist: HashShard(t.x) }
          └─BatchNestedLoopJoin { type: Inner, predicate: true, output: all }
            ├─BatchExchange { order: [], dist: Single }
            │ └─BatchHashAgg { group_key: [t.x], aggs: [] }
            │   └─BatchExchange { order: [], dist: HashShard(t.x) }
            │     └─BatchScan { table: t, columns: [t.x], distribution: SomeShard }
            └─BatchValues { rows: [[]] }
  stream_error: |-
    Not supported: streaming nested-loop join
    HINT: The non-equal join in the query requires a nested-loop join executor, which could be very expensive to run. Consider rewriting the query to use dynamic filter as a substitute if possible.
    See also: https://docs.risingwave.com/docs/current/sql-pattern-dynamic-filters/
