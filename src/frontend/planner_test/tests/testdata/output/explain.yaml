# This file is automatically generated. See `src/frontend/planner_test/README.md` for more information.
- sql: explain (distsql, trace, verbose) select 1;
  explain_output: |
    Begin:

    LogicalProject { exprs: [1:Int32] }
    └─LogicalValues { rows: [[]], schema: Schema { fields: [] } }

    Predicate Push Down:

    LogicalProject { exprs: [1:Int32] }
    └─LogicalValues { rows: [[]], schema: Schema { fields: [] } }

    Convert Over Window:

    apply TrivialProjectToValuesRule 1 time(s)

    LogicalValues { rows: [[1:Int32]], schema: Schema { fields: [1:Int32:Int32] } }

    Prune Columns:

    LogicalValues { rows: [[1:Int32]], schema: Schema { fields: [1:Int32:Int32] } }

    Predicate Push Down:

    LogicalValues { rows: [[1:Int32]], schema: Schema { fields: [1:Int32:Int32] } }

    Const eval exprs:

    LogicalValues { rows: [[1:Int32]], schema: Schema { fields: [1:Int32:Int32] } }

    To Batch Plan:

    BatchValues { rows: [[1:Int32]] }

    Inline Session Timezone:

    BatchValues { rows: [[1:Int32]] }

    To Batch Physical Plan:

    BatchValues { rows: [[1:Int32]] }

    To Batch Local Plan:

    BatchValues { rows: [[1:Int32]] }

    {
      "root_stage_id": 0,
      "stages": {
        "0": {
          "root": {
            "plan_node_id": 10009,
            "plan_node_type": "BatchValues",
            "schema": [
              {
                "dataType": {
                  "typeName": "INT32"
                },
                "name": "1:Int32"
              }
            ],
            "children": [],
            "source_stage_id": null
          },
          "parallelism": 1,
          "exchange_info": {
            "mode": "SINGLE"
          }
        }
      },
      "child_edges": {
        "0": []
      },
      "parent_edges": {
        "0": []
      },
      "batch_parallelism": 0
    }
- sql: |
    create table t1(v1 int);
    create table t2(v2 int);
    explain (logical) select * from t1 join t2 on v1=v2;
  explain_output: |
    LogicalJoin { type: Inner, on: (t1.v1 = t2.v2) }
    ├─LogicalScan { table: t1, columns: [v1] }
    └─LogicalScan { table: t2, columns: [v2] }
- sql: |
    create table t1(v1 int);
    create table t2(v2 int);
    explain (logical, trace) select * from t1 join t2 on v1=v2;
  explain_output: |+
    Begin:

    LogicalProject { exprs: [t1.v1, t2.v2] }
    └─LogicalJoin { type: Inner, on: (t1.v1 = t2.v2) }
      ├─LogicalScan { table: t1, columns: [v1, _row_id, _rw_timestamp] }
      └─LogicalScan { table: t2, columns: [v2, _row_id, _rw_timestamp] }

    Predicate Push Down:

    LogicalProject { exprs: [t1.v1, t2.v2] }
    └─LogicalJoin { type: Inner, on: (t1.v1 = t2.v2) }
      ├─LogicalScan { table: t1, columns: [v1, _row_id, _rw_timestamp] }
      └─LogicalScan { table: t2, columns: [v2, _row_id, _rw_timestamp] }

    Push down the calculation of inputs of join's condition:

    apply PushCalculationOfJoinRule 1 time(s)

    LogicalProject { exprs: [t1.v1, t2.v2] }
    └─LogicalJoin { type: Inner, on: (t1.v1 = t2.v2) }
      ├─LogicalScan { table: t1, columns: [v1, _row_id, _rw_timestamp] }
      └─LogicalScan { table: t2, columns: [v2, _row_id, _rw_timestamp] }

    Predicate Push Down:

    LogicalProject { exprs: [t1.v1, t2.v2] }
    └─LogicalJoin { type: Inner, on: (t1.v1 = t2.v2) }
      ├─LogicalScan { table: t1, columns: [v1, _row_id, _rw_timestamp] }
      └─LogicalScan { table: t2, columns: [v2, _row_id, _rw_timestamp] }

    Prune Columns:

    LogicalProject { exprs: [t1.v1, t2.v2] }
    └─LogicalJoin { type: Inner, on: (t1.v1 = t2.v2) }
      ├─LogicalScan { table: t1, columns: [v1] }
      └─LogicalScan { table: t2, columns: [v2] }

    Project Remove:

    apply ProjectEliminateRule 1 time(s)

    LogicalJoin { type: Inner, on: (t1.v1 = t2.v2) }
    ├─LogicalScan { table: t1, columns: [v1] }
    └─LogicalScan { table: t2, columns: [v2] }

- sql: |
    explain (logical) create table t1(v1 int);
  explain_output: |
    LogicalSource
- sql: |
    explain create table t (v1 int, v2 varchar);
  explain_output: |
    StreamMaterialize { columns: [v1, v2, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: Overwrite }
    └─StreamRowIdGen { row_id_index: 2 }
      └─StreamUnion { all: true }
        └─StreamExchange { dist: HashShard(_row_id) }
          └─StreamDml { columns: [v1, v2, _row_id] }
            └─StreamSource
- sql: |
    explain create table t (v1 int, v2 varchar) with ( connector = 'kafka', kafka.topic = 'kafka_3_partition_topic', kafka.brokers = '127.0.0.1:1234', kafka.scan.startup.mode='earliest'  ) FORMAT PLAIN ENCODE JSON;
  explain_output: |
    StreamMaterialize { columns: [v1, v2, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: Overwrite }
    └─StreamRowIdGen { row_id_index: 2 }
      └─StreamUnion { all: true }
        ├─StreamExchange [no_shuffle] { dist: SomeShard }
        │ └─StreamSource { source: t, columns: [v1, v2, _row_id] }
        └─StreamExchange { dist: HashShard(_row_id) }
          └─StreamDml { columns: [v1, v2, _row_id] }
            └─StreamSource
- sql: |
    explain create table t (v1 int, v2 varchar) append only with ( connector = 'kafka', kafka.topic = 'kafka_3_partition_topic', kafka.brokers = '127.0.0.1:1234', kafka.scan.startup.mode='earliest'  ) FORMAT PLAIN ENCODE JSON;
  explain_output: |
    StreamMaterialize { columns: [v1, v2, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: NoCheck }
    └─StreamRowIdGen { row_id_index: 2 }
      └─StreamUnion { all: true }
        ├─StreamExchange [no_shuffle] { dist: SomeShard }
        │ └─StreamSource { source: t, columns: [v1, v2, _row_id] }
        └─StreamExchange [no_shuffle] { dist: SomeShard }
          └─StreamDml { columns: [v1, v2, _row_id] }
            └─StreamSource
- sql: |
    explain create table t (v1 int, v2 varchar primary key) with ( connector = 'kafka', kafka.topic = 'kafka_3_partition_topic', kafka.brokers = '127.0.0.1:1234', kafka.scan.startup.mode='earliest'  ) FORMAT PLAIN ENCODE JSON;
  explain_output: |
    StreamMaterialize { columns: [v1, v2], stream_key: [v2], pk_columns: [v2], pk_conflict: Overwrite }
    └─StreamUnion { all: true }
      ├─StreamExchange { dist: HashShard(v2) }
      │ └─StreamSource { source: t, columns: [v1, v2] }
      └─StreamExchange { dist: HashShard(v2) }
        └─StreamDml { columns: [v1, v2] }
          └─StreamSource
- sql: |
    explain create table t (v1 int, v2 varchar primary key) append only with ( connector = 'kafka', kafka.topic = 'kafka_3_partition_topic', kafka.brokers = '127.0.0.1:1234', kafka.scan.startup.mode='earliest'  ) FORMAT PLAIN ENCODE JSON;
  explain_output: |
    StreamMaterialize { columns: [v1, v2], stream_key: [v2], pk_columns: [v2], pk_conflict: IgnoreConflict }
    └─StreamUnion { all: true }
      ├─StreamExchange { dist: HashShard(v2) }
      │ └─StreamSource { source: t, columns: [v1, v2] }
      └─StreamExchange { dist: HashShard(v2) }
        └─StreamDml { columns: [v1, v2] }
          └─StreamSource
