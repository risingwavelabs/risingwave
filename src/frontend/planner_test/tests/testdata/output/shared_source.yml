# This file is automatically generated. See `src/frontend/planner_test/README.md` for more information.
- id: create_source
  sql: |
    CREATE SOURCE s(x int,y int)
    WITH(
        connector='kafka',
        topic = 'mytopic',
        properties.bootstrap.server = '6',
        scan.startup.mode = 'earliest',
    ) FORMAT PLAIN ENCODE JSON;
- sql: |
    /* The shared source config doesn't affect table with connector. */
    EXPLAIN CREATE TABLE s(x int,y int)
    WITH(
        connector='kafka',
        topic = 'mytopic',
        properties.bootstrap.server = '6',
        scan.startup.mode = 'earliest',
    ) FORMAT PLAIN ENCODE JSON;
  explain_output: |
    StreamMaterialize { columns: [x, y, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: Overwrite }
    └─StreamRowIdGen { row_id_index: 2 }
      └─StreamUnion { all: true }
        ├─StreamExchange [no_shuffle] { dist: SomeShard }
        │ └─StreamSource { source: s, columns: [x, y, _row_id] }
        ├─StreamExchange { dist: HashShard(_row_id) }
        │ └─StreamDml { columns: [x, y, _row_id] }
        │   └─StreamSource
        └─StreamUpstreamSinkUnion
  with_config_map:
    streaming_use_shared_source: 'true'
- before:
  - create_source
  sql: |
    SET streaming_use_shared_source = false;
    select * from s;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [x, y] }
      └─BatchKafkaScan { source: s, columns: [x, y, _rw_kafka_timestamp, _row_id], filter: (None, None) }
  stream_plan: |-
    StreamMaterialize { columns: [x, y, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [x, y, _row_id] }
      └─StreamRowIdGen { row_id_index: 3 }
        └─StreamSource { source: s, columns: [x, y, _rw_kafka_timestamp, _row_id] }
  with_config_map:
    streaming_use_shared_source: 'false'
- before:
  - create_source
  sql: |
    SET streaming_use_shared_source = true;
    select * from s;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [x, y] }
      └─BatchKafkaScan { source: s, columns: [x, y, _rw_kafka_timestamp, _row_id], filter: (None, None) }
  stream_plan: |-
    StreamMaterialize { columns: [x, y, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [x, y, _row_id] }
      └─StreamRowIdGen { row_id_index: 3 }
        └─StreamSource { source: s, columns: [x, y, _rw_kafka_timestamp, _row_id] }
  with_config_map:
    streaming_use_shared_source: 'false'
- before:
  - create_source
  sql: |
    SET streaming_use_shared_source = false;
    select * from s;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [x, y] }
      └─BatchKafkaScan { source: s, columns: [x, y, _rw_kafka_timestamp, _rw_kafka_partition, _rw_kafka_offset, _row_id], filter: (None, None) }
  stream_plan: |-
    StreamMaterialize { columns: [x, y, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [x, y, _row_id] }
      └─StreamRowIdGen { row_id_index: 5 }
        └─StreamSourceScan { columns: [x, y, _rw_kafka_timestamp, _rw_kafka_partition, _rw_kafka_offset, _row_id] }
  with_config_map:
    streaming_use_shared_source: 'true'
- before:
  - create_source
  sql: |
    SET streaming_use_shared_source = true;
    select * from s;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [x, y] }
      └─BatchKafkaScan { source: s, columns: [x, y, _rw_kafka_timestamp, _rw_kafka_partition, _rw_kafka_offset, _row_id], filter: (None, None) }
  stream_plan: |-
    StreamMaterialize { columns: [x, y, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [x, y, _row_id] }
      └─StreamRowIdGen { row_id_index: 5 }
        └─StreamSourceScan { columns: [x, y, _rw_kafka_timestamp, _rw_kafka_partition, _rw_kafka_offset, _row_id] }
  with_config_map:
    streaming_use_shared_source: 'true'
- before:
  - create_source
  sql: |
    CREATE TABLE t(x int,y int);
    set streaming_use_snapshot_backfill = true;
    select * from s union select * from t;
  logical_plan: |-
    LogicalUnion { all: false }
    ├─LogicalProject { exprs: [x, y] }
    │ └─LogicalSource { source: s, is_shared: true, columns: [x, y, _rw_kafka_timestamp, _rw_kafka_partition, _rw_kafka_offset, _row_id] }
    └─LogicalProject { exprs: [t.x, t.y] }
      └─LogicalScan { table: t, columns: [t.x, t.y, t._row_id, t._rw_timestamp] }
  stream_plan: |-
    StreamMaterialize { columns: [x, y], stream_key: [x, y], pk_columns: [x, y], pk_conflict: NoCheck }
    └─StreamProject { exprs: [x, y], noop_update_hint: true }
      └─StreamHashAgg { group_key: [x, y], aggs: [count] }
        └─StreamExchange { dist: HashShard(x, y) }
          └─StreamUnion { all: true }
            ├─StreamExchange { dist: HashShard(_row_id, 0:Int32) }
            │ └─StreamProject { exprs: [x, y, _row_id, 0:Int32] }
            │   └─StreamRowIdGen { row_id_index: 5 }
            │     └─StreamSourceScan { columns: [x, y, _rw_kafka_timestamp, _rw_kafka_partition, _rw_kafka_offset, _row_id] }
            └─StreamExchange { dist: HashShard(t._row_id, 1:Int32) }
              └─StreamProject { exprs: [t.x, t.y, t._row_id, 1:Int32] }
                └─StreamTableScan { table: t, columns: [t.x, t.y, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
  with_config_map:
    streaming_use_shared_source: 'true'
