# This file is automatically generated. See `src/frontend/planner_test/README.md` for more information.
- id: create_source
  sql: |
    CREATE SOURCE s(x int,y int)
    WITH(
        connector='kafka',
        topic = 'mytopic',
        properties.bootstrap.server = '6',
        scan.startup.mode = 'earliest',
    ) FORMAT PLAIN ENCODE JSON;
- sql: |
    /* The shared source config doesn't affect table with connector. */
    EXPLAIN CREATE TABLE s(x int,y int)
    WITH(
        connector='kafka',
        topic = 'mytopic',
        properties.bootstrap.server = '6',
        scan.startup.mode = 'earliest',
    ) FORMAT PLAIN ENCODE JSON;
  explain_output: |
    StreamMaterialize { columns: [x, y, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: Overwrite }
    └─StreamRowIdGen { row_id_index: 2 }
      └─StreamUnion { all: true }
        ├─StreamExchange [no_shuffle] { dist: SomeShard }
        │ └─StreamSource { source: s, columns: [x, y, _row_id] }
        └─StreamExchange { dist: HashShard(_row_id) }
          └─StreamDml { columns: [x, y, _row_id] }
            └─StreamSource
  with_config_map:
    streaming_use_shared_source: 'true'
- before:
  - create_source
  sql: |
    SET streaming_use_shared_source = false;
    select * from s;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [x, y] }
      └─BatchKafkaScan { source: s, columns: [x, y, _rw_kafka_timestamp, _row_id], filter: (None, None) }
  stream_plan: |-
    StreamMaterialize { columns: [x, y, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [x, y, _row_id] }
      └─StreamRowIdGen { row_id_index: 3 }
        └─StreamSource { source: s, columns: [x, y, _rw_kafka_timestamp, _row_id] }
  with_config_map:
    streaming_use_shared_source: 'false'
- before:
  - create_source
  sql: |
    SET streaming_use_shared_source = true;
    select * from s;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [x, y] }
      └─BatchKafkaScan { source: s, columns: [x, y, _rw_kafka_timestamp, _row_id], filter: (None, None) }
  stream_plan: |-
    StreamMaterialize { columns: [x, y, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [x, y, _row_id] }
      └─StreamRowIdGen { row_id_index: 3 }
        └─StreamSource { source: s, columns: [x, y, _rw_kafka_timestamp, _row_id] }
  with_config_map:
    streaming_use_shared_source: 'false'
- before:
  - create_source
  sql: |
    SET streaming_use_shared_source = false;
    select * from s;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [x, y] }
      └─BatchKafkaScan { source: s, columns: [x, y, _rw_kafka_timestamp, _rw_kafka_partition, _rw_kafka_offset, _row_id], filter: (None, None) }
  stream_plan: |-
    StreamMaterialize { columns: [x, y, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [x, y, _row_id] }
      └─StreamRowIdGen { row_id_index: 5 }
        └─StreamSourceScan { columns: [x, y, _rw_kafka_timestamp, _rw_kafka_partition, _rw_kafka_offset, _row_id] }
  with_config_map:
    streaming_use_shared_source: 'true'
- before:
  - create_source
  sql: |
    SET streaming_use_shared_source = true;
    select * from s;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [x, y] }
      └─BatchKafkaScan { source: s, columns: [x, y, _rw_kafka_timestamp, _rw_kafka_partition, _rw_kafka_offset, _row_id], filter: (None, None) }
  stream_plan: |-
    StreamMaterialize { columns: [x, y, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [x, y, _row_id] }
      └─StreamRowIdGen { row_id_index: 5 }
        └─StreamSourceScan { columns: [x, y, _rw_kafka_timestamp, _rw_kafka_partition, _rw_kafka_offset, _row_id] }
  with_config_map:
    streaming_use_shared_source: 'true'
