# This file is automatically generated. See `src/frontend/planner_test/README.md` for more information.
- id: create_tables
  sql: |
    set rw_streaming_enable_delta_join = true;
    create table A      (k1 int, k2 int, k3 int, v int);
    create index Ak1   on A(k1) include(k1,k2,k3,v);
    create index Ak1k2 on A(k1,k2) include(k1,k2,k3,v);
    create table B      (k1 int, k2 int, k3 int, v int);
    create index Bk1   on B(k1) include(k1,k2,k3,v);
- id: A_join_B_onk1
  before:
  - create_tables
  sql: select A.v, B.v as Bv from A join B using(k1);
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchLookupJoin { type: Inner, predicate: a.k1 = bk1.k1, output: [a.v, bk1.v], lookup table: bk1 }
      └─BatchExchange { order: [], dist: UpstreamHashShard(a.k1) }
        └─BatchScan { table: a, columns: [a.k1, a.v], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [v, bv, ak1.a._row_id(hidden), ak1.k1(hidden), bk1.b._row_id(hidden)], stream_key: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_columns: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_conflict: NoCheck }
    └─StreamExchange { dist: HashShard(ak1.a._row_id, ak1.k1, bk1.b._row_id) }
      └─StreamDeltaJoin { type: Inner, predicate: ak1.k1 = bk1.k1, output: [ak1.v, bk1.v, ak1.a._row_id, ak1.k1, bk1.b._row_id] }
        ├─StreamTableScan { table: ak1, columns: [ak1.k1, ak1.v, ak1.a._row_id], stream_scan_type: Backfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) }
        └─StreamTableScan { table: bk1, columns: [bk1.k1, bk1.v, bk1.b._row_id], stream_scan_type: UpstreamOnly, stream_key: [bk1.b._row_id], pk: [k1, b._row_id], dist: UpstreamHashShard(bk1.k1) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [v, bv, ak1.a._row_id(hidden), ak1.k1(hidden), bk1.b._row_id(hidden)], stream_key: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_columns: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_conflict: NoCheck }
    └── StreamExchange Hash([2, 3, 4]) from 1

    Fragment 1
    Union
    ├── StreamExchange Hash([2, 4, 3]) from 4
    └── StreamExchange Hash([2, 4, 3]) from 5

    Fragment 2
    StreamTableScan { table: ak1, columns: [ak1.k1, ak1.v, ak1.a._row_id], stream_scan_type: Backfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) } { tables: [ StreamScan: 0 ] }
    ├── Upstream
    └── BatchPlanNode

    Fragment 3
    StreamTableScan { table: bk1, columns: [bk1.k1, bk1.v, bk1.b._row_id], stream_scan_type: UpstreamOnly, stream_key: [bk1.b._row_id], pk: [k1, b._row_id], dist: UpstreamHashShard(bk1.k1) } { tables: [ StreamScan: 1 ] }
    ├── Upstream
    └── BatchPlanNode

    Fragment 4
    Lookup
    ├── StreamExchange Hash([0]) from 3
    └── StreamExchange NoShuffle from 2

    Fragment 5
    Lookup
    ├── StreamExchange Hash([0]) from 2
    └── StreamExchange NoShuffle from 3

    Table 0 { columns: [ vnode, k1, a__row_id, backfill_finished, row_count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2, 3, 4 ], distribution key: [ 0 ], read pk prefix len hint: 1, vnode column idx: 0 }

    Table 1 { columns: [ vnode, k1, b__row_id, backfill_finished, row_count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2, 3, 4 ], distribution key: [ 0 ], read pk prefix len hint: 1, vnode column idx: 0 }

    Table 4294967294 { columns: [ v, bv, ak1.a._row_id, ak1.k1, bk1.b._row_id, _rw_timestamp ], primary key: [ $2 ASC, $4 ASC, $3 ASC ], value indices: [ 0, 1, 2, 3, 4 ], distribution key: [ 2, 3, 4 ], read pk prefix len hint: 3 }

- id: Ak1_join_B_onk1
  before:
  - create_tables
  sql: select A.v, B.v as Bv from Ak1 as A join B using(k1)
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchLookupJoin { type: Inner, predicate: ak1.k1 = bk1.k1, output: [ak1.v, bk1.v], lookup table: bk1 }
      └─BatchExchange { order: [], dist: UpstreamHashShard(ak1.k1) }
        └─BatchScan { table: ak1, columns: [ak1.k1, ak1.v], distribution: UpstreamHashShard(ak1.k1) }
  stream_plan: |-
    StreamMaterialize { columns: [v, bv, ak1.a._row_id(hidden), ak1.k1(hidden), bk1.b._row_id(hidden)], stream_key: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_columns: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_conflict: NoCheck }
    └─StreamExchange { dist: HashShard(ak1.a._row_id, ak1.k1, bk1.b._row_id) }
      └─StreamDeltaJoin { type: Inner, predicate: ak1.k1 = bk1.k1, output: [ak1.v, bk1.v, ak1.a._row_id, ak1.k1, bk1.b._row_id] }
        ├─StreamTableScan { table: ak1, columns: [ak1.k1, ak1.v, ak1.a._row_id], stream_scan_type: Backfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) }
        └─StreamTableScan { table: bk1, columns: [bk1.k1, bk1.v, bk1.b._row_id], stream_scan_type: UpstreamOnly, stream_key: [bk1.b._row_id], pk: [k1, b._row_id], dist: UpstreamHashShard(bk1.k1) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [v, bv, ak1.a._row_id(hidden), ak1.k1(hidden), bk1.b._row_id(hidden)], stream_key: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_columns: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_conflict: NoCheck }
    └── StreamExchange Hash([2, 3, 4]) from 1

    Fragment 1
    Union
    ├── StreamExchange Hash([2, 4, 3]) from 4
    └── StreamExchange Hash([2, 4, 3]) from 5

    Fragment 2
    StreamTableScan { table: ak1, columns: [ak1.k1, ak1.v, ak1.a._row_id], stream_scan_type: Backfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) } { tables: [ StreamScan: 0 ] }
    ├── Upstream
    └── BatchPlanNode

    Fragment 3
    StreamTableScan { table: bk1, columns: [bk1.k1, bk1.v, bk1.b._row_id], stream_scan_type: UpstreamOnly, stream_key: [bk1.b._row_id], pk: [k1, b._row_id], dist: UpstreamHashShard(bk1.k1) } { tables: [ StreamScan: 1 ] }
    ├── Upstream
    └── BatchPlanNode

    Fragment 4
    Lookup
    ├── StreamExchange Hash([0]) from 3
    └── StreamExchange NoShuffle from 2

    Fragment 5
    Lookup
    ├── StreamExchange Hash([0]) from 2
    └── StreamExchange NoShuffle from 3

    Table 0 { columns: [ vnode, k1, a__row_id, backfill_finished, row_count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2, 3, 4 ], distribution key: [ 0 ], read pk prefix len hint: 1, vnode column idx: 0 }

    Table 1 { columns: [ vnode, k1, b__row_id, backfill_finished, row_count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2, 3, 4 ], distribution key: [ 0 ], read pk prefix len hint: 1, vnode column idx: 0 }

    Table 4294967294 { columns: [ v, bv, ak1.a._row_id, ak1.k1, bk1.b._row_id, _rw_timestamp ], primary key: [ $2 ASC, $4 ASC, $3 ASC ], value indices: [ 0, 1, 2, 3, 4 ], distribution key: [ 2, 3, 4 ], read pk prefix len hint: 3 }

- id: A_join_Bk1_onk1
  before:
  - create_tables
  sql: select A.v, B.v as Bv from A join Bk1 as B using(k1)
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchLookupJoin { type: Inner, predicate: a.k1 = bk1.k1, output: [a.v, bk1.v], lookup table: bk1 }
      └─BatchExchange { order: [], dist: UpstreamHashShard(a.k1) }
        └─BatchScan { table: a, columns: [a.k1, a.v], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [v, bv, ak1.a._row_id(hidden), ak1.k1(hidden), bk1.b._row_id(hidden)], stream_key: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_columns: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_conflict: NoCheck }
    └─StreamExchange { dist: HashShard(ak1.a._row_id, ak1.k1, bk1.b._row_id) }
      └─StreamDeltaJoin { type: Inner, predicate: ak1.k1 = bk1.k1, output: [ak1.v, bk1.v, ak1.a._row_id, ak1.k1, bk1.b._row_id] }
        ├─StreamTableScan { table: ak1, columns: [ak1.k1, ak1.v, ak1.a._row_id], stream_scan_type: Backfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) }
        └─StreamTableScan { table: bk1, columns: [bk1.k1, bk1.v, bk1.b._row_id], stream_scan_type: UpstreamOnly, stream_key: [bk1.b._row_id], pk: [k1, b._row_id], dist: UpstreamHashShard(bk1.k1) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [v, bv, ak1.a._row_id(hidden), ak1.k1(hidden), bk1.b._row_id(hidden)], stream_key: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_columns: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_conflict: NoCheck }
    └── StreamExchange Hash([2, 3, 4]) from 1

    Fragment 1
    Union
    ├── StreamExchange Hash([2, 4, 3]) from 4
    └── StreamExchange Hash([2, 4, 3]) from 5

    Fragment 2
    StreamTableScan { table: ak1, columns: [ak1.k1, ak1.v, ak1.a._row_id], stream_scan_type: Backfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) } { tables: [ StreamScan: 0 ] }
    ├── Upstream
    └── BatchPlanNode

    Fragment 3
    StreamTableScan { table: bk1, columns: [bk1.k1, bk1.v, bk1.b._row_id], stream_scan_type: UpstreamOnly, stream_key: [bk1.b._row_id], pk: [k1, b._row_id], dist: UpstreamHashShard(bk1.k1) } { tables: [ StreamScan: 1 ] }
    ├── Upstream
    └── BatchPlanNode

    Fragment 4
    Lookup
    ├── StreamExchange Hash([0]) from 3
    └── StreamExchange NoShuffle from 2

    Fragment 5
    Lookup
    ├── StreamExchange Hash([0]) from 2
    └── StreamExchange NoShuffle from 3

    Table 0 { columns: [ vnode, k1, a__row_id, backfill_finished, row_count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2, 3, 4 ], distribution key: [ 0 ], read pk prefix len hint: 1, vnode column idx: 0 }

    Table 1 { columns: [ vnode, k1, b__row_id, backfill_finished, row_count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2, 3, 4 ], distribution key: [ 0 ], read pk prefix len hint: 1, vnode column idx: 0 }

    Table 4294967294 { columns: [ v, bv, ak1.a._row_id, ak1.k1, bk1.b._row_id, _rw_timestamp ], primary key: [ $2 ASC, $4 ASC, $3 ASC ], value indices: [ 0, 1, 2, 3, 4 ], distribution key: [ 2, 3, 4 ], read pk prefix len hint: 3 }

- id: Ak1_join_Bk1_onk1
  before:
  - create_tables
  sql: select A.v, B.v as Bv from Ak1 as A join Bk1 as B using(k1)
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchLookupJoin { type: Inner, predicate: ak1.k1 = bk1.k1, output: [ak1.v, bk1.v], lookup table: bk1 }
      └─BatchExchange { order: [], dist: UpstreamHashShard(ak1.k1) }
        └─BatchScan { table: ak1, columns: [ak1.k1, ak1.v], distribution: UpstreamHashShard(ak1.k1) }
  stream_plan: |-
    StreamMaterialize { columns: [v, bv, ak1.a._row_id(hidden), ak1.k1(hidden), bk1.b._row_id(hidden)], stream_key: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_columns: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_conflict: NoCheck }
    └─StreamExchange { dist: HashShard(ak1.a._row_id, ak1.k1, bk1.b._row_id) }
      └─StreamDeltaJoin { type: Inner, predicate: ak1.k1 = bk1.k1, output: [ak1.v, bk1.v, ak1.a._row_id, ak1.k1, bk1.b._row_id] }
        ├─StreamTableScan { table: ak1, columns: [ak1.k1, ak1.v, ak1.a._row_id], stream_scan_type: Backfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) }
        └─StreamTableScan { table: bk1, columns: [bk1.k1, bk1.v, bk1.b._row_id], stream_scan_type: UpstreamOnly, stream_key: [bk1.b._row_id], pk: [k1, b._row_id], dist: UpstreamHashShard(bk1.k1) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [v, bv, ak1.a._row_id(hidden), ak1.k1(hidden), bk1.b._row_id(hidden)], stream_key: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_columns: [ak1.a._row_id, bk1.b._row_id, ak1.k1], pk_conflict: NoCheck }
    └── StreamExchange Hash([2, 3, 4]) from 1

    Fragment 1
    Union
    ├── StreamExchange Hash([2, 4, 3]) from 4
    └── StreamExchange Hash([2, 4, 3]) from 5

    Fragment 2
    StreamTableScan { table: ak1, columns: [ak1.k1, ak1.v, ak1.a._row_id], stream_scan_type: Backfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) } { tables: [ StreamScan: 0 ] }
    ├── Upstream
    └── BatchPlanNode

    Fragment 3
    StreamTableScan { table: bk1, columns: [bk1.k1, bk1.v, bk1.b._row_id], stream_scan_type: UpstreamOnly, stream_key: [bk1.b._row_id], pk: [k1, b._row_id], dist: UpstreamHashShard(bk1.k1) } { tables: [ StreamScan: 1 ] }
    ├── Upstream
    └── BatchPlanNode

    Fragment 4
    Lookup
    ├── StreamExchange Hash([0]) from 3
    └── StreamExchange NoShuffle from 2

    Fragment 5
    Lookup
    ├── StreamExchange Hash([0]) from 2
    └── StreamExchange NoShuffle from 3

    Table 0 { columns: [ vnode, k1, a__row_id, backfill_finished, row_count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2, 3, 4 ], distribution key: [ 0 ], read pk prefix len hint: 1, vnode column idx: 0 }

    Table 1 { columns: [ vnode, k1, b__row_id, backfill_finished, row_count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2, 3, 4 ], distribution key: [ 0 ], read pk prefix len hint: 1, vnode column idx: 0 }

    Table 4294967294 { columns: [ v, bv, ak1.a._row_id, ak1.k1, bk1.b._row_id, _rw_timestamp ], primary key: [ $2 ASC, $4 ASC, $3 ASC ], value indices: [ 0, 1, 2, 3, 4 ], distribution key: [ 2, 3, 4 ], read pk prefix len hint: 3 }

- id: aggk1_from_A
  before:
  - create_tables
  sql: |
    select max(v) as max_v
    from A
    group by k1;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [max(a.v)] }
      └─BatchHashAgg { group_key: [a.k1], aggs: [max(a.v)] }
        └─BatchExchange { order: [], dist: HashShard(a.k1) }
          └─BatchScan { table: a, columns: [a.k1, a.v], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [max_v, a.k1(hidden)], stream_key: [a.k1], pk_columns: [a.k1], pk_conflict: NoCheck }
    └─StreamProject { exprs: [max(a.v), a.k1] }
      └─StreamHashAgg { group_key: [a.k1], aggs: [max(a.v), count] }
        └─StreamExchange { dist: HashShard(a.k1) }
          └─StreamTableScan { table: a, columns: [a.k1, a.v, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [max_v, a.k1(hidden)], stream_key: [a.k1], pk_columns: [a.k1], pk_conflict: NoCheck }
    └── StreamProject { exprs: [max(a.v), a.k1] }
        └── StreamHashAgg { group_key: [a.k1], aggs: [max(a.v), count] }
            ├── tables: [ HashAggState: 1, HashAggCall0: 0 ]
            └── StreamExchange Hash([0]) from 1

    Fragment 1
    StreamTableScan { table: a, columns: [a.k1, a.v, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
    ├── tables: [ StreamScan: 2 ]
    ├── Upstream
    └── BatchPlanNode

    Table 0
    ├── columns: [ a_k1, a_v, a__row_id, _rw_timestamp ]
    ├── primary key: [ $0 ASC, $1 DESC, $2 ASC ]
    ├── value indices: [ 1, 2 ]
    ├── distribution key: [ 0 ]
    └── read pk prefix len hint: 1

    Table 1 { columns: [ a_k1, max(a_v), count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 2
    ├── columns: [ vnode, _row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 4294967294 { columns: [ max_v, a.k1, _rw_timestamp ], primary key: [ $1 ASC ], value indices: [ 0, 1 ], distribution key: [ 1 ], read pk prefix len hint: 1 }

- id: aggk1_from_Ak1
  before:
  - create_tables
  sql: |
    select max(v) as max_v
    from Ak1 as A
    group by k1;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [max(ak1.v)] }
      └─BatchSortAgg { group_key: [ak1.k1], aggs: [max(ak1.v)] }
        └─BatchScan { table: ak1, columns: [ak1.k1, ak1.v], distribution: UpstreamHashShard(ak1.k1) }
  stream_plan: |-
    StreamMaterialize { columns: [max_v, ak1.k1(hidden)], stream_key: [ak1.k1], pk_columns: [ak1.k1], pk_conflict: NoCheck }
    └─StreamProject { exprs: [max(ak1.v), ak1.k1] }
      └─StreamHashAgg { group_key: [ak1.k1], aggs: [max(ak1.v), count] }
        └─StreamTableScan { table: ak1, columns: [ak1.k1, ak1.v, ak1.a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [max_v, ak1.k1(hidden)], stream_key: [ak1.k1], pk_columns: [ak1.k1], pk_conflict: NoCheck }
    └── StreamProject { exprs: [max(ak1.v), ak1.k1] }
        └── StreamHashAgg { group_key: [ak1.k1], aggs: [max(ak1.v), count] } { tables: [ HashAggState: 1, HashAggCall0: 0 ] }
            └── StreamTableScan { table: ak1, columns: [ak1.k1, ak1.v, ak1.a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) }
                ├── tables: [ StreamScan: 2 ]
                ├── Upstream
                └── BatchPlanNode

    Table 0 { columns: [ ak1_k1, ak1_v, ak1_a__row_id, _rw_timestamp ], primary key: [ $0 ASC, $1 DESC, $2 ASC ], value indices: [ 1, 2 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 1 { columns: [ ak1_k1, max(ak1_v), count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 2
    ├── columns: [ vnode, k1, a__row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3, 4 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 4294967294 { columns: [ max_v, ak1.k1, _rw_timestamp ], primary key: [ $1 ASC ], value indices: [ 0, 1 ], distribution key: [ 1 ], read pk prefix len hint: 1 }

- id: aggk1_from_Ak1k2
  before:
  - create_tables
  sql: |
    select max(v) as max_v
    from Ak1k2 as A
    group by k1;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [max(ak1k2.v)] }
      └─BatchSortAgg { group_key: [ak1k2.k1], aggs: [max(ak1k2.v)] }
        └─BatchScan { table: ak1k2, columns: [ak1k2.k1, ak1k2.v], distribution: UpstreamHashShard(ak1k2.k1) }
  stream_plan: |-
    StreamMaterialize { columns: [max_v, ak1k2.k1(hidden)], stream_key: [ak1k2.k1], pk_columns: [ak1k2.k1], pk_conflict: NoCheck }
    └─StreamProject { exprs: [max(ak1k2.v), ak1k2.k1] }
      └─StreamHashAgg { group_key: [ak1k2.k1], aggs: [max(ak1k2.v), count] }
        └─StreamTableScan { table: ak1k2, columns: [ak1k2.k1, ak1k2.v, ak1k2.k2, ak1k2.a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [ak1k2.a._row_id], pk: [k1, k2, a._row_id], dist: UpstreamHashShard(ak1k2.k1) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [max_v, ak1k2.k1(hidden)], stream_key: [ak1k2.k1], pk_columns: [ak1k2.k1], pk_conflict: NoCheck }
    └── StreamProject { exprs: [max(ak1k2.v), ak1k2.k1] }
        └── StreamHashAgg { group_key: [ak1k2.k1], aggs: [max(ak1k2.v), count] } { tables: [ HashAggState: 1, HashAggCall0: 0 ] }
            └── StreamTableScan { table: ak1k2, columns: [ak1k2.k1, ak1k2.v, ak1k2.k2, ak1k2.a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [ak1k2.a._row_id], pk: [k1, k2, a._row_id], dist: UpstreamHashShard(ak1k2.k1) }
                ├── tables: [ StreamScan: 2 ]
                ├── Upstream
                └── BatchPlanNode

    Table 0 { columns: [ ak1k2_k1, ak1k2_v, ak1k2_a__row_id, _rw_timestamp ], primary key: [ $0 ASC, $1 DESC, $2 ASC ], value indices: [ 1, 2 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 1 { columns: [ ak1k2_k1, max(ak1k2_v), count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 2 { columns: [ vnode, k1, k2, a__row_id, backfill_finished, row_count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2, 3, 4, 5 ], distribution key: [ 0 ], read pk prefix len hint: 1, vnode column idx: 0 }

    Table 4294967294 { columns: [ max_v, ak1k2.k1, _rw_timestamp ], primary key: [ $1 ASC ], value indices: [ 0, 1 ], distribution key: [ 1 ], read pk prefix len hint: 1 }

- id: aggk2_from_Ak1k2
  before:
  - create_tables
  sql: |
    select max(v) as max_v
    from Ak1k2 as A
    group by k2;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [max(ak1k2.v)] }
      └─BatchHashAgg { group_key: [ak1k2.k2], aggs: [max(ak1k2.v)] }
        └─BatchExchange { order: [], dist: HashShard(ak1k2.k2) }
          └─BatchScan { table: ak1k2, columns: [ak1k2.k2, ak1k2.v], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [max_v, ak1k2.k2(hidden)], stream_key: [ak1k2.k2], pk_columns: [ak1k2.k2], pk_conflict: NoCheck }
    └─StreamProject { exprs: [max(ak1k2.v), ak1k2.k2] }
      └─StreamHashAgg { group_key: [ak1k2.k2], aggs: [max(ak1k2.v), count] }
        └─StreamExchange { dist: HashShard(ak1k2.k2) }
          └─StreamTableScan { table: ak1k2, columns: [ak1k2.k2, ak1k2.v, ak1k2.k1, ak1k2.a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [ak1k2.a._row_id], pk: [k1, k2, a._row_id], dist: UpstreamHashShard(ak1k2.k1) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [max_v, ak1k2.k2(hidden)], stream_key: [ak1k2.k2], pk_columns: [ak1k2.k2], pk_conflict: NoCheck }
    └── StreamProject { exprs: [max(ak1k2.v), ak1k2.k2] }
        └── StreamHashAgg { group_key: [ak1k2.k2], aggs: [max(ak1k2.v), count] } { tables: [ HashAggState: 1, HashAggCall0: 0 ] }
            └── StreamExchange Hash([0]) from 1

    Fragment 1
    StreamTableScan { table: ak1k2, columns: [ak1k2.k2, ak1k2.v, ak1k2.k1, ak1k2.a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [ak1k2.a._row_id], pk: [k1, k2, a._row_id], dist: UpstreamHashShard(ak1k2.k1) }
    ├── tables: [ StreamScan: 2 ]
    ├── Upstream
    └── BatchPlanNode

    Table 0 { columns: [ ak1k2_k2, ak1k2_v, ak1k2_a__row_id, _rw_timestamp ], primary key: [ $0 ASC, $1 DESC, $2 ASC ], value indices: [ 1, 2 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 1 { columns: [ ak1k2_k2, max(ak1k2_v), count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 2
    ├── columns: [ vnode, k1, k2, a__row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3, 4, 5 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 4294967294 { columns: [ max_v, ak1k2.k2, _rw_timestamp ], primary key: [ $1 ASC ], value indices: [ 0, 1 ], distribution key: [ 1 ], read pk prefix len hint: 1 }

- id: aggk1k2_from_Ak1k2
  before:
  - create_tables
  sql: |
    select sum(v) as sum_v
    from Ak1k2 as A
    group by k1, k2;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [sum(ak1k2.v)] }
      └─BatchSortAgg { group_key: [ak1k2.k1, ak1k2.k2], aggs: [sum(ak1k2.v)] }
        └─BatchScan { table: ak1k2, columns: [ak1k2.k1, ak1k2.k2, ak1k2.v], distribution: UpstreamHashShard(ak1k2.k1) }
  stream_plan: |-
    StreamMaterialize { columns: [sum_v, ak1k2.k1(hidden), ak1k2.k2(hidden)], stream_key: [ak1k2.k1, ak1k2.k2], pk_columns: [ak1k2.k1, ak1k2.k2], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum(ak1k2.v), ak1k2.k1, ak1k2.k2] }
      └─StreamHashAgg { group_key: [ak1k2.k1, ak1k2.k2], aggs: [sum(ak1k2.v), count] }
        └─StreamTableScan { table: ak1k2, columns: [ak1k2.k1, ak1k2.k2, ak1k2.v, ak1k2.a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [ak1k2.a._row_id], pk: [k1, k2, a._row_id], dist: UpstreamHashShard(ak1k2.k1) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [sum_v, ak1k2.k1(hidden), ak1k2.k2(hidden)], stream_key: [ak1k2.k1, ak1k2.k2], pk_columns: [ak1k2.k1, ak1k2.k2], pk_conflict: NoCheck }
    └── StreamProject { exprs: [sum(ak1k2.v), ak1k2.k1, ak1k2.k2] }
        └── StreamHashAgg { group_key: [ak1k2.k1, ak1k2.k2], aggs: [sum(ak1k2.v), count] } { tables: [ HashAggState: 0 ] }
            └── StreamTableScan { table: ak1k2, columns: [ak1k2.k1, ak1k2.k2, ak1k2.v, ak1k2.a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [ak1k2.a._row_id], pk: [k1, k2, a._row_id], dist: UpstreamHashShard(ak1k2.k1) }
                ├── tables: [ StreamScan: 1 ]
                ├── Upstream
                └── BatchPlanNode

    Table 0 { columns: [ ak1k2_k1, ak1k2_k2, sum(ak1k2_v), count, _rw_timestamp ], primary key: [ $0 ASC, $1 ASC ], value indices: [ 2, 3 ], distribution key: [ 0 ], read pk prefix len hint: 2 }

    Table 1 { columns: [ vnode, k1, k2, a__row_id, backfill_finished, row_count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2, 3, 4, 5 ], distribution key: [ 0 ], read pk prefix len hint: 1, vnode column idx: 0 }

    Table 4294967294 { columns: [ sum_v, ak1k2.k1, ak1k2.k2, _rw_timestamp ], primary key: [ $1 ASC, $2 ASC ], value indices: [ 0, 1, 2 ], distribution key: [ 1 ], read pk prefix len hint: 2 }

- id: aggk1k2_from_Ak1
  before:
  - create_tables
  sql: |
    select sum(v) as sum_v
    from Ak1 as A
    group by k1, k2;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [sum(ak1.v)] }
      └─BatchHashAgg { group_key: [ak1.k1, ak1.k2], aggs: [sum(ak1.v)] }
        └─BatchScan { table: ak1, columns: [ak1.k1, ak1.k2, ak1.v], distribution: UpstreamHashShard(ak1.k1) }
  stream_plan: |-
    StreamMaterialize { columns: [sum_v, ak1.k1(hidden), ak1.k2(hidden)], stream_key: [ak1.k1, ak1.k2], pk_columns: [ak1.k1, ak1.k2], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum(ak1.v), ak1.k1, ak1.k2] }
      └─StreamHashAgg { group_key: [ak1.k1, ak1.k2], aggs: [sum(ak1.v), count] }
        └─StreamTableScan { table: ak1, columns: [ak1.k1, ak1.k2, ak1.v, ak1.a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [sum_v, ak1.k1(hidden), ak1.k2(hidden)], stream_key: [ak1.k1, ak1.k2], pk_columns: [ak1.k1, ak1.k2], pk_conflict: NoCheck }
    └── StreamProject { exprs: [sum(ak1.v), ak1.k1, ak1.k2] }
        └── StreamHashAgg { group_key: [ak1.k1, ak1.k2], aggs: [sum(ak1.v), count] } { tables: [ HashAggState: 0 ] }
            └── StreamTableScan { table: ak1, columns: [ak1.k1, ak1.k2, ak1.v, ak1.a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) }
                ├── tables: [ StreamScan: 1 ]
                ├── Upstream
                └── BatchPlanNode

    Table 0 { columns: [ ak1_k1, ak1_k2, sum(ak1_v), count, _rw_timestamp ], primary key: [ $0 ASC, $1 ASC ], value indices: [ 2, 3 ], distribution key: [ 0 ], read pk prefix len hint: 2 }

    Table 1
    ├── columns: [ vnode, k1, a__row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3, 4 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 4294967294 { columns: [ sum_v, ak1.k1, ak1.k2, _rw_timestamp ], primary key: [ $1 ASC, $2 ASC ], value indices: [ 0, 1, 2 ], distribution key: [ 1 ], read pk prefix len hint: 2 }

- id: aggk1_from_aggk1
  before:
  - create_tables
  sql: |
    select
      max(num) as max_num
    from (
      select
        count(*) as num, k1
      from A
      group by k1
    )
    group by k1;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [max(count)] }
      └─BatchHashAgg { group_key: [a.k1], aggs: [max(count)] }
        └─BatchHashAgg { group_key: [a.k1], aggs: [count] }
          └─BatchExchange { order: [], dist: HashShard(a.k1) }
            └─BatchScan { table: a, columns: [a.k1], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [max_num, a.k1(hidden)], stream_key: [a.k1], pk_columns: [a.k1], pk_conflict: NoCheck }
    └─StreamProject { exprs: [max(count), a.k1] }
      └─StreamHashAgg { group_key: [a.k1], aggs: [max(count), count] }
        └─StreamHashAgg { group_key: [a.k1], aggs: [count] }
          └─StreamExchange { dist: HashShard(a.k1) }
            └─StreamTableScan { table: a, columns: [a.k1, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [max_num, a.k1(hidden)], stream_key: [a.k1], pk_columns: [a.k1], pk_conflict: NoCheck }
    └── StreamProject { exprs: [max(count), a.k1] }
        └── StreamHashAgg { group_key: [a.k1], aggs: [max(count), count] }
            ├── tables: [ HashAggState: 1, HashAggCall0: 0 ]
            └── StreamHashAgg { group_key: [a.k1], aggs: [count] } { tables: [ HashAggState: 2 ] }
                └── StreamExchange Hash([0]) from 1

    Fragment 1
    StreamTableScan { table: a, columns: [a.k1, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
    ├── tables: [ StreamScan: 3 ]
    ├── Upstream
    └── BatchPlanNode

    Table 0 { columns: [ a_k1, count, _rw_timestamp ], primary key: [ $0 ASC, $1 DESC ], value indices: [ 0, 1 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 1 { columns: [ a_k1, max(count), count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 2 { columns: [ a_k1, count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 3
    ├── columns: [ vnode, _row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 4294967294 { columns: [ max_num, a.k1, _rw_timestamp ], primary key: [ $1 ASC ], value indices: [ 0, 1 ], distribution key: [ 1 ], read pk prefix len hint: 1 }

- id: aggk1_from_aggk1k2
  before:
  - create_tables
  sql: |
    select
      max(num) as max_num
    from (
      select
        count(*) as num, k1
      from A
      group by k1, k2
    )
    group by k1;
  logical_plan: |-
    LogicalProject { exprs: [max(count)] }
    └─LogicalAgg { group_key: [a.k1], aggs: [max(count)] }
      └─LogicalProject { exprs: [a.k1, count] }
        └─LogicalProject { exprs: [count, a.k1] }
          └─LogicalAgg { group_key: [a.k1, a.k2], aggs: [count] }
            └─LogicalProject { exprs: [a.k1, a.k2] }
              └─LogicalScan { table: a, columns: [a.k1, a.k2, a.k3, a.v, a._row_id, a._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalProject { exprs: [max(count)] }
    └─LogicalAgg { group_key: [a.k1], aggs: [max(count)] }
      └─LogicalAgg { group_key: [a.k1, a.k2], aggs: [count] }
        └─LogicalScan { table: a, columns: [a.k1, a.k2] }
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [max(count)] }
      └─BatchHashAgg { group_key: [a.k1], aggs: [max(count)] }
        └─BatchExchange { order: [], dist: HashShard(a.k1) }
          └─BatchHashAgg { group_key: [a.k1, a.k2], aggs: [count] }
            └─BatchExchange { order: [], dist: HashShard(a.k1, a.k2) }
              └─BatchScan { table: a, columns: [a.k1, a.k2], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [max_num, a.k1(hidden)], stream_key: [a.k1], pk_columns: [a.k1], pk_conflict: NoCheck }
    └─StreamProject { exprs: [max(count), a.k1] }
      └─StreamHashAgg { group_key: [a.k1], aggs: [max(count), count] }
        └─StreamExchange { dist: HashShard(a.k1) }
          └─StreamHashAgg { group_key: [a.k1, a.k2], aggs: [count] }
            └─StreamExchange { dist: HashShard(a.k1, a.k2) }
              └─StreamTableScan { table: a, columns: [a.k1, a.k2, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [max_num, a.k1(hidden)], stream_key: [a.k1], pk_columns: [a.k1], pk_conflict: NoCheck }
    └── StreamProject { exprs: [max(count), a.k1] }
        └── StreamHashAgg { group_key: [a.k1], aggs: [max(count), count] }
            ├── tables: [ HashAggState: 1, HashAggCall0: 0 ]
            └── StreamExchange Hash([0]) from 1

    Fragment 1
    StreamHashAgg { group_key: [a.k1, a.k2], aggs: [count] } { tables: [ HashAggState: 2 ] }
    └── StreamExchange Hash([0, 1]) from 2

    Fragment 2
    StreamTableScan { table: a, columns: [a.k1, a.k2, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
    ├── tables: [ StreamScan: 3 ]
    ├── Upstream
    └── BatchPlanNode

    Table 0
    ├── columns: [ a_k1, count, a_k2, _rw_timestamp ]
    ├── primary key: [ $0 ASC, $1 DESC, $2 ASC ]
    ├── value indices: [ 0, 1, 2 ]
    ├── distribution key: [ 0 ]
    └── read pk prefix len hint: 1

    Table 1 { columns: [ a_k1, max(count), count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 2 { columns: [ a_k1, a_k2, count, _rw_timestamp ], primary key: [ $0 ASC, $1 ASC ], value indices: [ 2 ], distribution key: [ 0, 1 ], read pk prefix len hint: 2 }

    Table 3
    ├── columns: [ vnode, _row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 4294967294 { columns: [ max_num, a.k1, _rw_timestamp ], primary key: [ $1 ASC ], value indices: [ 0, 1 ], distribution key: [ 1 ], read pk prefix len hint: 1 }

- id: aggk2_from_aggk1k2
  before:
  - create_tables
  sql: |
    select
      max(num) as max_num
    from (
      select
        count(*) as num, k2
      from A
      group by k1, k2
    )
    group by k2;
  logical_plan: |-
    LogicalProject { exprs: [max(count)] }
    └─LogicalAgg { group_key: [a.k2], aggs: [max(count)] }
      └─LogicalProject { exprs: [a.k2, count] }
        └─LogicalProject { exprs: [count, a.k2] }
          └─LogicalAgg { group_key: [a.k1, a.k2], aggs: [count] }
            └─LogicalProject { exprs: [a.k1, a.k2] }
              └─LogicalScan { table: a, columns: [a.k1, a.k2, a.k3, a.v, a._row_id, a._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalProject { exprs: [max(count)] }
    └─LogicalAgg { group_key: [a.k2], aggs: [max(count)] }
      └─LogicalAgg { group_key: [a.k1, a.k2], aggs: [count] }
        └─LogicalScan { table: a, columns: [a.k1, a.k2] }
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [max(count)] }
      └─BatchHashAgg { group_key: [a.k2], aggs: [max(count)] }
        └─BatchExchange { order: [], dist: HashShard(a.k2) }
          └─BatchHashAgg { group_key: [a.k1, a.k2], aggs: [count] }
            └─BatchExchange { order: [], dist: HashShard(a.k1, a.k2) }
              └─BatchScan { table: a, columns: [a.k1, a.k2], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [max_num, a.k2(hidden)], stream_key: [a.k2], pk_columns: [a.k2], pk_conflict: NoCheck }
    └─StreamProject { exprs: [max(count), a.k2] }
      └─StreamHashAgg { group_key: [a.k2], aggs: [max(count), count] }
        └─StreamExchange { dist: HashShard(a.k2) }
          └─StreamHashAgg { group_key: [a.k1, a.k2], aggs: [count] }
            └─StreamExchange { dist: HashShard(a.k1, a.k2) }
              └─StreamTableScan { table: a, columns: [a.k1, a.k2, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [max_num, a.k2(hidden)], stream_key: [a.k2], pk_columns: [a.k2], pk_conflict: NoCheck }
    └── StreamProject { exprs: [max(count), a.k2] }
        └── StreamHashAgg { group_key: [a.k2], aggs: [max(count), count] }
            ├── tables: [ HashAggState: 1, HashAggCall0: 0 ]
            └── StreamExchange Hash([1]) from 1

    Fragment 1
    StreamHashAgg { group_key: [a.k1, a.k2], aggs: [count] } { tables: [ HashAggState: 2 ] }
    └── StreamExchange Hash([0, 1]) from 2

    Fragment 2
    StreamTableScan { table: a, columns: [a.k1, a.k2, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
    ├── tables: [ StreamScan: 3 ]
    ├── Upstream
    └── BatchPlanNode

    Table 0
    ├── columns: [ a_k2, count, a_k1, _rw_timestamp ]
    ├── primary key: [ $0 ASC, $1 DESC, $2 ASC ]
    ├── value indices: [ 0, 1, 2 ]
    ├── distribution key: [ 0 ]
    └── read pk prefix len hint: 1

    Table 1 { columns: [ a_k2, max(count), count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 2 { columns: [ a_k1, a_k2, count, _rw_timestamp ], primary key: [ $0 ASC, $1 ASC ], value indices: [ 2 ], distribution key: [ 0, 1 ], read pk prefix len hint: 2 }

    Table 3
    ├── columns: [ vnode, _row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 4294967294 { columns: [ max_num, a.k2, _rw_timestamp ], primary key: [ $1 ASC ], value indices: [ 0, 1 ], distribution key: [ 1 ], read pk prefix len hint: 1 }

- id: aggk1k2_from_aggk1k2
  before:
  - create_tables
  sql: |
    select
      max(num) as max_num
    from (
      select
        count(*) as num, k1, k2
      from A
      group by k1, k2
    )
    group by k1, k2;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [max(count)] }
      └─BatchHashAgg { group_key: [a.k1, a.k2], aggs: [max(count)] }
        └─BatchHashAgg { group_key: [a.k1, a.k2], aggs: [count] }
          └─BatchExchange { order: [], dist: HashShard(a.k1, a.k2) }
            └─BatchScan { table: a, columns: [a.k1, a.k2], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [max_num, a.k1(hidden), a.k2(hidden)], stream_key: [a.k1, a.k2], pk_columns: [a.k1, a.k2], pk_conflict: NoCheck }
    └─StreamProject { exprs: [max(count), a.k1, a.k2] }
      └─StreamHashAgg { group_key: [a.k1, a.k2], aggs: [max(count), count] }
        └─StreamHashAgg { group_key: [a.k1, a.k2], aggs: [count] }
          └─StreamExchange { dist: HashShard(a.k1, a.k2) }
            └─StreamTableScan { table: a, columns: [a.k1, a.k2, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [max_num, a.k1(hidden), a.k2(hidden)], stream_key: [a.k1, a.k2], pk_columns: [a.k1, a.k2], pk_conflict: NoCheck }
    └── StreamProject { exprs: [max(count), a.k1, a.k2] }
        └── StreamHashAgg { group_key: [a.k1, a.k2], aggs: [max(count), count] } { tables: [ HashAggState: 1, HashAggCall0: 0 ] }
            └── StreamHashAgg { group_key: [a.k1, a.k2], aggs: [count] } { tables: [ HashAggState: 2 ] }
                └── StreamExchange Hash([0, 1]) from 1

    Fragment 1
    StreamTableScan { table: a, columns: [a.k1, a.k2, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
    ├── tables: [ StreamScan: 3 ]
    ├── Upstream
    └── BatchPlanNode

    Table 0
    ├── columns: [ a_k1, a_k2, count, _rw_timestamp ]
    ├── primary key: [ $0 ASC, $1 ASC, $2 DESC ]
    ├── value indices: [ 0, 1, 2 ]
    ├── distribution key: [ 0, 1 ]
    └── read pk prefix len hint: 2

    Table 1
    ├── columns: [ a_k1, a_k2, max(count), count, _rw_timestamp ]
    ├── primary key: [ $0 ASC, $1 ASC ]
    ├── value indices: [ 2, 3 ]
    ├── distribution key: [ 0, 1 ]
    └── read pk prefix len hint: 2

    Table 2 { columns: [ a_k1, a_k2, count, _rw_timestamp ], primary key: [ $0 ASC, $1 ASC ], value indices: [ 2 ], distribution key: [ 0, 1 ], read pk prefix len hint: 2 }

    Table 3
    ├── columns: [ vnode, _row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 4294967294
    ├── columns: [ max_num, a.k1, a.k2, _rw_timestamp ]
    ├── primary key: [ $1 ASC, $2 ASC ]
    ├── value indices: [ 0, 1, 2 ]
    ├── distribution key: [ 1, 2 ]
    └── read pk prefix len hint: 2

- id: Ak1_join_aggk1_onk1
  before:
  - create_tables
  sql: |
    with B as (
      select
        count(*) as num, k1
      from A
      group by k1
    )
    select A.v, B.num as Bv from Ak1 as A join B using(k1)
  logical_plan: |-
    LogicalProject { exprs: [ak1.v, count] }
    └─LogicalJoin { type: Inner, on: (ak1.k1 = a.k1), output: all }
      ├─LogicalScan { table: ak1, columns: [ak1.k1, ak1.k2, ak1.k3, ak1.v, ak1.a._row_id, ak1._rw_timestamp] }
      └─LogicalShare { id: 4 }
        └─LogicalProject { exprs: [count, a.k1] }
          └─LogicalAgg { group_key: [a.k1], aggs: [count] }
            └─LogicalProject { exprs: [a.k1] }
              └─LogicalScan { table: a, columns: [a.k1, a.k2, a.k3, a.v, a._row_id, a._rw_timestamp] }
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchHashJoin { type: Inner, predicate: ak1.k1 = a.k1, output: [ak1.v, count] }
      ├─BatchExchange { order: [], dist: HashShard(ak1.k1) }
      │ └─BatchScan { table: ak1, columns: [ak1.k1, ak1.v], distribution: UpstreamHashShard(ak1.k1) }
      └─BatchHashAgg { group_key: [a.k1], aggs: [count] }
        └─BatchExchange { order: [], dist: HashShard(a.k1) }
          └─BatchScan { table: a, columns: [a.k1], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [v, bv, ak1.a._row_id(hidden), ak1.k1(hidden), a.k1(hidden)], stream_key: [ak1.a._row_id, ak1.k1], pk_columns: [ak1.a._row_id, ak1.k1], pk_conflict: NoCheck }
    └─StreamExchange { dist: HashShard(ak1.a._row_id, ak1.k1) }
      └─StreamHashJoin { type: Inner, predicate: ak1.k1 = a.k1, output: [ak1.v, count, ak1.a._row_id, ak1.k1, a.k1] }
        ├─StreamExchange { dist: HashShard(ak1.k1) }
        │ └─StreamTableScan { table: ak1, columns: [ak1.k1, ak1.v, ak1.a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) }
        └─StreamHashAgg { group_key: [a.k1], aggs: [count] }
          └─StreamExchange { dist: HashShard(a.k1) }
            └─StreamTableScan { table: a, columns: [a.k1, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [v, bv, ak1.a._row_id(hidden), ak1.k1(hidden), a.k1(hidden)], stream_key: [ak1.a._row_id, ak1.k1], pk_columns: [ak1.a._row_id, ak1.k1], pk_conflict: NoCheck }
    └── StreamExchange Hash([2, 3]) from 1

    Fragment 1
    StreamHashJoin { type: Inner, predicate: ak1.k1 = a.k1, output: [ak1.v, count, ak1.a._row_id, ak1.k1, a.k1] }
    ├── tables: [ HashJoinLeft: 0, HashJoinDegreeLeft: 1, HashJoinRight: 2, HashJoinDegreeRight: 3 ]
    ├── StreamExchange Hash([0]) from 2
    └── StreamHashAgg { group_key: [a.k1], aggs: [count] } { tables: [ HashAggState: 5 ] }
        └── StreamExchange Hash([0]) from 3

    Fragment 2
    StreamTableScan { table: ak1, columns: [ak1.k1, ak1.v, ak1.a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) }
    ├── tables: [ StreamScan: 4 ]
    ├── Upstream
    └── BatchPlanNode

    Fragment 3
    StreamTableScan { table: a, columns: [a.k1, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
    ├── tables: [ StreamScan: 6 ]
    ├── Upstream
    └── BatchPlanNode

    Table 0 { columns: [ ak1_k1, ak1_v, ak1_a__row_id, _rw_timestamp ], primary key: [ $0 ASC, $2 ASC ], value indices: [ 0, 1, 2 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 1 { columns: [ ak1_k1, ak1_a__row_id, _degree, _rw_timestamp ], primary key: [ $0 ASC, $1 ASC ], value indices: [ 2 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 2 { columns: [ a_k1, count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 0, 1 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 3 { columns: [ a_k1, _degree, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 4
    ├── columns: [ vnode, k1, a__row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3, 4 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 5 { columns: [ a_k1, count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 6
    ├── columns: [ vnode, _row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 4294967294
    ├── columns: [ v, bv, ak1.a._row_id, ak1.k1, a.k1, _rw_timestamp ]
    ├── primary key: [ $2 ASC, $3 ASC ]
    ├── value indices: [ 0, 1, 2, 3, 4 ]
    ├── distribution key: [ 2, 3 ]
    └── read pk prefix len hint: 2

- id: aggk1_join_Ak1_onk1
  before:
  - create_tables
  sql: |
    with B as (
      select
        count(*) as num, k1
      from A
      group by k1
    )
    select A.v, B.num as Bv from B join Ak1 as A using(k1)
  logical_plan: |-
    LogicalProject { exprs: [ak1.v, count] }
    └─LogicalJoin { type: Inner, on: (a.k1 = ak1.k1), output: all }
      ├─LogicalShare { id: 4 }
      │ └─LogicalProject { exprs: [count, a.k1] }
      │   └─LogicalAgg { group_key: [a.k1], aggs: [count] }
      │     └─LogicalProject { exprs: [a.k1] }
      │       └─LogicalScan { table: a, columns: [a.k1, a.k2, a.k3, a.v, a._row_id, a._rw_timestamp] }
      └─LogicalScan { table: ak1, columns: [ak1.k1, ak1.k2, ak1.k3, ak1.v, ak1.a._row_id, ak1._rw_timestamp] }
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchLookupJoin { type: Inner, predicate: a.k1 = ak1.k1, output: [ak1.v, count], lookup table: ak1 }
      └─BatchExchange { order: [], dist: UpstreamHashShard(a.k1) }
        └─BatchHashAgg { group_key: [a.k1], aggs: [count] }
          └─BatchExchange { order: [], dist: HashShard(a.k1) }
            └─BatchScan { table: a, columns: [a.k1], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [v, bv, a.k1(hidden), ak1.a._row_id(hidden)], stream_key: [a.k1, ak1.a._row_id], pk_columns: [a.k1, ak1.a._row_id], pk_conflict: NoCheck }
    └─StreamExchange { dist: HashShard(a.k1, ak1.a._row_id) }
      └─StreamHashJoin { type: Inner, predicate: a.k1 = ak1.k1, output: [ak1.v, count, a.k1, ak1.a._row_id] }
        ├─StreamHashAgg { group_key: [a.k1], aggs: [count] }
        │ └─StreamExchange { dist: HashShard(a.k1) }
        │   └─StreamTableScan { table: a, columns: [a.k1, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
        └─StreamExchange { dist: HashShard(ak1.k1) }
          └─StreamTableScan { table: ak1, columns: [ak1.k1, ak1.v, ak1.a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [v, bv, a.k1(hidden), ak1.a._row_id(hidden)], stream_key: [a.k1, ak1.a._row_id], pk_columns: [a.k1, ak1.a._row_id], pk_conflict: NoCheck }
    └── StreamExchange Hash([2, 3]) from 1

    Fragment 1
    StreamHashJoin { type: Inner, predicate: a.k1 = ak1.k1, output: [ak1.v, count, a.k1, ak1.a._row_id] }
    ├── tables: [ HashJoinLeft: 0, HashJoinDegreeLeft: 1, HashJoinRight: 2, HashJoinDegreeRight: 3 ]
    ├── StreamHashAgg { group_key: [a.k1], aggs: [count] } { tables: [ HashAggState: 4 ] }
    │   └── StreamExchange Hash([0]) from 2
    └── StreamExchange Hash([0]) from 3

    Fragment 2
    StreamTableScan { table: a, columns: [a.k1, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
    ├── tables: [ StreamScan: 5 ]
    ├── Upstream
    └── BatchPlanNode

    Fragment 3
    StreamTableScan { table: ak1, columns: [ak1.k1, ak1.v, ak1.a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [ak1.a._row_id], pk: [k1, a._row_id], dist: UpstreamHashShard(ak1.k1) }
    ├── tables: [ StreamScan: 6 ]
    ├── Upstream
    └── BatchPlanNode

    Table 0 { columns: [ a_k1, count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 0, 1 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 1 { columns: [ a_k1, _degree, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 2 { columns: [ ak1_k1, ak1_v, ak1_a__row_id, _rw_timestamp ], primary key: [ $0 ASC, $2 ASC ], value indices: [ 0, 1, 2 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 3 { columns: [ ak1_k1, ak1_a__row_id, _degree, _rw_timestamp ], primary key: [ $0 ASC, $1 ASC ], value indices: [ 2 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 4 { columns: [ a_k1, count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 5
    ├── columns: [ vnode, _row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 6
    ├── columns: [ vnode, k1, a__row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3, 4 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 4294967294
    ├── columns: [ v, bv, a.k1, ak1.a._row_id, _rw_timestamp ]
    ├── primary key: [ $2 ASC, $3 ASC ]
    ├── value indices: [ 0, 1, 2, 3 ]
    ├── distribution key: [ 2, 3 ]
    └── read pk prefix len hint: 2

- id: aggk1_join_aggk1_onk1
  before:
  - create_tables
  sql: |
    with A as (
      select
        count(*) as num, k1
      from A
      group by k1
    ), B as (
      select
        count(*) as num, k1
      from B
      group by k1
    )
    select A.num, B.num as Bv from A join B using(k1)
  logical_plan: |-
    LogicalProject { exprs: [count, count] }
    └─LogicalJoin { type: Inner, on: (a.k1 = b.k1), output: all }
      ├─LogicalShare { id: 4 }
      │ └─LogicalProject { exprs: [count, a.k1] }
      │   └─LogicalAgg { group_key: [a.k1], aggs: [count] }
      │     └─LogicalProject { exprs: [a.k1] }
      │       └─LogicalScan { table: a, columns: [a.k1, a.k2, a.k3, a.v, a._row_id, a._rw_timestamp] }
      └─LogicalShare { id: 8 }
        └─LogicalProject { exprs: [count, b.k1] }
          └─LogicalAgg { group_key: [b.k1], aggs: [count] }
            └─LogicalProject { exprs: [b.k1] }
              └─LogicalScan { table: b, columns: [b.k1, b.k2, b.k3, b.v, b._row_id, b._rw_timestamp] }
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchHashJoin { type: Inner, predicate: a.k1 = b.k1, output: [count, count] }
      ├─BatchHashAgg { group_key: [a.k1], aggs: [count] }
      │ └─BatchExchange { order: [], dist: HashShard(a.k1) }
      │   └─BatchScan { table: a, columns: [a.k1], distribution: SomeShard }
      └─BatchHashAgg { group_key: [b.k1], aggs: [count] }
        └─BatchExchange { order: [], dist: HashShard(b.k1) }
          └─BatchScan { table: b, columns: [b.k1], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [num, bv, a.k1(hidden), b.k1(hidden)], stream_key: [a.k1], pk_columns: [a.k1], pk_conflict: NoCheck }
    └─StreamExchange { dist: HashShard(a.k1) }
      └─StreamHashJoin { type: Inner, predicate: a.k1 = b.k1, output: [count, count, a.k1, b.k1] }
        ├─StreamHashAgg { group_key: [a.k1], aggs: [count] }
        │ └─StreamExchange { dist: HashShard(a.k1) }
        │   └─StreamTableScan { table: a, columns: [a.k1, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
        └─StreamHashAgg { group_key: [b.k1], aggs: [count] }
          └─StreamExchange { dist: HashShard(b.k1) }
            └─StreamTableScan { table: b, columns: [b.k1, b._row_id], stream_scan_type: ArrangementBackfill, stream_key: [b._row_id], pk: [_row_id], dist: UpstreamHashShard(b._row_id) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [num, bv, a.k1(hidden), b.k1(hidden)], stream_key: [a.k1], pk_columns: [a.k1], pk_conflict: NoCheck }
    └── StreamExchange Hash([2]) from 1

    Fragment 1
    StreamHashJoin { type: Inner, predicate: a.k1 = b.k1, output: [count, count, a.k1, b.k1] }
    ├── tables: [ HashJoinLeft: 0, HashJoinDegreeLeft: 1, HashJoinRight: 2, HashJoinDegreeRight: 3 ]
    ├── StreamHashAgg { group_key: [a.k1], aggs: [count] } { tables: [ HashAggState: 4 ] }
    │   └── StreamExchange Hash([0]) from 2
    └── StreamHashAgg { group_key: [b.k1], aggs: [count] } { tables: [ HashAggState: 6 ] }
        └── StreamExchange Hash([0]) from 3

    Fragment 2
    StreamTableScan { table: a, columns: [a.k1, a._row_id], stream_scan_type: ArrangementBackfill, stream_key: [a._row_id], pk: [_row_id], dist: UpstreamHashShard(a._row_id) }
    ├── tables: [ StreamScan: 5 ]
    ├── Upstream
    └── BatchPlanNode

    Fragment 3
    StreamTableScan { table: b, columns: [b.k1, b._row_id], stream_scan_type: ArrangementBackfill, stream_key: [b._row_id], pk: [_row_id], dist: UpstreamHashShard(b._row_id) }
    ├── tables: [ StreamScan: 7 ]
    ├── Upstream
    └── BatchPlanNode

    Table 0 { columns: [ a_k1, count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 0, 1 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 1 { columns: [ a_k1, _degree, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 2 { columns: [ b_k1, count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 0, 1 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 3 { columns: [ b_k1, _degree, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 4 { columns: [ a_k1, count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 5
    ├── columns: [ vnode, _row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 6 { columns: [ b_k1, count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1 ], distribution key: [ 0 ], read pk prefix len hint: 1 }

    Table 7
    ├── columns: [ vnode, _row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 4294967294
    ├── columns: [ num, bv, a.k1, b.k1, _rw_timestamp ]
    ├── primary key: [ $2 ASC ]
    ├── value indices: [ 0, 1, 2, 3 ]
    ├── distribution key: [ 2 ]
    └── read pk prefix len hint: 1

- sql: |
    create table t1 (row_id int, uid int, v int, created_at timestamp);
    select * from hop(t1, created_at, interval '15' minute, interval '30' minute);
  logical_plan: |-
    LogicalProject { exprs: [t1.row_id, t1.uid, t1.v, t1.created_at, window_start, window_end] }
    └─LogicalHopWindow { time_col: t1.created_at, slide: 00:15:00, size: 00:30:00, output: all }
      └─LogicalFilter { predicate: IsNotNull(t1.created_at) }
        └─LogicalScan { table: t1, columns: [t1.row_id, t1.uid, t1.v, t1.created_at, t1._row_id, t1._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalHopWindow { time_col: t1.created_at, slide: 00:15:00, size: 00:30:00, output: all }
    └─LogicalScan { table: t1, columns: [t1.row_id, t1.uid, t1.v, t1.created_at], predicate: IsNotNull(t1.created_at) }
  batch_plan: |-
    BatchHopWindow { time_col: t1.created_at, slide: 00:15:00, size: 00:30:00, output: all }
    └─BatchExchange { order: [], dist: Single }
      └─BatchFilter { predicate: IsNotNull(t1.created_at) }
        └─BatchScan { table: t1, columns: [t1.row_id, t1.uid, t1.v, t1.created_at], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [row_id, uid, v, created_at, window_start, window_end, t1._row_id(hidden)], stream_key: [t1._row_id, window_start, window_end], pk_columns: [t1._row_id, window_start, window_end], pk_conflict: NoCheck }
    └─StreamHopWindow { time_col: t1.created_at, slide: 00:15:00, size: 00:30:00, output: [t1.row_id, t1.uid, t1.v, t1.created_at, window_start, window_end, t1._row_id] }
      └─StreamFilter { predicate: IsNotNull(t1.created_at) }
        └─StreamTableScan { table: t1, columns: [t1.row_id, t1.uid, t1.v, t1.created_at, t1._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t1._row_id], pk: [_row_id], dist: UpstreamHashShard(t1._row_id) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [row_id, uid, v, created_at, window_start, window_end, t1._row_id(hidden)], stream_key: [t1._row_id, window_start, window_end], pk_columns: [t1._row_id, window_start, window_end], pk_conflict: NoCheck }
    └── StreamHopWindow { time_col: t1.created_at, slide: 00:15:00, size: 00:30:00, output: [t1.row_id, t1.uid, t1.v, t1.created_at, window_start, window_end, t1._row_id] }
        └── StreamFilter { predicate: IsNotNull(t1.created_at) }
            └── StreamTableScan { table: t1, columns: [t1.row_id, t1.uid, t1.v, t1.created_at, t1._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t1._row_id], pk: [_row_id], dist: UpstreamHashShard(t1._row_id) }
                ├── tables: [ StreamScan: 0 ]
                ├── Upstream
                └── BatchPlanNode

    Table 0 { columns: [ vnode, _row_id, backfill_finished, row_count, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1, 2, 3 ], distribution key: [ 0 ], read pk prefix len hint: 1, vnode column idx: 0 }

    Table 4294967294
    ├── columns: [ row_id, uid, v, created_at, window_start, window_end, t1._row_id, _rw_timestamp ]
    ├── primary key: [ $6 ASC, $4 ASC, $5 ASC ]
    ├── value indices: [ 0, 1, 2, 3, 4, 5, 6 ]
    ├── distribution key: [ 6 ]
    └── read pk prefix len hint: 3

