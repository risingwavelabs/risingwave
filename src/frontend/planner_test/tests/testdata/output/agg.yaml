# This file is automatically generated. See `src/frontend/planner_test/README.md` for more information.
- sql: |
    values(sum(1));
  binder_error: |
    Failed to bind expression: sum(1)

    Caused by:
      Invalid input syntax: aggregate functions are not allowed in VALUES
- sql: |
    values(count(1));
  binder_error: |
    Failed to bind expression: count(1)

    Caused by:
      Invalid input syntax: aggregate functions are not allowed in VALUES
- sql: |
    values(min(1));
  binder_error: |
    Failed to bind expression: min(1)

    Caused by:
      Invalid input syntax: aggregate functions are not allowed in VALUES
- sql: |
    values(1 + max(1));
  binder_error: |
    Failed to bind expression: 1 + max(1)

    Caused by:
      Invalid input syntax: aggregate functions are not allowed in VALUES
- sql: |
    create table t (v1 int);
    select v1 from t where min(v1);
  binder_error: |
    Failed to bind expression: min(v1)

    Caused by:
      Invalid input syntax: aggregate functions are not allowed in WHERE
- sql: |
    create table t(v1 int, v2 int, v3 int);
    select v1, min(v2) + max(v3) * count(v1) as agg from t group by v1;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.v1, (min(t.v2) + (max(t.v3) * count(t.v1))) as $expr1] }
      └─BatchHashAgg { group_key: [t.v1], aggs: [min(t.v2), max(t.v3), count(t.v1)] }
        └─BatchExchange { order: [], dist: HashShard(t.v1) }
          └─BatchScan { table: t, columns: [t.v1, t.v2, t.v3], distribution: SomeShard }
  batch_local_plan: |-
    BatchProject { exprs: [t.v1, (min(t.v2) + (max(t.v3) * count(t.v1))) as $expr1] }
    └─BatchHashAgg { group_key: [t.v1], aggs: [min(t.v2), max(t.v3), count(t.v1)] }
      └─BatchExchange { order: [], dist: Single }
        └─BatchScan { table: t, columns: [t.v1, t.v2, t.v3], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [v1, agg], stream_key: [v1], pk_columns: [v1], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.v1, (min(t.v2) + (max(t.v3) * count(t.v1))) as $expr1] }
      └─StreamHashAgg { group_key: [t.v1], aggs: [min(t.v2), max(t.v3), count(t.v1), count] }
        └─StreamExchange { dist: HashShard(t.v1) }
          └─StreamTableScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t(v1 int, v2 int, v3 int);
    select min(v1) + max(v2) * count(v3) as agg from t;
  batch_plan: |-
    BatchProject { exprs: [(min(min(t.v1)) + (max(max(t.v2)) * sum0(count(t.v3)))) as $expr1] }
    └─BatchSimpleAgg { aggs: [min(min(t.v1)), max(max(t.v2)), sum0(count(t.v3))] }
      └─BatchExchange { order: [], dist: Single }
        └─BatchSimpleAgg { aggs: [min(t.v1), max(t.v2), count(t.v3)] }
          └─BatchScan { table: t, columns: [t.v1, t.v2, t.v3], distribution: SomeShard }
  batch_local_plan: |-
    BatchProject { exprs: [(min(t.v1) + (max(t.v2) * count(t.v3))) as $expr1] }
    └─BatchSimpleAgg { aggs: [min(t.v1), max(t.v2), count(t.v3)] }
      └─BatchExchange { order: [], dist: Single }
        └─BatchScan { table: t, columns: [t.v1, t.v2, t.v3], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [agg], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [(min(min(t.v1)) + (max(max(t.v2)) * sum0(count(t.v3)))) as $expr1] }
      └─StreamSimpleAgg { aggs: [min(min(t.v1)), max(max(t.v2)), sum0(count(t.v3)), count] }
        └─StreamExchange { dist: Single }
          └─StreamHashAgg { group_key: [_vnode], aggs: [min(t.v1), max(t.v2), count(t.v3), count] }
            └─StreamProject { exprs: [t.v1, t.v2, t.v3, t._row_id, Vnode(t._row_id) as _vnode] }
              └─StreamTableScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t(v1 int, v2 int);
    select v1 from t group by v2;
  planner_error: 'Invalid input syntax: column must appear in the GROUP BY clause or be used in an aggregate function'
- sql: |
    create table t(v1 int, v2 int);
    select sum(v1), v1 from t group by v2, v2;
  planner_error: 'Invalid input syntax: column must appear in the GROUP BY clause or be used in an aggregate function'
- sql: |
    create table t(v1 int, v2 int, v3 int);
    select v3, min(v1) * avg(v1+v2) as agg from t group by v3;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.v3, (min(t.v1)::Decimal * (sum($expr1)::Decimal / count($expr1)::Decimal)) as $expr2] }
      └─BatchHashAgg { group_key: [t.v3], aggs: [min(t.v1), sum($expr1), count($expr1)] }
        └─BatchExchange { order: [], dist: HashShard(t.v3) }
          └─BatchProject { exprs: [t.v3, t.v1, (t.v1 + t.v2) as $expr1] }
            └─BatchScan { table: t, columns: [t.v1, t.v2, t.v3], distribution: SomeShard }
  batch_local_plan: |-
    BatchProject { exprs: [t.v3, (min(t.v1)::Decimal * (sum($expr1)::Decimal / count($expr1)::Decimal)) as $expr2] }
    └─BatchHashAgg { group_key: [t.v3], aggs: [min(t.v1), sum($expr1), count($expr1)] }
      └─BatchExchange { order: [], dist: Single }
        └─BatchProject { exprs: [t.v3, t.v1, (t.v1 + t.v2) as $expr1] }
          └─BatchScan { table: t, columns: [t.v1, t.v2, t.v3], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [v3, agg], stream_key: [v3], pk_columns: [v3], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.v3, (min(t.v1)::Decimal * (sum($expr1)::Decimal / count($expr1)::Decimal)) as $expr2] }
      └─StreamHashAgg { group_key: [t.v3], aggs: [min(t.v1), sum($expr1), count($expr1), count] }
        └─StreamExchange { dist: HashShard(t.v3) }
          └─StreamProject { exprs: [t.v3, t.v1, (t.v1 + t.v2) as $expr1, t._row_id] }
            └─StreamTableScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: test logical_agg with complex group expression
  sql: |
    create table t(v1 int, v2 int);
    select min(v1), sum(v1 + v2) from t group by v1 + v2;
  logical_plan: |-
    LogicalProject { exprs: [min(t.v1), sum($expr1)] }
    └─LogicalAgg { group_key: [$expr1], aggs: [min(t.v1), sum($expr1)] }
      └─LogicalProject { exprs: [(t.v1 + t.v2) as $expr1, t.v1] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
- name: test logical_agg with complex group expression
  sql: |
    create table t(v1 int, v2 int, v3 int);
    select v1, sum(v1 * v2) as sum from t group by (v1 + v2) / v3, v1;
  logical_plan: |-
    LogicalProject { exprs: [t.v1, sum($expr2)] }
    └─LogicalAgg { group_key: [$expr1, t.v1], aggs: [sum($expr2)] }
      └─LogicalProject { exprs: [((t.v1 + t.v2) / t.v3) as $expr1, t.v1, (t.v1 * t.v2) as $expr2] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id, t._rw_timestamp] }
- name: test logical_agg with complex group expression
  sql: |
    create table t(v1 int, v2 int);
    select v1 + v2 from t group by v1 + v2;
  logical_plan: |-
    LogicalProject { exprs: [$expr1] }
    └─LogicalAgg { group_key: [$expr1], aggs: [] }
      └─LogicalProject { exprs: [(t.v1 + t.v2) as $expr1] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
- name: "test logical_agg with complex group expression \nshould complain about nested agg call \n"
  sql: |
    create table t(v1 int, v2 int);
    select avg(sum(v1 + v2)) from t group by v1 + v2;
  planner_error: |-
    Feature is not yet implemented: aggregate function inside aggregation calls
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- name: test logical_agg with complex select expression
  sql: |
    create table t(v1 int, v2 int);
    select v1 + v2 from t group by v1, v2;
  logical_plan: |-
    LogicalProject { exprs: [(t.v1 + t.v2) as $expr1] }
    └─LogicalAgg { group_key: [t.v1, t.v2], aggs: [] }
      └─LogicalProject { exprs: [t.v1, t.v2] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
- sql: |
    create table t(v1 int, v2 int);
    select v1 from t group by v1 + v2;
  planner_error: 'Invalid input syntax: column must appear in the GROUP BY clause or be used in an aggregate function'
- name: group by output column ordinal ok
  sql: |
    select 4 + 5 group by 1;
  logical_plan: |-
    LogicalProject { exprs: [$expr1] }
    └─LogicalAgg { group_key: [$expr1], aggs: [] }
      └─LogicalProject { exprs: [(4:Int32 + 5:Int32) as $expr1] }
        └─LogicalValues { rows: [[]], schema: Schema { fields: [] } }
- name: group by const int expr
  sql: |
    select 4 + 5 group by 3 - 2; -- not folded
  logical_plan: |-
    LogicalProject { exprs: [(4:Int32 + 5:Int32) as $expr2] }
    └─LogicalAgg { group_key: [$expr1], aggs: [] }
      └─LogicalProject { exprs: [(3:Int32 - 2:Int32) as $expr1] }
        └─LogicalValues { rows: [[]], schema: Schema { fields: [] } }
- name: group by output column ordinal of agg
  sql: |
    select sum(2) group by 1;
  planner_error: |-
    Feature is not yet implemented: aggregate function inside GROUP BY
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- name: group by output column ordinal non-integer const
  sql: |
    select 4 + 5 group by null; -- no implicit cast
  binder_error: 'Bind error: non-integer constant in GROUP BY'
- name: group by output column ordinal bigint
  sql: |
    select 4 + 5 group by 2147483648;
  binder_error: 'Bind error: non-integer constant in GROUP BY'
- name: group by output column ordinal negative
  sql: |
    select 4 + 5 group by -2147483648;
  binder_error: 'Bind error: GROUP BY position -2147483648 is not in select list'
- name: group by output column ordinal zero
  sql: |
    select 4 + 5 group by 0;
  binder_error: 'Bind error: GROUP BY position 0 is not in select list'
- name: group by output column ordinal out of bound (excl extra order by exprs)
  sql: |
    select 4 + 5 group by 2 order by 7 + 8, 3 + 2;
  binder_error: 'Bind error: GROUP BY position 2 is not in select list'
- name: group by output column name ok
  sql: |
    select 4 + 5 as a group by a;
  logical_plan: |-
    LogicalProject { exprs: [$expr1] }
    └─LogicalAgg { group_key: [$expr1], aggs: [] }
      └─LogicalProject { exprs: [(4:Int32 + 5:Int32) as $expr1] }
        └─LogicalValues { rows: [[]], schema: Schema { fields: [] } }
- name: group by output column name ambiguous 2
  sql: |
    select 4 + 5 as a, 2 + 3 as a group by a;
  binder_error: 'Bind error: GROUP BY "a" is ambiguous'
- name: group by output column name ambiguous 3
  sql: |
    select 4 + 5 as a, 2 + 3 as a, 3 + 4 as a group by a;
  binder_error: 'Bind error: GROUP BY "a" is ambiguous'
- name: group by output column name not ambiguous when unused
  sql: |
    select 4 + 5 as a, 2 + 3 as a, 3 + 4 as b group by b;
  logical_plan: |-
    LogicalProject { exprs: [(4:Int32 + 5:Int32) as $expr2, (2:Int32 + 3:Int32) as $expr3, $expr1] }
    └─LogicalAgg { group_key: [$expr1], aggs: [] }
      └─LogicalProject { exprs: [(3:Int32 + 4:Int32) as $expr1] }
        └─LogicalValues { rows: [[]], schema: Schema { fields: [] } }
- name: group by output column name not ambiguous when input preferred
  sql: |
    create table t (a int);
    select 4 + 5 as a, 2 + 3 as a from t group by a;
  logical_plan: |-
    LogicalProject { exprs: [(4:Int32 + 5:Int32) as $expr1, (2:Int32 + 3:Int32) as $expr2] }
    └─LogicalAgg { group_key: [t.a], aggs: [] }
      └─LogicalProject { exprs: [t.a] }
        └─LogicalScan { table: t, columns: [t.a, t._row_id, t._rw_timestamp] }
- name: group by output column name expr disallowed
  sql: |
    select 4 + 5 as a group by a + 1;
  binder_error: |
    Failed to bind expression: a + 1

    Caused by:
      Item not found: Invalid column: a
- name: group by prefers input while order by prefers output
  sql: |
    create table t (a int);
    select 4 + 5 as a from t group by a order by a;
  batch_plan: |-
    BatchExchange { order: [9:Int32 ASC], dist: Single }
    └─BatchSort { order: [9:Int32 ASC] }
      └─BatchProject { exprs: [9:Int32] }
        └─BatchHashAgg { group_key: [t.a], aggs: [] }
          └─BatchExchange { order: [], dist: HashShard(t.a) }
            └─BatchScan { table: t, columns: [t.a], distribution: SomeShard }
- name: group by column not found
  sql: |
    select 4 + 5 as a group by b;
  binder_error: |
    Failed to bind expression: b

    Caused by:
      Item not found: Invalid column: b
- sql: |
    create table t(v1 int, v2 int);
    select count(v1 + v2) as cnt, sum(v1 + v2) as sum from t;
  batch_plan: |-
    BatchSimpleAgg { aggs: [sum0(count($expr1)), sum(sum($expr1))] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchSimpleAgg { aggs: [count($expr1), sum($expr1)] }
        └─BatchProject { exprs: [(t.v1 + t.v2) as $expr1] }
          └─BatchScan { table: t, columns: [t.v1, t.v2], distribution: SomeShard }
  batch_local_plan: |-
    BatchSimpleAgg { aggs: [count($expr1), sum($expr1)] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchProject { exprs: [(t.v1 + t.v2) as $expr1] }
        └─BatchScan { table: t, columns: [t.v1, t.v2], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [cnt, sum], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum0(count($expr1)), sum(sum($expr1))] }
      └─StreamSimpleAgg { aggs: [sum0(count($expr1)), sum(sum($expr1)), count] }
        └─StreamExchange { dist: Single }
          └─StreamStatelessSimpleAgg { aggs: [count($expr1), sum($expr1)] }
            └─StreamProject { exprs: [(t.v1 + t.v2) as $expr1, t._row_id] }
              └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t(v1 int, v2 int, v3 int);
    select v1, sum(v2 + v3) / count(v2 + v3) + max(v1) as agg from t group by v1;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.v1, ((sum($expr1) / count($expr1)) + max(t.v1)) as $expr2] }
      └─BatchHashAgg { group_key: [t.v1], aggs: [sum($expr1), count($expr1), max(t.v1)] }
        └─BatchExchange { order: [], dist: HashShard(t.v1) }
          └─BatchProject { exprs: [t.v1, (t.v2 + t.v3) as $expr1] }
            └─BatchScan { table: t, columns: [t.v1, t.v2, t.v3], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [v1, agg], stream_key: [v1], pk_columns: [v1], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.v1, ((sum($expr1) / count($expr1)) + max(t.v1)) as $expr2] }
      └─StreamHashAgg { group_key: [t.v1], aggs: [sum($expr1), count($expr1), max(t.v1), count] }
        └─StreamExchange { dist: HashShard(t.v1) }
          └─StreamProject { exprs: [t.v1, (t.v2 + t.v3) as $expr1, t._row_id] }
            └─StreamTableScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t (v1 real);
    select v1, count(*) from t group by v1;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchHashAgg { group_key: [t.v1], aggs: [count] }
      └─BatchExchange { order: [], dist: HashShard(t.v1) }
        └─BatchScan { table: t, columns: [t.v1], distribution: SomeShard }
- name: Use BatchSortAgg, when input provides order
  sql: |
    create table t(v1 int, v2 int);
    create materialized view mv as select * from t order by v1 desc;
    select v1, max(v2) from mv group by v1;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchSortAgg { group_key: [mv.v1], aggs: [max(mv.v2)] }
      └─BatchScan { table: mv, columns: [mv.v1, mv.v2], distribution: UpstreamHashShard(mv.v1) }
- sql: |
    create table t(v1 int, v2 int);
    select v1, max(v2) from t group by v1 order by v1 desc;
  batch_plan: |-
    BatchExchange { order: [t.v1 DESC], dist: Single }
    └─BatchSort { order: [t.v1 DESC] }
      └─BatchHashAgg { group_key: [t.v1], aggs: [max(t.v2)] }
        └─BatchExchange { order: [], dist: HashShard(t.v1) }
          └─BatchScan { table: t, columns: [t.v1, t.v2], distribution: SomeShard }
- sql: |
    create table t(k1 int, k2 int, v1 int);
    SELECT max(v1), k1, k2 from t group by k1, k2 order by k1;
  batch_plan: |-
    BatchExchange { order: [t.k1 ASC], dist: Single }
    └─BatchProject { exprs: [max(t.v1), t.k1, t.k2] }
      └─BatchSort { order: [t.k1 ASC] }
        └─BatchHashAgg { group_key: [t.k1, t.k2], aggs: [max(t.v1)] }
          └─BatchExchange { order: [], dist: HashShard(t.k1, t.k2) }
            └─BatchScan { table: t, columns: [t.k1, t.k2, t.v1], distribution: SomeShard }
- sql: |
    create table t(v1 int, v2 int);
    select max(v2), v1 from t group by v1 order by v1 desc;
  batch_plan: |-
    BatchExchange { order: [t.v1 DESC], dist: Single }
    └─BatchProject { exprs: [max(t.v2), t.v1] }
      └─BatchSort { order: [t.v1 DESC] }
        └─BatchHashAgg { group_key: [t.v1], aggs: [max(t.v2)] }
          └─BatchExchange { order: [], dist: HashShard(t.v1) }
            └─BatchScan { table: t, columns: [t.v1, t.v2], distribution: SomeShard }
- sql: |
    create table t (a int, b int, c int, primary key (a, b, c));
    select a, c, first_value(b order by b), count(*) from t group by a, c;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchHashAgg { group_key: [t.a, t.c], aggs: [first_value(t.b order_by(t.b ASC)), count] }
      └─BatchExchange { order: [], dist: HashShard(t.a, t.c) }
        └─BatchScan { table: t, columns: [t.a, t.b, t.c], distribution: UpstreamHashShard(t.a, t.b, t.c) }
- sql: |
    create table t (a int, b int, c int, primary key (a, b, c));
    select a, c, first_value(b order by b), count(*) from t group by a, c having a = 1;;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchHashAgg { group_key: [t.a, t.c], aggs: [first_value(t.b order_by(t.b ASC)), count] }
      └─BatchExchange { order: [], dist: HashShard(t.a, t.c) }
        └─BatchScan { table: t, columns: [t.a, t.b, t.c], scan_ranges: [t.a = Int32(1)], distribution: UpstreamHashShard(t.a, t.b, t.c) }
- name: Not use BatchSortAgg, when input provides order
  sql: |
    create table t(v1 int, v2 int);
    create materialized view mv as select * from t order by v1 desc;
    select v1, max(v2) from mv group by v1;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchHashAgg { group_key: [mv.v1], aggs: [max(mv.v2)] }
      └─BatchScan { table: mv, columns: [mv.v1, mv.v2], distribution: UpstreamHashShard(mv.v1) }
  with_config_map:
    RW_BATCH_ENABLE_SORT_AGG: 'false'
- name: Not use BatchSortAgg, when output requires order
  sql: |
    create table t(v1 int, v2 int);
    select v1, max(v2) from t group by v1 order by v1 desc;
  batch_plan: |-
    BatchExchange { order: [t.v1 DESC], dist: Single }
    └─BatchSort { order: [t.v1 DESC] }
      └─BatchHashAgg { group_key: [t.v1], aggs: [max(t.v2)] }
        └─BatchExchange { order: [], dist: HashShard(t.v1) }
          └─BatchScan { table: t, columns: [t.v1, t.v2], distribution: SomeShard }
  with_config_map:
    RW_BATCH_ENABLE_SORT_AGG: 'false'
- name: Not use BatchSortAgg, when required order satisfies input order
  sql: |
    create table t(k1 int, k2 int, v1 int);
    SELECT max(v1), k1, k2 from t group by k1, k2 order by k1;
  batch_plan: |-
    BatchExchange { order: [t.k1 ASC], dist: Single }
    └─BatchProject { exprs: [max(t.v1), t.k1, t.k2] }
      └─BatchSort { order: [t.k1 ASC] }
        └─BatchHashAgg { group_key: [t.k1, t.k2], aggs: [max(t.v1)] }
          └─BatchExchange { order: [], dist: HashShard(t.k1, t.k2) }
            └─BatchScan { table: t, columns: [t.k1, t.k2, t.v1], distribution: SomeShard }
  with_config_map:
    RW_BATCH_ENABLE_SORT_AGG: 'false'
- name: Not use BatchSortAgg, when output requires order with swapped output
  sql: |
    create table t(v1 int, v2 int);
    select max(v2), v1 from t group by v1 order by v1 desc;
  batch_plan: |-
    BatchExchange { order: [t.v1 DESC], dist: Single }
    └─BatchProject { exprs: [max(t.v2), t.v1] }
      └─BatchSort { order: [t.v1 DESC] }
        └─BatchHashAgg { group_key: [t.v1], aggs: [max(t.v2)] }
          └─BatchExchange { order: [], dist: HashShard(t.v1) }
            └─BatchScan { table: t, columns: [t.v1, t.v2], distribution: SomeShard }
  with_config_map:
    RW_BATCH_ENABLE_SORT_AGG: 'false'
- sql: |
    create table t (v1 real);
    select count(*) from t;
  batch_plan: |-
    BatchSimpleAgg { aggs: [sum0(count)] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchSimpleAgg { aggs: [count] }
        └─BatchScan { table: t, columns: [], distribution: SomeShard }
- name: having with agg call
  sql: |
    create table t (v1 real);
    select 1 from t having sum(v1) > 5;
  batch_plan: |-
    BatchProject { exprs: [1:Int32] }
    └─BatchFilter { predicate: (sum(sum(t.v1)) > 5:Float64) }
      └─BatchSimpleAgg { aggs: [sum(sum(t.v1))] }
        └─BatchExchange { order: [], dist: Single }
          └─BatchSimpleAgg { aggs: [sum(t.v1)] }
            └─BatchScan { table: t, columns: [t.v1], distribution: SomeShard }
- name: having with group column
  sql: |
    create table t (v1 real);
    select 1 from t group by v1 having v1 > 5;
  logical_plan: |-
    LogicalProject { exprs: [1:Int32] }
    └─LogicalFilter { predicate: (t.v1 > 5:Int32::Float64) }
      └─LogicalAgg { group_key: [t.v1], aggs: [] }
        └─LogicalProject { exprs: [t.v1] }
          └─LogicalScan { table: t, columns: [t.v1, t._row_id, t._rw_timestamp] }
- name: having with non-group column
  sql: |
    create table t (v1 real, v2 int);
    select 1 from t group by v1 having v2 > 5;
  planner_error: 'Invalid input syntax: column must appear in the GROUP BY clause or be used in an aggregate function'
- name: distinct without agg
  sql: |
    create table t (v1 int, v2 int);
    select distinct v1 from t;
  logical_plan: |-
    LogicalAgg { group_key: [t.v1], aggs: [] }
    └─LogicalProject { exprs: [t.v1] }
      └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
  stream_plan: |-
    StreamMaterialize { columns: [v1], stream_key: [v1], pk_columns: [v1], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.v1], noop_update_hint: true }
      └─StreamHashAgg { group_key: [t.v1], aggs: [count] }
        └─StreamExchange { dist: HashShard(t.v1) }
          └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: distinct with agg
  sql: |
    create table t (v1 int, v2 int);
    select distinct sum(v1) from t group by v2;
  logical_plan: |-
    LogicalAgg { group_key: [sum(t.v1)], aggs: [] }
    └─LogicalProject { exprs: [sum(t.v1)] }
      └─LogicalAgg { group_key: [t.v2], aggs: [sum(t.v1)] }
        └─LogicalProject { exprs: [t.v2, t.v1] }
          └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
  stream_plan: |-
    StreamMaterialize { columns: [sum], stream_key: [sum], pk_columns: [sum], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum(t.v1)], noop_update_hint: true }
      └─StreamHashAgg { group_key: [sum(t.v1)], aggs: [count] }
        └─StreamExchange { dist: HashShard(sum(t.v1)) }
          └─StreamProject { exprs: [t.v2, sum(t.v1)] }
            └─StreamHashAgg { group_key: [t.v2], aggs: [sum(t.v1), count] }
              └─StreamExchange { dist: HashShard(t.v2) }
                └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: distinct on
  sql: |
    create table t (v1 int, v2 int, v3 int);
    select distinct on (v1, v3) v1, v2 from t order by v3, v1;
  logical_plan: |-
    LogicalProject { exprs: [t.v1, t.v2] }
    └─LogicalTopN { order: [t.v3 ASC, t.v1 ASC], limit: 1, offset: 0, group_key: [t.v1, t.v3] }
      └─LogicalProject { exprs: [t.v1, t.v2, t.v3] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchProject { exprs: [t.v1, t.v2] }
    └─BatchExchange { order: [t.v3 ASC, t.v1 ASC], dist: Single }
      └─BatchSort { order: [t.v3 ASC, t.v1 ASC] }
        └─BatchGroupTopN { order: [t.v3 ASC, t.v1 ASC], limit: 1, offset: 0, group_key: [t.v1, t.v3] }
          └─BatchExchange { order: [], dist: HashShard(t.v1, t.v3) }
            └─BatchScan { table: t, columns: [t.v1, t.v2, t.v3], distribution: SomeShard }
- name: distinct on order by distinct columns
  sql: |
    create table t (v1 int, v2 int, v3 int);
    select distinct on (v1, v3) v1, v2 from t order by v1, v3;
  logical_plan: |-
    LogicalProject { exprs: [t.v1, t.v2] }
    └─LogicalTopN { order: [t.v1 ASC, t.v3 ASC], limit: 1, offset: 0, group_key: [t.v1, t.v3] }
      └─LogicalProject { exprs: [t.v1, t.v2, t.v3] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchProject { exprs: [t.v1, t.v2] }
    └─BatchExchange { order: [t.v1 ASC, t.v3 ASC], dist: Single }
      └─BatchSort { order: [t.v1 ASC, t.v3 ASC] }
        └─BatchGroupTopN { order: [t.v1 ASC, t.v3 ASC], limit: 1, offset: 0, group_key: [t.v1, t.v3] }
          └─BatchExchange { order: [], dist: HashShard(t.v1, t.v3) }
            └─BatchScan { table: t, columns: [t.v1, t.v2, t.v3], distribution: SomeShard }
- name: distinct on order by part of the distinct columns
  sql: |
    create table t (v1 int, v2 int);
    select distinct on (v1) v1, v2 from t order by v1;
  logical_plan: |-
    LogicalTopN { order: [t.v1 ASC], limit: 1, offset: 0, group_key: [t.v1] }
    └─LogicalProject { exprs: [t.v1, t.v2] }
      └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
- name: distinct on order by distinct columns and additional sorting columns
  sql: |
    create table t (id int, ts timestamp, foo int, bar varchar);
    select distinct on (id) * from t order by id, ts desc;
  logical_plan: |-
    LogicalTopN { order: [t.id ASC, t.ts DESC], limit: 1, offset: 0, group_key: [t.id] }
    └─LogicalProject { exprs: [t.id, t.ts, t.foo, t.bar] }
      └─LogicalScan { table: t, columns: [t.id, t.ts, t.foo, t.bar, t._row_id, t._rw_timestamp] }
- name: distinct on with expression
  sql: |
    create table t (v1 int, v2 int, v3 int);
    select distinct on(v1) v2 + v3 from t order by v1;
  logical_plan: |-
    LogicalProject { exprs: [$expr1] }
    └─LogicalTopN { order: [t.v1 ASC], limit: 1, offset: 0, group_key: [t.v1] }
      └─LogicalProject { exprs: [(t.v2 + t.v3) as $expr1, t.v1] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchProject { exprs: [$expr1] }
    └─BatchExchange { order: [t.v1 ASC], dist: Single }
      └─BatchSort { order: [t.v1 ASC] }
        └─BatchGroupTopN { order: [t.v1 ASC], limit: 1, offset: 0, group_key: [t.v1] }
          └─BatchExchange { order: [], dist: HashShard(t.v1) }
            └─BatchProject { exprs: [(t.v2 + t.v3) as $expr1, t.v1] }
              └─BatchScan { table: t, columns: [t.v1, t.v2, t.v3], distribution: SomeShard }
- name: arguments out-of-order
  sql: |
    create table t(v1 int, v2 int, v3 int);
    select count(v3), min(v2), max(v1) from t;
  logical_plan: |-
    LogicalProject { exprs: [count(t.v3), min(t.v2), max(t.v1)] }
    └─LogicalAgg { aggs: [count(t.v3), min(t.v2), max(t.v1)] }
      └─LogicalProject { exprs: [t.v3, t.v2, t.v1] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalAgg { aggs: [count(t.v3), min(t.v2), max(t.v1)] }
    └─LogicalScan { table: t, columns: [t.v1, t.v2, t.v3] }
  batch_plan: |-
    BatchSimpleAgg { aggs: [sum0(count(t.v3)), min(min(t.v2)), max(max(t.v1))] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchSimpleAgg { aggs: [count(t.v3), min(t.v2), max(t.v1)] }
        └─BatchScan { table: t, columns: [t.v1, t.v2, t.v3], distribution: SomeShard }
- name: simple-agg arguments out-of-order
  sql: |
    create table t(v1 int, v2 int, v3 int);
    select min(v1) + max(v3) * count(v2) as agg from t;
  logical_plan: |-
    LogicalProject { exprs: [(min(t.v1) + (max(t.v3) * count(t.v2))) as $expr1] }
    └─LogicalAgg { aggs: [min(t.v1), max(t.v3), count(t.v2)] }
      └─LogicalProject { exprs: [t.v1, t.v3, t.v2] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalProject { exprs: [(min(t.v1) + (max(t.v3) * count(t.v2))) as $expr1] }
    └─LogicalAgg { aggs: [min(t.v1), max(t.v3), count(t.v2)] }
      └─LogicalScan { table: t, columns: [t.v1, t.v2, t.v3] }
  batch_plan: |-
    BatchProject { exprs: [(min(min(t.v1)) + (max(max(t.v3)) * sum0(count(t.v2)))) as $expr1] }
    └─BatchSimpleAgg { aggs: [min(min(t.v1)), max(max(t.v3)), sum0(count(t.v2))] }
      └─BatchExchange { order: [], dist: Single }
        └─BatchSimpleAgg { aggs: [min(t.v1), max(t.v3), count(t.v2)] }
          └─BatchScan { table: t, columns: [t.v1, t.v2, t.v3], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [agg], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [(min(min(t.v1)) + (max(max(t.v3)) * sum0(count(t.v2)))) as $expr1] }
      └─StreamSimpleAgg { aggs: [min(min(t.v1)), max(max(t.v3)), sum0(count(t.v2)), count] }
        └─StreamExchange { dist: Single }
          └─StreamHashAgg { group_key: [_vnode], aggs: [min(t.v1), max(t.v3), count(t.v2), count] }
            └─StreamProject { exprs: [t.v1, t.v2, t.v3, t._row_id, Vnode(t._row_id) as _vnode] }
              └─StreamTableScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: dup group key
  sql: |
    create table t(v1 int) with (appendonly = false);
    select v1 from t group by v1, v1;
  logical_plan: |-
    LogicalProject { exprs: [t.v1] }
    └─LogicalAgg { group_key: [t.v1], aggs: [] }
      └─LogicalProject { exprs: [t.v1] }
        └─LogicalScan { table: t, columns: [t.v1, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalAgg { group_key: [t.v1], aggs: [] }
    └─LogicalScan { table: t, columns: [t.v1] }
  stream_plan: |-
    StreamMaterialize { columns: [v1], stream_key: [v1], pk_columns: [v1], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.v1], noop_update_hint: true }
      └─StreamHashAgg { group_key: [t.v1], aggs: [count] }
        └─StreamExchange { dist: HashShard(t.v1) }
          └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: dup group key
  sql: |
    create table t(v1 int, v2 int, v3 int) with (appendonly = false);
    select v2, min(v1) as min_v1, v3, max(v1) as max_v1 from t group by v3, v2, v2;
  logical_plan: |-
    LogicalProject { exprs: [t.v2, min(t.v1), t.v3, max(t.v1)] }
    └─LogicalAgg { group_key: [t.v3, t.v2], aggs: [min(t.v1), max(t.v1)] }
      └─LogicalProject { exprs: [t.v3, t.v2, t.v1] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalProject { exprs: [t.v2, min(t.v1), t.v3, max(t.v1)] }
    └─LogicalAgg { group_key: [t.v2, t.v3], aggs: [min(t.v1), max(t.v1)] }
      └─LogicalScan { table: t, columns: [t.v1, t.v2, t.v3] }
  stream_plan: |-
    StreamMaterialize { columns: [v2, min_v1, v3, max_v1], stream_key: [v2, v3], pk_columns: [v2, v3], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.v2, min(t.v1), t.v3, max(t.v1)] }
      └─StreamHashAgg { group_key: [t.v2, t.v3], aggs: [min(t.v1), max(t.v1), count] }
        └─StreamExchange { dist: HashShard(t.v2, t.v3) }
          └─StreamTableScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: order by agg input
  sql: |
    create table t(v1 int);
    select sum(v1 order by v1) as s1 from t;
  logical_plan: |-
    LogicalProject { exprs: [sum(t.v1)] }
    └─LogicalAgg { aggs: [sum(t.v1)] }
      └─LogicalProject { exprs: [t.v1] }
        └─LogicalScan { table: t, columns: [t.v1, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalAgg { aggs: [sum(t.v1)] }
    └─LogicalScan { table: t, columns: [t.v1] }
  stream_plan: |-
    StreamMaterialize { columns: [s1], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum(sum(t.v1))] }
      └─StreamSimpleAgg { aggs: [sum(sum(t.v1)), count] }
        └─StreamExchange { dist: Single }
          └─StreamStatelessSimpleAgg { aggs: [sum(t.v1)] }
            └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: order by other columns
  sql: |
    create table t(v1 int, v2 varchar);
    select sum(v1 order by v2) as s1 from t;
  logical_plan: |-
    LogicalProject { exprs: [sum(t.v1)] }
    └─LogicalAgg { aggs: [sum(t.v1)] }
      └─LogicalProject { exprs: [t.v1] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalAgg { aggs: [sum(t.v1)] }
    └─LogicalScan { table: t, columns: [t.v1] }
  stream_plan: |-
    StreamMaterialize { columns: [s1], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum(sum(t.v1))] }
      └─StreamSimpleAgg { aggs: [sum(sum(t.v1)), count] }
        └─StreamExchange { dist: Single }
          └─StreamStatelessSimpleAgg { aggs: [sum(t.v1)] }
            └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: order by ASC/DESC and default
  sql: |
    create table t(v1 int, v2 varchar, v3 int);
    select sum(v1 order by v1, v2 ASC, v3 DESC) as s1 from t;
  logical_plan: |-
    LogicalProject { exprs: [sum(t.v1)] }
    └─LogicalAgg { aggs: [sum(t.v1)] }
      └─LogicalProject { exprs: [t.v1] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalAgg { aggs: [sum(t.v1)] }
    └─LogicalScan { table: t, columns: [t.v1] }
  stream_plan: |-
    StreamMaterialize { columns: [s1], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum(sum(t.v1))] }
      └─StreamSimpleAgg { aggs: [sum(sum(t.v1)), count] }
        └─StreamExchange { dist: Single }
          └─StreamStatelessSimpleAgg { aggs: [sum(t.v1)] }
            └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: filter clause
  sql: |
    create table t(v1 int);
    select sum(v1) FILTER (WHERE v1 > 0) AS sa from t;
  logical_plan: |-
    LogicalProject { exprs: [sum(t.v1) filter((t.v1 > 0:Int32))] }
    └─LogicalAgg { aggs: [sum(t.v1) filter((t.v1 > 0:Int32))] }
      └─LogicalProject { exprs: [t.v1] }
        └─LogicalScan { table: t, columns: [t.v1, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalAgg { aggs: [sum(t.v1) filter((t.v1 > 0:Int32))] }
    └─LogicalScan { table: t, columns: [t.v1] }
  stream_plan: |-
    StreamMaterialize { columns: [sa], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum(sum(t.v1) filter((t.v1 > 0:Int32)))] }
      └─StreamSimpleAgg { aggs: [sum(sum(t.v1) filter((t.v1 > 0:Int32))), count] }
        └─StreamExchange { dist: Single }
          └─StreamStatelessSimpleAgg { aggs: [sum(t.v1) filter((t.v1 > 0:Int32))] }
            └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: |
    filter clause
    extra calculation, should reuse result from project
  sql: |
    create table t(a int, b int);
    select sum(a * b) filter (where a * b > 0) as sab from t;
  logical_plan: |-
    LogicalProject { exprs: [sum($expr1) filter(((t.a * t.b) > 0:Int32))] }
    └─LogicalAgg { aggs: [sum($expr1) filter(((t.a * t.b) > 0:Int32))] }
      └─LogicalProject { exprs: [t.a, t.b, (t.a * t.b) as $expr1] }
        └─LogicalScan { table: t, columns: [t.a, t.b, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalAgg { aggs: [sum($expr1) filter(((t.a * t.b) > 0:Int32))] }
    └─LogicalProject { exprs: [t.a, t.b, (t.a * t.b) as $expr1] }
      └─LogicalScan { table: t, columns: [t.a, t.b] }
- name: complex filter clause
  sql: |
    create table t(a int, b int);
    select max(a * b) FILTER (WHERE a < b AND a + b < 100 AND a * b != a + b - 1) AS sab from t;
  logical_plan: |-
    LogicalProject { exprs: [max($expr1) filter((t.a < t.b) AND ((t.a + t.b) < 100:Int32) AND ((t.a * t.b) <> ((t.a + t.b) - 1:Int32)))] }
    └─LogicalAgg { aggs: [max($expr1) filter((t.a < t.b) AND ((t.a + t.b) < 100:Int32) AND ((t.a * t.b) <> ((t.a + t.b) - 1:Int32)))] }
      └─LogicalProject { exprs: [t.a, t.b, (t.a * t.b) as $expr1] }
        └─LogicalScan { table: t, columns: [t.a, t.b, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalAgg { aggs: [max($expr1) filter((t.a < t.b) AND ((t.a + t.b) < 100:Int32) AND ((t.a * t.b) <> ((t.a + t.b) - 1:Int32)))] }
    └─LogicalProject { exprs: [t.a, t.b, (t.a * t.b) as $expr1] }
      └─LogicalScan { table: t, columns: [t.a, t.b] }
  stream_plan: |-
    StreamMaterialize { columns: [sab], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [max(max($expr1) filter((t.a < t.b) AND ((t.a + t.b) < 100:Int32) AND ((t.a * t.b) <> ((t.a + t.b) - 1:Int32))))] }
      └─StreamSimpleAgg { aggs: [max(max($expr1) filter((t.a < t.b) AND ((t.a + t.b) < 100:Int32) AND ((t.a * t.b) <> ((t.a + t.b) - 1:Int32)))), count] }
        └─StreamExchange { dist: Single }
          └─StreamHashAgg { group_key: [$expr2], aggs: [max($expr1) filter((t.a < t.b) AND ((t.a + t.b) < 100:Int32) AND ((t.a * t.b) <> ((t.a + t.b) - 1:Int32))), count] }
            └─StreamProject { exprs: [t.a, t.b, (t.a * t.b) as $expr1, t._row_id, Vnode(t._row_id) as $expr2] }
              └─StreamTableScan { table: t, columns: [t.a, t.b, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: avg filter clause + group by
  sql: |
    create table t(a int, b int);
    select avg(a) FILTER (WHERE a > b) AS avga from t group by b ;
  logical_plan: |-
    LogicalProject { exprs: [(sum(t.a) filter((t.a > t.b))::Decimal / count(t.a) filter((t.a > t.b))::Decimal) as $expr1] }
    └─LogicalAgg { group_key: [t.b], aggs: [sum(t.a) filter((t.a > t.b)), count(t.a) filter((t.a > t.b))] }
      └─LogicalProject { exprs: [t.b, t.a] }
        └─LogicalScan { table: t, columns: [t.a, t.b, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalProject { exprs: [(sum(t.a) filter((t.a > t.b))::Decimal / count(t.a) filter((t.a > t.b))::Decimal) as $expr1] }
    └─LogicalAgg { group_key: [t.b], aggs: [sum(t.a) filter((t.a > t.b)), count(t.a) filter((t.a > t.b))] }
      └─LogicalScan { table: t, columns: [t.a, t.b] }
  stream_plan: |-
    StreamMaterialize { columns: [avga, t.b(hidden)], stream_key: [t.b], pk_columns: [t.b], pk_conflict: NoCheck }
    └─StreamProject { exprs: [(sum(t.a) filter((t.a > t.b))::Decimal / count(t.a) filter((t.a > t.b))::Decimal) as $expr1, t.b] }
      └─StreamHashAgg { group_key: [t.b], aggs: [sum(t.a) filter((t.a > t.b)), count(t.a) filter((t.a > t.b)), count] }
        └─StreamExchange { dist: HashShard(t.b) }
          └─StreamTableScan { table: t, columns: [t.a, t.b, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: count filter clause
  sql: |
    create table t(a int, b int);
    select count(*) FILTER (WHERE a > b) AS cnt_agb from t;
  logical_plan: |-
    LogicalProject { exprs: [count filter((t.a > t.b))] }
    └─LogicalAgg { aggs: [count filter((t.a > t.b))] }
      └─LogicalProject { exprs: [t.a, t.b] }
        └─LogicalScan { table: t, columns: [t.a, t.b, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalAgg { aggs: [count filter((t.a > t.b))] }
    └─LogicalScan { table: t, columns: [t.a, t.b] }
  stream_plan: |-
    StreamMaterialize { columns: [cnt_agb], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum0(count filter((t.a > t.b)))] }
      └─StreamSimpleAgg { aggs: [sum0(count filter((t.a > t.b))), count] }
        └─StreamExchange { dist: Single }
          └─StreamStatelessSimpleAgg { aggs: [count filter((t.a > t.b))] }
            └─StreamTableScan { table: t, columns: [t.a, t.b, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: filter clause + non-boolean function
  sql: |
    create table t(a int, b int);
    select avg(a) FILTER (WHERE abs(a)) AS avga from t;
  binder_error: |
    Failed to bind expression: avg(a) FILTER (WHERE abs(a))

    Caused by:
      argument of FILTER must be boolean, not type Int32
- name: filter clause + subquery
  sql: |
    create table t(a int, b int);
    select avg(a) FILTER (WHERE 0 < (select max(a) from t)) AS avga from t;
  binder_error: |
    Failed to bind expression: avg(a) FILTER (WHERE 0 < (SELECT max(a) FROM t))

    Caused by:
      Feature is not yet implemented: subquery in filter clause
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- name: aggregation in filter clause
  sql: |
    create table t(a int, b int);
    select avg(a) FILTER (WHERE a < avg(b)) AS avga from t;
  binder_error: |
    Failed to bind expression: avg(a) FILTER (WHERE a < avg(b))

    Caused by:
      Feature is not yet implemented: aggregation function in filter clause
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- name: filter clause + non-boolean function
  sql: |
    create table t(a int, b int);
    select abs(a) FILTER (WHERE a > 0) AS avga from t;
  binder_error: |
    Failed to bind expression: abs(a) FILTER (WHERE a > 0)

    Caused by:
      Invalid input syntax: `FILTER` is not allowed in scalar/table function call
- name: prune column before filter
  sql: |
    create table t(v1 int, v2 int);
    with sub(a, b) as (select min(v1), sum(v2) filter (where v2 < 5) from t) select b from sub;
  batch_plan: |-
    BatchSimpleAgg { aggs: [sum(sum(t.v2) filter((t.v2 < 5:Int32)))] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchSimpleAgg { aggs: [sum(t.v2) filter((t.v2 < 5:Int32))] }
        └─BatchScan { table: t, columns: [t.v2], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [b], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum(sum(t.v2) filter((t.v2 < 5:Int32)))] }
      └─StreamSimpleAgg { aggs: [sum(sum(t.v2) filter((t.v2 < 5:Int32))), count] }
        └─StreamExchange { dist: Single }
          └─StreamStatelessSimpleAgg { aggs: [sum(t.v2) filter((t.v2 < 5:Int32))] }
            └─StreamTableScan { table: t, columns: [t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: distinct agg on empty args
  sql: |
    create table t(x int, y varchar);
    select count(distinct *) from t;
  binder_error: |
    Failed to bind expression: count(DISTINCT *)

    Caused by:
      Invalid input syntax: DISTINCT is not allowed for aggregate function `count` without args
- name: only distinct agg
  sql: |
    create table t(a int, b int, c int);
    select a, count(distinct b) as distinct_b_num, sum(distinct c) filter(where c < 100) as distinct_c_sum from t group by a;
  optimized_logical_plan_for_batch: |-
    LogicalAgg { group_key: [t.a_expanded], aggs: [count(t.b_expanded) filter((flag = 0:Int64)), sum(t.c_expanded) filter((count filter((t.c < 100:Int32)) > 0:Int64) AND (flag = 1:Int64))] }
    └─LogicalAgg { group_key: [t.a_expanded, t.b_expanded, t.c_expanded, flag], aggs: [count filter((t.c < 100:Int32))] }
      └─LogicalExpand { column_subsets: [[t.a, t.b], [t.a, t.c]] }
        └─LogicalScan { table: t, columns: [t.a, t.b, t.c] }
- name: single distinct agg and non-disintct agg
  sql: |
    create table t(a int, b int, c int);
    select a, count(distinct b) as distinct_b_num, sum(c) as sum_c from t group by a;
  optimized_logical_plan_for_batch: |-
    LogicalAgg { group_key: [t.a], aggs: [count(t.b), sum(sum(t.c))] }
    └─LogicalAgg { group_key: [t.a, t.b], aggs: [sum(t.c)] }
      └─LogicalScan { table: t, columns: [t.a, t.b, t.c] }
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchHashAgg { group_key: [t.a], aggs: [count(t.b), sum(sum(t.c))] }
      └─BatchExchange { order: [], dist: HashShard(t.a) }
        └─BatchHashAgg { group_key: [t.a, t.b], aggs: [sum(t.c)] }
          └─BatchExchange { order: [], dist: HashShard(t.a, t.b) }
            └─BatchScan { table: t, columns: [t.a, t.b, t.c], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [a, distinct_b_num, sum_c], stream_key: [a], pk_columns: [a], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.a, count(distinct t.b), sum(t.c)] }
      └─StreamHashAgg { group_key: [t.a], aggs: [count(distinct t.b), sum(t.c), count] }
        └─StreamExchange { dist: HashShard(t.a) }
          └─StreamTableScan { table: t, columns: [t.a, t.b, t.c, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: distinct agg and non-disintct agg with intersected argument
  sql: |
    create table t(a int, b int, c int);
    select a, count(distinct b) as distinct_b_num, count(distinct c) as distinct_c_sum, sum(c) as sum_c from t group by a;
  optimized_logical_plan_for_batch: |-
    LogicalAgg { group_key: [t.a_expanded], aggs: [count(t.b_expanded) filter((flag = 1:Int64)), count(t.c_expanded) filter((flag = 0:Int64)), sum(sum(t.c_expanded)) filter((flag = 0:Int64))] }
    └─LogicalAgg { group_key: [t.a_expanded, t.b_expanded, t.c_expanded, flag], aggs: [sum(t.c_expanded)] }
      └─LogicalExpand { column_subsets: [[t.a, t.c], [t.a, t.b]] }
        └─LogicalScan { table: t, columns: [t.a, t.b, t.c] }
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchHashAgg { group_key: [t.a_expanded], aggs: [count(t.b_expanded) filter((flag = 1:Int64)), count(t.c_expanded) filter((flag = 0:Int64)), sum(sum(t.c_expanded)) filter((flag = 0:Int64))] }
      └─BatchExchange { order: [], dist: HashShard(t.a_expanded) }
        └─BatchHashAgg { group_key: [t.a_expanded, t.b_expanded, t.c_expanded, flag], aggs: [sum(t.c_expanded)] }
          └─BatchExchange { order: [], dist: HashShard(t.a_expanded, t.b_expanded, t.c_expanded, flag) }
            └─BatchExpand { column_subsets: [[t.a, t.c], [t.a, t.b]] }
              └─BatchScan { table: t, columns: [t.a, t.b, t.c], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [a, distinct_b_num, distinct_c_sum, sum_c], stream_key: [a], pk_columns: [a], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.a, count(distinct t.b), count(distinct t.c), sum(t.c)] }
      └─StreamHashAgg { group_key: [t.a], aggs: [count(distinct t.b), count(distinct t.c), sum(t.c), count] }
        └─StreamExchange { dist: HashShard(t.a) }
          └─StreamTableScan { table: t, columns: [t.a, t.b, t.c, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: distinct & string_agg
  sql: |
    create table t(x int, y varchar);
    select string_agg(y, ','), count(distinct x) from t;
  logical_plan: |-
    LogicalProject { exprs: [string_agg(t.y, ',':Varchar), count(distinct t.x)] }
    └─LogicalAgg { aggs: [string_agg(t.y, ',':Varchar), count(distinct t.x)] }
      └─LogicalProject { exprs: [t.y, ',':Varchar, t.x] }
        └─LogicalScan { table: t, columns: [t.x, t.y, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchSimpleAgg { aggs: [string_agg(t.y, ',':Varchar), count(distinct t.x)] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchProject { exprs: [t.y, ',':Varchar, t.x] }
        └─BatchScan { table: t, columns: [t.x, t.y], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [string_agg, count], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [string_agg(t.y, ',':Varchar), count(distinct t.x)] }
      └─StreamSimpleAgg { aggs: [string_agg(t.y, ',':Varchar), count(distinct t.x), count] }
        └─StreamExchange { dist: Single }
          └─StreamProject { exprs: [t.y, ',':Varchar, t.x, t._row_id] }
            └─StreamTableScan { table: t, columns: [t.x, t.y, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: distinct & order by on different calls
  sql: |
    create table t(x int, y varchar);
    select string_agg(y, ',' order by y), count(distinct x) from t;
  logical_plan: |-
    LogicalProject { exprs: [string_agg(t.y, ',':Varchar order_by(t.y ASC)), count(distinct t.x)] }
    └─LogicalAgg { aggs: [string_agg(t.y, ',':Varchar order_by(t.y ASC)), count(distinct t.x)] }
      └─LogicalProject { exprs: [t.y, ',':Varchar, t.x] }
        └─LogicalScan { table: t, columns: [t.x, t.y, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchSimpleAgg { aggs: [string_agg(t.y, ',':Varchar order_by(t.y ASC)), count(distinct t.x)] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchProject { exprs: [t.y, ',':Varchar, t.x] }
        └─BatchScan { table: t, columns: [t.x, t.y], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [string_agg, count], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [string_agg(t.y, ',':Varchar order_by(t.y ASC)), count(distinct t.x)] }
      └─StreamSimpleAgg { aggs: [string_agg(t.y, ',':Varchar order_by(t.y ASC)), count(distinct t.x), count] }
        └─StreamExchange { dist: Single }
          └─StreamProject { exprs: [t.y, ',':Varchar, t.x, t._row_id] }
            └─StreamTableScan { table: t, columns: [t.x, t.y, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: distinct & order by on same calls
  sql: |
    create table t(x int, y varchar);
    select string_agg(distinct y, ',' order by y), count(distinct x) from t;
  logical_plan: |-
    LogicalProject { exprs: [string_agg(distinct t.y, ',':Varchar order_by(t.y ASC)), count(distinct t.x)] }
    └─LogicalAgg { aggs: [string_agg(distinct t.y, ',':Varchar order_by(t.y ASC)), count(distinct t.x)] }
      └─LogicalProject { exprs: [t.y, ',':Varchar, t.x] }
        └─LogicalScan { table: t, columns: [t.x, t.y, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchSimpleAgg { aggs: [string_agg(distinct t.y, ',':Varchar order_by(t.y ASC)), count(distinct t.x)] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchProject { exprs: [t.y, ',':Varchar, t.x] }
        └─BatchScan { table: t, columns: [t.x, t.y], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [string_agg, count], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [string_agg(distinct t.y, ',':Varchar order_by(t.y ASC)), count(distinct t.x)] }
      └─StreamSimpleAgg { aggs: [string_agg(distinct t.y, ',':Varchar order_by(t.y ASC)), count(distinct t.x), count] }
        └─StreamExchange { dist: Single }
          └─StreamProject { exprs: [t.y, ',':Varchar, t.x, t._row_id] }
            └─StreamTableScan { table: t, columns: [t.x, t.y, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: distinct & invalid order by
  sql: |
    create table t(x int, y varchar);
    select string_agg(distinct y, ',' order by x) from t;
  binder_error: |
    Failed to bind expression: string_agg(DISTINCT y, ',' ORDER BY x)

    Caused by:
      Invalid input syntax: ORDER BY expressions must match regular arguments of the aggregate for `string_agg` when DISTINCT is provided
- name: distinct with multiple non-const args
  sql: |
    create table t(x int, y varchar);
    select string_agg(distinct y, x::varchar) from t;
  binder_error: |
    Failed to bind expression: string_agg(DISTINCT y, CAST(x AS CHARACTER VARYING))

    Caused by:
      Feature is not yet implemented: non-constant arguments other than the first one for DISTINCT aggregation is not supported now
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- name: distinct agg with filter
  sql: |
    create table t(a int, b int, c int);
    select a, count(distinct b) filter(where b < 100), sum(c) from t group by a;
  optimized_logical_plan_for_batch: |-
    LogicalAgg { group_key: [t.a], aggs: [count(t.b) filter((count filter((t.b < 100:Int32)) > 0:Int64)), sum(sum(t.c))] }
    └─LogicalAgg { group_key: [t.a, t.b], aggs: [count filter((t.b < 100:Int32)), sum(t.c)] }
      └─LogicalScan { table: t, columns: [t.a, t.b, t.c] }
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchHashAgg { group_key: [t.a], aggs: [count(t.b) filter((count filter((t.b < 100:Int32)) > 0:Int64)), sum(sum(t.c))] }
      └─BatchExchange { order: [], dist: HashShard(t.a) }
        └─BatchHashAgg { group_key: [t.a, t.b], aggs: [count filter((t.b < 100:Int32)), sum(t.c)] }
          └─BatchExchange { order: [], dist: HashShard(t.a, t.b) }
            └─BatchScan { table: t, columns: [t.a, t.b, t.c], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [a, count, sum], stream_key: [a], pk_columns: [a], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.a, count(distinct t.b) filter((t.b < 100:Int32)), sum(t.c)] }
      └─StreamHashAgg { group_key: [t.a], aggs: [count(distinct t.b) filter((t.b < 100:Int32)), sum(t.c), count] }
        └─StreamExchange { dist: HashShard(t.a) }
          └─StreamTableScan { table: t, columns: [t.a, t.b, t.c, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: non-distinct agg with filter
  sql: |
    create table t(a int, b int, c int);
    select a, count(distinct b), sum(c) filter(where b < 100) from t group by a;
  optimized_logical_plan_for_batch: |-
    LogicalAgg { group_key: [t.a], aggs: [count(t.b), sum(sum(t.c) filter((t.b < 100:Int32)))] }
    └─LogicalAgg { group_key: [t.a, t.b], aggs: [sum(t.c) filter((t.b < 100:Int32))] }
      └─LogicalScan { table: t, columns: [t.a, t.b, t.c] }
- name: combined order by & filter clauses
  sql: |
    create table t(a varchar, b int);
    select sum(length(a) * b order by length(a) + b) filter (where b < 100 AND b * 2 > 10) as s1 from t;
  logical_plan: |-
    LogicalProject { exprs: [sum($expr1) filter((t.b < 100:Int32) AND ((t.b * 2:Int32) > 10:Int32))] }
    └─LogicalAgg { aggs: [sum($expr1) filter((t.b < 100:Int32) AND ((t.b * 2:Int32) > 10:Int32))] }
      └─LogicalProject { exprs: [t.b, (Length(t.a) * t.b) as $expr1] }
        └─LogicalScan { table: t, columns: [t.a, t.b, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalAgg { aggs: [sum($expr1) filter((t.b < 100:Int32) AND ((t.b * 2:Int32) > 10:Int32))] }
    └─LogicalProject { exprs: [t.b, (Length(t.a) * t.b) as $expr1] }
      └─LogicalScan { table: t, columns: [t.a, t.b] }
  stream_plan: |-
    StreamMaterialize { columns: [s1], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum(sum($expr1) filter((t.b < 100:Int32) AND ((t.b * 2:Int32) > 10:Int32)))] }
      └─StreamSimpleAgg { aggs: [sum(sum($expr1) filter((t.b < 100:Int32) AND ((t.b * 2:Int32) > 10:Int32))), count] }
        └─StreamExchange { dist: Single }
          └─StreamStatelessSimpleAgg { aggs: [sum($expr1) filter((t.b < 100:Int32) AND ((t.b * 2:Int32) > 10:Int32))] }
            └─StreamProject { exprs: [t.b, (Length(t.a) * t.b) as $expr1, t._row_id] }
              └─StreamTableScan { table: t, columns: [t.a, t.b, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t(v1 int, v2 int);
    with z(a, b) as (select count(distinct v1), count(v2) from t) select a from z;
  optimized_logical_plan_for_batch: |-
    LogicalAgg { aggs: [count(t.v1)] }
    └─LogicalAgg { group_key: [t.v1], aggs: [] }
      └─LogicalScan { table: t, columns: [t.v1] }
- name: input is sharded by group key
  sql: |
    create table t(x int);
    create index i on t(x);
    select count(*) as cnt from i group by x;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [count] }
      └─BatchSortAgg { group_key: [i.x], aggs: [count] }
        └─BatchScan { table: i, columns: [i.x], distribution: UpstreamHashShard(i.x) }
  stream_plan: |-
    StreamMaterialize { columns: [cnt, i.x(hidden)], stream_key: [i.x], pk_columns: [i.x], pk_conflict: NoCheck }
    └─StreamProject { exprs: [count, i.x] }
      └─StreamHashAgg { group_key: [i.x], aggs: [count] }
        └─StreamTableScan { table: i, columns: [i.x, i.t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [i.t._row_id], pk: [x, t._row_id], dist: UpstreamHashShard(i.x) }
- name: distinct aggregates only have one distinct argument doesn't need expand
  sql: |
    create table t(x int, y int);
    select count(x), sum(distinct y), sum(distinct y) from t;
  optimized_logical_plan_for_batch: |-
    LogicalProject { exprs: [sum0(count(t.x)), sum(t.y), sum(t.y)] }
    └─LogicalAgg { aggs: [sum0(count(t.x)), sum(t.y)] }
      └─LogicalAgg { group_key: [t.y], aggs: [count(t.x)] }
        └─LogicalScan { table: t, columns: [t.x, t.y] }
  optimized_logical_plan_for_stream: |-
    LogicalProject { exprs: [sum0(count(t.x)), sum(t.y), sum(t.y)] }
    └─LogicalAgg { aggs: [sum0(count(t.x)), sum(t.y)] }
      └─LogicalAgg { group_key: [t.y], aggs: [count(t.x)] }
        └─LogicalScan { table: t, columns: [t.x, t.y] }
  with_config_map:
    RW_FORCE_SPLIT_DISTINCT_AGG: 'true'
- sql: |
    create table t(x int, y int);
    select count(y), sum(distinct y) from t;
  optimized_logical_plan_for_batch: |-
    LogicalAgg { aggs: [sum0(count(t.y)), sum(t.y)] }
    └─LogicalAgg { group_key: [t.y], aggs: [count(t.y)] }
      └─LogicalScan { table: t, columns: [t.y] }
  optimized_logical_plan_for_stream: |-
    LogicalAgg { aggs: [sum0(count(t.y)), sum(t.y)] }
    └─LogicalAgg { group_key: [t.y], aggs: [count(t.y)] }
      └─LogicalScan { table: t, columns: [t.y] }
  with_config_map:
    RW_FORCE_SPLIT_DISTINCT_AGG: 'true'
- sql: |
    create table t(x int, y int);
    select count(distinct x), sum(distinct y) from t;
  optimized_logical_plan_for_batch: |-
    LogicalAgg { aggs: [count(t.x_expanded) filter((flag = 0:Int64)), sum(t.y_expanded) filter((flag = 1:Int64))] }
    └─LogicalAgg { group_key: [t.x_expanded, t.y_expanded, flag], aggs: [] }
      └─LogicalExpand { column_subsets: [[t.x], [t.y]] }
        └─LogicalScan { table: t, columns: [t.x, t.y] }
  optimized_logical_plan_for_stream: |-
    LogicalAgg { aggs: [count(t.x_expanded) filter((flag = 0:Int64)), sum(t.y_expanded) filter((flag = 1:Int64))] }
    └─LogicalAgg { group_key: [t.x_expanded, t.y_expanded, flag], aggs: [] }
      └─LogicalExpand { column_subsets: [[t.x], [t.y]] }
        └─LogicalScan { table: t, columns: [t.x, t.y, t._row_id, t._rw_timestamp] }
  with_config_map:
    RW_FORCE_SPLIT_DISTINCT_AGG: 'true'
- name: remove unnecessary distinct for max and min
  sql: |
    create table t(x int, y int);
    select max(distinct x), min(distinct y) from t;
  optimized_logical_plan_for_batch: |-
    LogicalAgg { aggs: [max(t.x), min(t.y)] }
    └─LogicalScan { table: t, columns: [t.x, t.y] }
  optimized_logical_plan_for_stream: |-
    LogicalAgg { aggs: [max(t.x), min(t.y)] }
    └─LogicalScan { table: t, columns: [t.x, t.y] }
  with_config_map:
    RW_FORCE_SPLIT_DISTINCT_AGG: 'true'
- name: agg filter - subquery
  sql: |
    /* This case is valid in PostgreSQL */
    create table a (a1 int, a2 int);
    select count(a1) filter (where (select true)) from a;
  binder_error: |
    Failed to bind expression: count(a1) FILTER (WHERE (SELECT true))

    Caused by:
      Feature is not yet implemented: subquery in filter clause
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- name: agg filter - agg
  sql: |
    /* This case is valid in PostgreSQL */
    create table a (a1 int, a2 int);
    create table b (b1 int, b2 int);
    select 1 from a having exists(
      select count(b1) filter (where min(a1) < 3) from b
    );
  binder_error: |
    Failed to bind expression: EXISTS (SELECT count(b1) FILTER (WHERE min(a1) < 3) FROM b)

    Caused by these errors (recent errors listed first):
      1: Failed to bind expression: count(b1) FILTER (WHERE min(a1) < 3)
      2: Feature is not yet implemented: aggregation function in filter clause
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- name: agg filter - table function
  sql: |
    /* This case is NOT valid in PostgreSQL */
    create table a (a1 int, a2 int);
    select count(a1) filter (where unnest(array[1]) < 1) from a;
  binder_error: |
    Failed to bind expression: count(a1) FILTER (WHERE unnest(ARRAY[1]) < 1)

    Caused by:
      Invalid input syntax: table functions are not allowed in FILTER
- name: agg order by - subquery
  sql: |
    /* This case is valid in PostgreSQL */
    create table a (a1 int, a2 int);
    select string_agg('', '' order by (select true)) from a;
  planner_error: |-
    Feature is not yet implemented: subquery inside aggregation calls order by
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- name: agg order by - agg (correlated in having)
  sql: |
    create table a (a1 int, a2 int);
    create table sb (b1 varchar, b2 varchar);
    select 1 from a having exists(
      select string_agg(b1, '' order by min(a1)) from sb -- valid in PostgreSQL
      -- select string_agg('', '' order by min(a1)) from sb -- NOT valid in PostgreSQL
    );
  planner_error: |-
    Feature is not yet implemented: correlated subquery in HAVING or SELECT with agg
    Tracking issue: https://github.com/risingwavelabs/risingwave/issues/2275
- name: agg order by - agg (correlated in where)
  sql: |
    /* This case is NOT valid in PostgreSQL */
    create table a (a1 int, a2 int);
    create table sb (b1 varchar, b2 varchar);
    select 1 from a where exists(
      select string_agg(b1, '' order by min(a1)) from sb
    );
  planner_error: |-
    Feature is not yet implemented: aggregate function inside aggregation calls order by
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- name: agg order by - table function
  sql: |
    /* This case is NOT valid in PostgreSQL */
    create table a (a1 int, a2 int);
    select string_agg('', '' order by unnest(array[1])) from a;
  planner_error: |-
    Feature is not yet implemented: table function inside aggregation calls order by
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- name: agg input - subquery
  sql: |
    /* This case is valid in PostgreSQL */
    create table a (a1 int, a2 int);
    select count(a1 + (select 1)) from a;
  planner_error: |-
    Feature is not yet implemented: subquery inside aggregation calls
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- name: agg input - agg
  sql: |
    /* This case is valid in PostgreSQL */
    create table a (a1 int, a2 int);
    create table b (b1 int, b2 int);
    select 1 from a having exists(
      select count(b1 + min(a1)) from b
    );
  planner_error: |-
    Feature is not yet implemented: correlated subquery in HAVING or SELECT with agg
    Tracking issue: https://github.com/risingwavelabs/risingwave/issues/2275
- name: agg input - table function
  sql: |
    /* This case is NOT valid in PostgreSQL */
    create table a (a1 int, a2 int);
    select count(a1 + unnest(array[1])) from a;
  planner_error: |-
    Feature is not yet implemented: table function inside aggregation calls
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- name: group by - subquery
  sql: |
    /* This case is valid in PostgreSQL */
    create table a (a1 int, a2 int);
    select count(a1) from a group by (select true);
  planner_error: |-
    Feature is not yet implemented: subquery inside GROUP BY
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- name: group by - agg
  sql: |
    /* This case is valid in PostgreSQL */
    create table a (a1 int, a2 int);
    create table b (b1 int, b2 int);
    select 1 from a having exists(
      select count(b1) from b group by min(a1)
    );
  planner_error: |-
    Feature is not yet implemented: correlated subquery in HAVING or SELECT with agg
    Tracking issue: https://github.com/risingwavelabs/risingwave/issues/2275
- name: group by - table function
  sql: |
    /* This case is valid in PostgreSQL */
    create table a (a1 int, a2 int);
    select count(a1) from a group by unnest(array[1]);
  planner_error: |-
    Feature is not yet implemented: table function inside GROUP BY
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- name: post-agg project set - ok
  sql: |
    create table t (v1 int, v2 int);
    select min(v1), unnest(array[2, max(v2)]) from t;
  logical_plan: |-
    LogicalProject { exprs: [min(t.v1), Unnest(Array(2:Int32, $1))] }
    └─LogicalProjectSet { select_list: [$0, Unnest(Array(2:Int32, $1))] }
      └─LogicalAgg { aggs: [min(t.v1), max(t.v2)] }
        └─LogicalProject { exprs: [t.v1, t.v2] }
          └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
- name: post-agg project set - error
  sql: |
    create table t (v1 int, v2 int);
    select min(v1), unnest(array[2, v2]) from t;
  planner_error: 'Invalid input syntax: column must appear in the GROUP BY clause or be used in an aggregate function'
- name: post-agg project set - grouped
  sql: |
    create table t (v1 int, v2 int);
    select min(v1), unnest(array[2, v2]) from t group by v2;
  logical_plan: |-
    LogicalProject { exprs: [min(t.v1), Unnest(Array(2:Int32, $0))] }
    └─LogicalProjectSet { select_list: [$1, Unnest(Array(2:Int32, $0))] }
      └─LogicalAgg { group_key: [t.v2], aggs: [min(t.v1)] }
        └─LogicalProject { exprs: [t.v2, t.v1] }
          └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
- name: min/max on index
  sql: |
    create table t (v1 varchar, v2 int);
    create index idx on t(v2 desc);
    select max(v2) from t;
  logical_plan: |-
    LogicalProject { exprs: [max(t.v2)] }
    └─LogicalAgg { aggs: [max(t.v2)] }
      └─LogicalProject { exprs: [t.v2] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalAgg { aggs: [max(idx.v2)] }
    └─LogicalTopN { order: [idx.v2 DESC], limit: 1, offset: 0 }
      └─LogicalFilter { predicate: IsNotNull(idx.v2) }
        └─LogicalScan { table: idx, columns: [idx.v2] }
- name: min/max on index with group by, shall NOT optimize
  sql: |
    create table t (v1 int, v2 int);
    create index idx on t(v2 desc);
    select max(v2) from t group by v1;
  logical_plan: |-
    LogicalProject { exprs: [max(t.v2)] }
    └─LogicalAgg { group_key: [t.v1], aggs: [max(t.v2)] }
      └─LogicalProject { exprs: [t.v1, t.v2] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalProject { exprs: [max(t.v2)] }
    └─LogicalAgg { group_key: [t.v1], aggs: [max(t.v2)] }
      └─LogicalScan { table: t, columns: [t.v1, t.v2] }
- name: min/max on primary key
  sql: |
    create table t (v1 int primary key);
    select min(v1) from t;
  logical_plan: |-
    LogicalProject { exprs: [min(t.v1)] }
    └─LogicalAgg { aggs: [min(t.v1)] }
      └─LogicalProject { exprs: [t.v1] }
        └─LogicalScan { table: t, columns: [t.v1, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalAgg { aggs: [min(t.v1)] }
    └─LogicalTopN { order: [t.v1 ASC], limit: 1, offset: 0 }
      └─LogicalFilter { predicate: IsNotNull(t.v1) }
        └─LogicalScan { table: t, columns: [t.v1] }
- name: stddev_samp
  sql: |
    create table t (v1 int);
    select stddev_samp(v1), stddev_pop(v1) from t;
  logical_plan: |-
    LogicalProject { exprs: [Case((count(t.v1) <= 1:Int32), null:Decimal, Sqrt((Greatest((sum($expr1)::Decimal - ((sum(t.v1)::Decimal * sum(t.v1)::Decimal) / count(t.v1)::Decimal)), 0:Int32::Decimal) / (count(t.v1) - 1:Int32)::Decimal))) as $expr2, Case((count(t.v1) = 0:Int32), null:Decimal, Sqrt((Greatest((sum($expr1)::Decimal - ((sum(t.v1)::Decimal * sum(t.v1)::Decimal) / count(t.v1)::Decimal)), 0:Int32::Decimal) / count(t.v1)::Decimal))) as $expr3] }
    └─LogicalAgg { aggs: [sum($expr1), sum(t.v1), count(t.v1)] }
      └─LogicalProject { exprs: [(t.v1 * t.v1) as $expr1, t.v1] }
        └─LogicalScan { table: t, columns: [t.v1, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchProject { exprs: [Case((sum0(count(t.v1)) <= 1:Int32), null:Decimal, Sqrt((Greatest((sum(sum($expr1))::Decimal - ((sum(sum(t.v1))::Decimal * sum(sum(t.v1))::Decimal) / sum0(count(t.v1))::Decimal)), 0:Decimal) / (sum0(count(t.v1)) - 1:Int32)::Decimal))) as $expr2, Case((sum0(count(t.v1)) = 0:Int32), null:Decimal, Sqrt((Greatest((sum(sum($expr1))::Decimal - ((sum(sum(t.v1))::Decimal * sum(sum(t.v1))::Decimal) / sum0(count(t.v1))::Decimal)), 0:Decimal) / sum0(count(t.v1))::Decimal))) as $expr3] }
    └─BatchSimpleAgg { aggs: [sum(sum($expr1)), sum(sum(t.v1)), sum0(count(t.v1))] }
      └─BatchExchange { order: [], dist: Single }
        └─BatchSimpleAgg { aggs: [sum($expr1), sum(t.v1), count(t.v1)] }
          └─BatchProject { exprs: [(t.v1 * t.v1) as $expr1, t.v1] }
            └─BatchScan { table: t, columns: [t.v1], distribution: SomeShard }
  batch_local_plan: |-
    BatchProject { exprs: [Case((count(t.v1) <= 1:Int32), null:Decimal, Sqrt((Greatest((sum($expr1)::Decimal - ((sum(t.v1)::Decimal * sum(t.v1)::Decimal) / count(t.v1)::Decimal)), 0:Decimal) / (count(t.v1) - 1:Int32)::Decimal))) as $expr2, Case((count(t.v1) = 0:Int32), null:Decimal, Sqrt((Greatest((sum($expr1)::Decimal - ((sum(t.v1)::Decimal * sum(t.v1)::Decimal) / count(t.v1)::Decimal)), 0:Decimal) / count(t.v1)::Decimal))) as $expr3] }
    └─BatchSimpleAgg { aggs: [sum($expr1), sum(t.v1), count(t.v1)] }
      └─BatchExchange { order: [], dist: Single }
        └─BatchProject { exprs: [(t.v1 * t.v1) as $expr1, t.v1] }
          └─BatchScan { table: t, columns: [t.v1], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [stddev_samp, stddev_pop], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [Case((sum0(count(t.v1)) <= 1:Int32), null:Decimal, Sqrt((Greatest((sum(sum($expr1))::Decimal - ((sum(sum(t.v1))::Decimal * sum(sum(t.v1))::Decimal) / sum0(count(t.v1))::Decimal)), 0:Decimal) / (sum0(count(t.v1)) - 1:Int32)::Decimal))) as $expr2, Case((sum0(count(t.v1)) = 0:Int32), null:Decimal, Sqrt((Greatest((sum(sum($expr1))::Decimal - ((sum(sum(t.v1))::Decimal * sum(sum(t.v1))::Decimal) / sum0(count(t.v1))::Decimal)), 0:Decimal) / sum0(count(t.v1))::Decimal))) as $expr3] }
      └─StreamSimpleAgg { aggs: [sum(sum($expr1)), sum(sum(t.v1)), sum0(count(t.v1)), count] }
        └─StreamExchange { dist: Single }
          └─StreamStatelessSimpleAgg { aggs: [sum($expr1), sum(t.v1), count(t.v1)] }
            └─StreamProject { exprs: [(t.v1 * t.v1) as $expr1, t.v1, t._row_id] }
              └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: stddev_samp with other columns
  sql: |
    select count(''), stddev_samp(1);
  logical_plan: |-
    LogicalProject { exprs: [count('':Varchar), Case((count(1:Int32) <= 1:Int32), null:Decimal, Sqrt((Greatest((sum($expr1)::Decimal - ((sum(1:Int32)::Decimal * sum(1:Int32)::Decimal) / count(1:Int32)::Decimal)), 0:Int32::Decimal) / (count(1:Int32) - 1:Int32)::Decimal))) as $expr2] }
    └─LogicalAgg { aggs: [count('':Varchar), sum($expr1), sum(1:Int32), count(1:Int32)] }
      └─LogicalProject { exprs: ['':Varchar, (1:Int32 * 1:Int32) as $expr1, 1:Int32] }
        └─LogicalValues { rows: [[]], schema: Schema { fields: [] } }
- name: stddev_samp with group
  sql: |
    create table t(v int, w float);
    select stddev_samp(v) from t group by w;
  logical_plan: |-
    LogicalProject { exprs: [Case((count(t.v) <= 1:Int32), null:Decimal, Sqrt((Greatest((sum($expr1)::Decimal - ((sum(t.v)::Decimal * sum(t.v)::Decimal) / count(t.v)::Decimal)), 0:Int32::Decimal) / (count(t.v) - 1:Int32)::Decimal))) as $expr2] }
    └─LogicalAgg { group_key: [t.w], aggs: [sum($expr1), sum(t.v), count(t.v)] }
      └─LogicalProject { exprs: [t.w, (t.v * t.v) as $expr1, t.v] }
        └─LogicalScan { table: t, columns: [t.v, t.w, t._row_id, t._rw_timestamp] }
- name: force two phase aggregation should succeed with UpstreamHashShard and SomeShard (batch only).
  sql: |
    SET QUERY_MODE TO DISTRIBUTED;
    SET RW_FORCE_TWO_PHASE_AGG=true;
    create table t(v1 int, v2 smallint, v3 varchar);
    select min(v3), sum(v1) from t group by v1, v3, v2;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [min(min(t.v3)), sum(sum(t.v1))] }
      └─BatchHashAgg { group_key: [t.v1, t.v2, t.v3], aggs: [min(min(t.v3)), sum(sum(t.v1))] }
        └─BatchExchange { order: [], dist: HashShard(t.v1, t.v2, t.v3) }
          └─BatchHashAgg { group_key: [t.v1, t.v2, t.v3], aggs: [min(t.v3), sum(t.v1)] }
            └─BatchScan { table: t, columns: [t.v1, t.v2, t.v3], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [min, sum, t.v1(hidden), t.v2(hidden), t.v3(hidden)], stream_key: [t.v1, t.v2, t.v3], pk_columns: [t.v1, t.v2, t.v3], pk_conflict: NoCheck }
    └─StreamProject { exprs: [min(min(t.v3)), sum(sum(t.v1)), t.v1, t.v2, t.v3] }
      └─StreamHashAgg { group_key: [t.v1, t.v2, t.v3], aggs: [min(min(t.v3)), sum(sum(t.v1)), count] }
        └─StreamExchange { dist: HashShard(t.v1, t.v2, t.v3) }
          └─StreamHashAgg { group_key: [t.v1, t.v2, t.v3, _vnode], aggs: [min(t.v3), sum(t.v1), count] }
            └─StreamProject { exprs: [t.v1, t.v2, t.v3, t._row_id, Vnode(t._row_id) as _vnode] }
              └─StreamTableScan { table: t, columns: [t.v1, t.v2, t.v3, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: enable two phase aggregation with simple agg should have two phase agg
  sql: |
    SET QUERY_MODE TO DISTRIBUTED;
    SET ENABLE_TWO_PHASE_AGG=true;
    create table t(v1 int, v2 int);
    select min(v1), sum(v2) from t;
  batch_plan: |-
    BatchSimpleAgg { aggs: [min(min(t.v1)), sum(sum(t.v2))] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchSimpleAgg { aggs: [min(t.v1), sum(t.v2)] }
        └─BatchScan { table: t, columns: [t.v1, t.v2], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [min, sum], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [min(min(t.v1)), sum(sum(t.v2))] }
      └─StreamSimpleAgg { aggs: [min(min(t.v1)), sum(sum(t.v2)), count] }
        └─StreamExchange { dist: Single }
          └─StreamHashAgg { group_key: [_vnode], aggs: [min(t.v1), sum(t.v2), count] }
            └─StreamProject { exprs: [t.v1, t.v2, t._row_id, Vnode(t._row_id) as _vnode] }
              └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: disable two phase aggregation with simple agg
  sql: |
    SET QUERY_MODE TO DISTRIBUTED;
    SET ENABLE_TWO_PHASE_AGG=false;
    create table t(v1 int, v2 int);
    select min(v1), sum(v2) from t;
  batch_plan: |-
    BatchSimpleAgg { aggs: [min(t.v1), sum(t.v2)] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchScan { table: t, columns: [t.v1, t.v2], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [min, sum], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [min(t.v1), sum(t.v2)] }
      └─StreamSimpleAgg { aggs: [min(t.v1), sum(t.v2), count] }
        └─StreamExchange { dist: Single }
          └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: force two phase agg with different distributions on inner and outer agg should have exchange
  sql: |
    set QUERY_MODE to DISTRIBUTED;
    set RW_FORCE_TWO_PHASE_AGG to TRUE;
    CREATE TABLE lineitem (
        l_orderkey BIGINT,
        l_tax NUMERIC,
        l_commitdate DATE,
        l_shipinstruct VARCHAR,
        PRIMARY KEY (l_orderkey)
    );
    SELECT
        max(sq_1.col_2) as col_0
    FROM
        (
            SELECT
                t_0.l_commitdate AS col_2
            FROM
                lineitem AS t_0
            GROUP BY
                t_0.l_tax,
                t_0.l_shipinstruct,
                t_0.l_orderkey,
                t_0.l_commitdate
        ) AS sq_1
    GROUP BY
        sq_1.col_2;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [max(max(internal_last_seen_value(lineitem.l_commitdate)))] }
      └─BatchHashAgg { group_key: [internal_last_seen_value(lineitem.l_commitdate)], aggs: [max(max(internal_last_seen_value(lineitem.l_commitdate)))] }
        └─BatchExchange { order: [], dist: HashShard(internal_last_seen_value(lineitem.l_commitdate)) }
          └─BatchHashAgg { group_key: [internal_last_seen_value(lineitem.l_commitdate)], aggs: [max(internal_last_seen_value(lineitem.l_commitdate))] }
            └─BatchSortAgg { group_key: [lineitem.l_orderkey], aggs: [internal_last_seen_value(lineitem.l_commitdate)] }
              └─BatchScan { table: lineitem, columns: [lineitem.l_orderkey, lineitem.l_commitdate], distribution: UpstreamHashShard(lineitem.l_orderkey) }
  stream_plan: |-
    StreamMaterialize { columns: [col_0, internal_last_seen_value(lineitem.l_commitdate)(hidden)], stream_key: [internal_last_seen_value(lineitem.l_commitdate)], pk_columns: [internal_last_seen_value(lineitem.l_commitdate)], pk_conflict: NoCheck }
    └─StreamProject { exprs: [max(max(internal_last_seen_value(lineitem.l_commitdate))), internal_last_seen_value(lineitem.l_commitdate)] }
      └─StreamHashAgg { group_key: [internal_last_seen_value(lineitem.l_commitdate)], aggs: [max(max(internal_last_seen_value(lineitem.l_commitdate))), count] }
        └─StreamExchange { dist: HashShard(internal_last_seen_value(lineitem.l_commitdate)) }
          └─StreamHashAgg { group_key: [internal_last_seen_value(lineitem.l_commitdate), $expr1], aggs: [max(internal_last_seen_value(lineitem.l_commitdate)), count] }
            └─StreamProject { exprs: [lineitem.l_orderkey, internal_last_seen_value(lineitem.l_commitdate), Vnode(lineitem.l_orderkey) as $expr1] }
              └─StreamHashAgg { group_key: [lineitem.l_orderkey], aggs: [internal_last_seen_value(lineitem.l_commitdate), count] }
                └─StreamTableScan { table: lineitem, columns: [lineitem.l_orderkey, lineitem.l_commitdate], stream_scan_type: ArrangementBackfill, stream_key: [lineitem.l_orderkey], pk: [l_orderkey], dist: UpstreamHashShard(lineitem.l_orderkey) }
- name: two phase agg on hop window input should use two phase agg
  sql: |
    SET QUERY_MODE TO DISTRIBUTED;
    SET RW_FORCE_TWO_PHASE_AGG=true;
    create table bid(date_time TIMESTAMP, auction int);
    SELECT
      max(CountBids.num) AS maxn,
      CountBids.starttime_c
    FROM (
      SELECT
        count(*) AS num,
        window_start AS starttime_c
      FROM HOP(bid, date_time, INTERVAL '2' SECOND, INTERVAL '10' SECOND)
      GROUP BY
        bid.auction,
        window_start
    ) as CountBids
    GROUP BY
    CountBids.starttime_c;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [max(max(sum0(count))), window_start] }
      └─BatchHashAgg { group_key: [window_start], aggs: [max(max(sum0(count)))] }
        └─BatchExchange { order: [], dist: HashShard(window_start) }
          └─BatchHashAgg { group_key: [window_start], aggs: [max(sum0(count))] }
            └─BatchHashAgg { group_key: [bid.auction, window_start], aggs: [sum0(count)] }
              └─BatchExchange { order: [], dist: HashShard(bid.auction, window_start) }
                └─BatchHashAgg { group_key: [bid.auction, window_start], aggs: [count] }
                  └─BatchHopWindow { time_col: bid.date_time, slide: 00:00:02, size: 00:00:10, output: [bid.auction, window_start] }
                    └─BatchFilter { predicate: IsNotNull(bid.date_time) }
                      └─BatchScan { table: bid, columns: [bid.date_time, bid.auction], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [maxn, starttime_c], stream_key: [starttime_c], pk_columns: [starttime_c], pk_conflict: NoCheck }
    └─StreamProject { exprs: [max(max(sum0(count))), window_start] }
      └─StreamHashAgg { group_key: [window_start], aggs: [max(max(sum0(count))), count] }
        └─StreamExchange { dist: HashShard(window_start) }
          └─StreamHashAgg { group_key: [window_start, $expr1], aggs: [max(sum0(count)), count] }
            └─StreamProject { exprs: [bid.auction, window_start, sum0(count), Vnode(bid.auction, window_start) as $expr1] }
              └─StreamHashAgg { group_key: [bid.auction, window_start], aggs: [sum0(count), count] }
                └─StreamExchange { dist: HashShard(bid.auction, window_start) }
                  └─StreamHashAgg { group_key: [bid.auction, window_start, _vnode], aggs: [count] }
                    └─StreamProject { exprs: [bid.auction, window_start, bid._row_id, Vnode(bid._row_id) as _vnode] }
                      └─StreamHopWindow { time_col: bid.date_time, slide: 00:00:02, size: 00:00:10, output: [bid.auction, window_start, bid._row_id] }
                        └─StreamFilter { predicate: IsNotNull(bid.date_time) }
                          └─StreamTableScan { table: bid, columns: [bid.date_time, bid.auction, bid._row_id], stream_scan_type: ArrangementBackfill, stream_key: [bid._row_id], pk: [_row_id], dist: UpstreamHashShard(bid._row_id) }
- name: two phase agg with stream SomeShard (via index) but pk satisfies output dist should use shuffle agg
  sql: |
    SET QUERY_MODE TO DISTRIBUTED;
    SET RW_FORCE_TWO_PHASE_AGG=true;
    create table t (id int primary key, col int);
    create index idx on t(col);
    with sq as (select id from idx) select count(*) from sq group by sq.id;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [sum0(count)] }
      └─BatchHashAgg { group_key: [idx.id], aggs: [sum0(count)] }
        └─BatchExchange { order: [], dist: HashShard(idx.id) }
          └─BatchHashAgg { group_key: [idx.id], aggs: [count] }
            └─BatchScan { table: idx, columns: [idx.id], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [count, idx.id(hidden)], stream_key: [idx.id], pk_columns: [idx.id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [count, idx.id] }
      └─StreamHashAgg { group_key: [idx.id], aggs: [count] }
        └─StreamExchange { dist: HashShard(idx.id) }
          └─StreamTableScan { table: idx, columns: [idx.id], stream_scan_type: ArrangementBackfill, stream_key: [idx.id], pk: [col, id], dist: SomeShard }
- name: two phase agg with stream SomeShard (via index) but pk does not satisfy output dist should use two phase agg
  sql: |
    SET QUERY_MODE TO DISTRIBUTED;
    SET RW_FORCE_TWO_PHASE_AGG=true;
    create table t (id int primary key, col1 int, col2 int);
    create index idx on t(id);
    with sq as (select col1, col2 from idx) select count(*) from sq group by sq.col1;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [sum0(count)] }
      └─BatchHashAgg { group_key: [idx.col1], aggs: [sum0(count)] }
        └─BatchExchange { order: [], dist: HashShard(idx.col1) }
          └─BatchHashAgg { group_key: [idx.col1], aggs: [count] }
            └─BatchScan { table: idx, columns: [idx.col1], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [count, idx.col1(hidden)], stream_key: [idx.col1], pk_columns: [idx.col1], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum0(count), idx.col1] }
      └─StreamHashAgg { group_key: [idx.col1], aggs: [sum0(count), count] }
        └─StreamExchange { dist: HashShard(idx.col1) }
          └─StreamHashAgg { group_key: [idx.col1, _vnode], aggs: [count] }
            └─StreamProject { exprs: [idx.col1, idx.id, Vnode(idx.id) as _vnode] }
              └─StreamTableScan { table: idx, columns: [idx.col1, idx.id], stream_scan_type: ArrangementBackfill, stream_key: [idx.id], pk: [id], dist: UpstreamHashShard(idx.id) }
- name: sort agg on an ascending index
  sql: |
    create table t (a int, b int);
    create index idx_asc on t(a asc);
    create index idx_desc on t(a desc);
    select a, count(*) cnt from t group by a order by a asc;
  batch_plan: |-
    BatchExchange { order: [t.a ASC], dist: Single }
    └─BatchSort { order: [t.a ASC] }
      └─BatchHashAgg { group_key: [t.a], aggs: [count] }
        └─BatchExchange { order: [], dist: HashShard(t.a) }
          └─BatchScan { table: t, columns: [t.a], distribution: SomeShard }
- name: sort agg on a descending index
  sql: |
    create table t (a int, b int);
    create index idx_asc on t(a asc);
    create index idx_desc on t(a desc);
    select a, count(*) cnt from t group by a order by a desc;
  batch_plan: |-
    BatchExchange { order: [t.a DESC], dist: Single }
    └─BatchSort { order: [t.a DESC] }
      └─BatchHashAgg { group_key: [t.a], aggs: [count] }
        └─BatchExchange { order: [], dist: HashShard(t.a) }
          └─BatchScan { table: t, columns: [t.a], distribution: SomeShard }
- sql: |
    create table t (x int, y int);
    select percentile_cont(x) within group (order by y) from t;
  binder_error: |
    Failed to bind expression: percentile_cont(x) WITHIN GROUP (ORDER BY y)

    Caused by:
      Feature is not yet implemented: variable as direct argument of ordered-set aggregate
    Tracking issue: https://github.com/risingwavelabs/risingwave/issues/14079
- sql: |
    create table t (x int, y int);
    select percentile_cont('abc') within group (order by y) from t;
  binder_error: |
    Failed to bind expression: percentile_cont('abc') WITHIN GROUP (ORDER BY y)

    Caused by:
      Feature is not yet implemented: variable as direct argument of ordered-set aggregate
    Tracking issue: https://github.com/risingwavelabs/risingwave/issues/14079
- sql: |
    create table t (x int, y int);
    select percentile_cont(1.3) within group (order by y) from t;
  binder_error: |
    Failed to bind expression: percentile_cont(1.3) WITHIN GROUP (ORDER BY y)

    Caused by:
      Invalid input syntax: direct arg in `percentile_cont` must between 0.0 and 1.0
- sql: |
    create table t (x int, y int);
    select percentile_cont(0, 0) within group (order by y) from t;
  binder_error: |
    Failed to bind expression: percentile_cont(0, 0) WITHIN GROUP (ORDER BY y)

    Caused by:
      Invalid input syntax: invalid direct args or within group argument for `percentile_cont` aggregation
- sql: |
    create table t (x int, y varchar);
    select percentile_cont(0) within group (order by y) from t;
  binder_error: |
    Failed to bind expression: percentile_cont(0) WITHIN GROUP (ORDER BY y)

    Caused by:
      Invalid input syntax: arg in `percentile_cont` must be castable to float64
- sql: |
    create table t (x int, y int);
    select percentile_cont(0) within group (order by y desc) from t;
  batch_plan: |-
    BatchSimpleAgg { aggs: [percentile_cont($expr1 order_by(t.y DESC))] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchProject { exprs: [t.y::Float64 as $expr1, t.y] }
        └─BatchScan { table: t, columns: [t.y], distribution: SomeShard }
- sql: |
    create table t (x int, y int);
    select percentile_cont(null) within group (order by y) from t;
  batch_plan: |-
    BatchSimpleAgg { aggs: [percentile_cont($expr1 order_by(t.y ASC))] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchProject { exprs: [t.y::Float64 as $expr1, t.y] }
        └─BatchScan { table: t, columns: [t.y], distribution: SomeShard }
- sql: |
    create table t (x int, y int);
    select percentile_cont(0.2 + 0.3) within group (order by y) from t;
  batch_plan: |-
    BatchSimpleAgg { aggs: [percentile_cont($expr1 order_by(t.y ASC))] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchProject { exprs: [t.y::Float64 as $expr1, t.y] }
        └─BatchScan { table: t, columns: [t.y], distribution: SomeShard }
- sql: |
    create table t (x int, y int);
    select percentile_cont(0.2 + null) within group (order by y) from t;
  batch_plan: |-
    BatchSimpleAgg { aggs: [percentile_cont($expr1 order_by(t.y ASC))] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchProject { exprs: [t.y::Float64 as $expr1, t.y] }
        └─BatchScan { table: t, columns: [t.y], distribution: SomeShard }
- sql: |
    create table t (x int, y int);
    select percentile_cont(0.8 + 0.8) within group (order by y) from t;
  binder_error: |
    Failed to bind expression: percentile_cont(0.8 + 0.8) WITHIN GROUP (ORDER BY y)

    Caused by:
      Invalid input syntax: direct arg in `percentile_cont` must between 0.0 and 1.0
- sql: |
    create table t (x int, y int);
    select percentile_cont(0.2 + x) within group (order by y) from t group by x;
  binder_error: |
    Failed to bind expression: percentile_cont(0.2 + x) WITHIN GROUP (ORDER BY y)

    Caused by:
      Feature is not yet implemented: variable as direct argument of ordered-set aggregate
    Tracking issue: https://github.com/risingwavelabs/risingwave/issues/14079
- sql: |
    create table t (x int, y varchar);
    select percentile_disc(1) within group (order by y desc) from t;
  batch_plan: |-
    BatchSimpleAgg { aggs: [percentile_disc(t.y order_by(t.y DESC))] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchScan { table: t, columns: [t.y], distribution: SomeShard }
- sql: |
    create table t (x int, y varchar);
    select mode() within group (order by y desc) from t;
  batch_plan: |-
    BatchSimpleAgg { aggs: [mode(t.y order_by(t.y DESC))] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchScan { table: t, columns: [t.y], distribution: SomeShard }
- sql: |
    create table t (x int, y varchar);
    select mode(1) within group (order by y desc) from t;
  binder_error: |
    Failed to bind expression: mode(1) WITHIN GROUP (ORDER BY y DESC)

    Caused by:
      Invalid input syntax: invalid direct args or within group argument for `mode` aggregation
- sql: |
    create table t (x int, y int);
    select first_value(x) from t;
  planner_error: 'Invalid input syntax: Aggregation function first_value requires ORDER BY clause'
- sql: |
    create table t (x int, y int);
    select last_value(x) from t;
  planner_error: 'Invalid input syntax: Aggregation function last_value requires ORDER BY clause'
- sql: |
    create table t (x int, y int);
    select first_value(x order by y asc) from t;
  batch_plan: |-
    BatchSimpleAgg { aggs: [first_value(t.x order_by(t.y ASC))] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchScan { table: t, columns: [t.x, t.y], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [first_value], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [first_value(t.x order_by(t.y ASC))] }
      └─StreamSimpleAgg { aggs: [first_value(t.x order_by(t.y ASC)), count] }
        └─StreamExchange { dist: Single }
          └─StreamTableScan { table: t, columns: [t.x, t.y, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [first_value], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    ├── tables: [ Materialize: 4294967294 ]
    └── StreamProject { exprs: [first_value(t.x order_by(t.y ASC))] }
        └── StreamSimpleAgg { aggs: [first_value(t.x order_by(t.y ASC)), count] }
            ├── tables: [ SimpleAggState: 1, SimpleAggCall0: 0 ]
            └── StreamExchange Single from 1

    Fragment 1
    StreamTableScan { table: t, columns: [t.x, t.y, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
    ├── tables: [ StreamScan: 2 ]
    ├── Upstream
    └── BatchPlanNode

    Table 0 { columns: [ t_y, t__row_id, t_x, _rw_timestamp ], primary key: [ $0 ASC, $1 ASC ], value indices: [ 0, 1, 2 ], distribution key: [], read pk prefix len hint: 0 }

    Table 1 { columns: [ first_value(t_x order_by(t_y ASC)), count, _rw_timestamp ], primary key: [], value indices: [ 0, 1 ], distribution key: [], read pk prefix len hint: 0 }

    Table 2
    ├── columns: [ vnode, _row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 4294967294 { columns: [ first_value, _rw_timestamp ], primary key: [], value indices: [ 0 ], distribution key: [], read pk prefix len hint: 0 }

- sql: |
    create table t (x int, y int);
    select first_value(distinct x order by x asc) from t;
  stream_dist_plan: |+
    Fragment 0
    StreamMaterialize { columns: [first_value], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    ├── tables: [ Materialize: 4294967294 ]
    └── StreamProject { exprs: [first_value(distinct t.x order_by(t.x ASC))] }
        └── StreamSimpleAgg { aggs: [first_value(distinct t.x order_by(t.x ASC)), count] }
            ├── tables: [ SimpleAggState: 1, SimpleAggCall0: 0, SimpleAggDedupForCol0: 2 ]
            └── StreamExchange Single from 1

    Fragment 1
    StreamTableScan { table: t, columns: [t.x, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
    ├── tables: [ StreamScan: 3 ]
    ├── Upstream
    └── BatchPlanNode

    Table 0 { columns: [ t_x, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 0 ], distribution key: [], read pk prefix len hint: 0 }

    Table 1
    ├── columns: [ first_value(distinct t_x order_by(t_x ASC)), count, _rw_timestamp ]
    ├── primary key: []
    ├── value indices: [ 0, 1 ]
    ├── distribution key: []
    └── read pk prefix len hint: 0

    Table 2 { columns: [ t_x, count_for_agg_call_0, _rw_timestamp ], primary key: [ $0 ASC ], value indices: [ 1 ], distribution key: [], read pk prefix len hint: 1 }

    Table 3
    ├── columns: [ vnode, _row_id, backfill_finished, row_count, _rw_timestamp ]
    ├── primary key: [ $0 ASC ]
    ├── value indices: [ 1, 2, 3 ]
    ├── distribution key: [ 0 ]
    ├── read pk prefix len hint: 1
    └── vnode column idx: 0

    Table 4294967294 { columns: [ first_value, _rw_timestamp ], primary key: [], value indices: [ 0 ], distribution key: [], read pk prefix len hint: 0 }

- sql: |
    create table t (x int, y int);
    select last_value(x order by y desc nulls last) from t;
  batch_plan: |-
    BatchSimpleAgg { aggs: [last_value(t.x order_by(t.y DESC NULLS LAST))] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchScan { table: t, columns: [t.x, t.y], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [last_value], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [last_value(t.x order_by(t.y DESC NULLS LAST))] }
      └─StreamSimpleAgg { aggs: [last_value(t.x order_by(t.y DESC NULLS LAST)), count] }
        └─StreamExchange { dist: Single }
          └─StreamTableScan { table: t, columns: [t.x, t.y, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    CREATE TABLE integers(i INTEGER);
    SELECT count(DISTINCT i) FROM integers;
  batch_plan: |-
    BatchSimpleAgg { aggs: [sum0(count(integers.i))] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchSimpleAgg { aggs: [count(integers.i)] }
        └─BatchHashAgg { group_key: [integers.i], aggs: [] }
          └─BatchExchange { order: [], dist: HashShard(integers.i) }
            └─BatchScan { table: integers, columns: [integers.i], distribution: SomeShard }
- sql: |
    CREATE TABLE t(id int primary key, a int, b int);
    SELECT count(*) FROM t group by a, id, b;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [count] }
      └─BatchSortAgg { group_key: [t.id], aggs: [count] }
        └─BatchScan { table: t, columns: [t.id], distribution: UpstreamHashShard(t.id) }
  stream_plan: |-
    StreamMaterialize { columns: [count, t.id(hidden)], stream_key: [t.id], pk_columns: [t.id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [count, t.id] }
      └─StreamHashAgg { group_key: [t.id], aggs: [count] }
        └─StreamTableScan { table: t, columns: [t.id], stream_scan_type: ArrangementBackfill, stream_key: [t.id], pk: [id], dist: UpstreamHashShard(t.id) }
- sql: |
    CREATE TABLE t (a int, b int);
    SELECT a, sum((sum(b))) OVER (PARTITION BY a ORDER BY a) FROM t GROUP BY a;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.a, sum] }
      └─BatchOverWindow { window_functions: [sum(sum(t.b)) OVER(PARTITION BY t.a ORDER BY t.a ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)] }
        └─BatchSort { order: [t.a ASC, t.a ASC] }
          └─BatchHashAgg { group_key: [t.a], aggs: [sum(t.b)] }
            └─BatchExchange { order: [], dist: HashShard(t.a) }
              └─BatchScan { table: t, columns: [t.a, t.b], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [a, sum], stream_key: [a], pk_columns: [a], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.a, sum] }
      └─StreamOverWindow { window_functions: [sum(sum(t.b)) OVER(PARTITION BY t.a ORDER BY t.a ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)] }
        └─StreamProject { exprs: [t.a, sum(t.b)] }
          └─StreamHashAgg { group_key: [t.a], aggs: [sum(t.b), count] }
            └─StreamExchange { dist: HashShard(t.a) }
              └─StreamTableScan { table: t, columns: [t.a, t.b, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    CREATE TABLE t (a int, b int);
    SELECT a, row_number() OVER (PARTITION BY a ORDER BY a DESC) FROM t GROUP BY a;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchOverWindow { window_functions: [row_number() OVER(PARTITION BY t.a ORDER BY t.a DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)] }
      └─BatchSort { order: [t.a ASC, t.a DESC] }
        └─BatchHashAgg { group_key: [t.a], aggs: [] }
          └─BatchExchange { order: [], dist: HashShard(t.a) }
            └─BatchScan { table: t, columns: [t.a], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [a, row_number], stream_key: [a], pk_columns: [a], pk_conflict: NoCheck }
    └─StreamOverWindow { window_functions: [row_number() OVER(PARTITION BY t.a ORDER BY t.a DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)] }
      └─StreamProject { exprs: [t.a], noop_update_hint: true }
        └─StreamHashAgg { group_key: [t.a], aggs: [count] }
          └─StreamExchange { dist: HashShard(t.a) }
            └─StreamTableScan { table: t, columns: [t.a, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    CREATE TABLE t (a int, b int, c int);
    SELECT a, b, sum(sum(c)) OVER (PARTITION BY a ORDER BY b)
    FROM t
    GROUP BY a, b;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.a, t.b, sum] }
      └─BatchOverWindow { window_functions: [sum(sum(t.c)) OVER(PARTITION BY t.a ORDER BY t.b ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)] }
        └─BatchExchange { order: [t.a ASC, t.b ASC], dist: HashShard(t.a) }
          └─BatchSort { order: [t.a ASC, t.b ASC] }
            └─BatchHashAgg { group_key: [t.a, t.b], aggs: [sum(t.c)] }
              └─BatchExchange { order: [], dist: HashShard(t.a, t.b) }
                └─BatchScan { table: t, columns: [t.a, t.b, t.c], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [a, b, sum], stream_key: [a, b], pk_columns: [a, b], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.a, t.b, sum] }
      └─StreamOverWindow { window_functions: [sum(sum(t.c)) OVER(PARTITION BY t.a ORDER BY t.b ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)] }
        └─StreamExchange { dist: HashShard(t.a) }
          └─StreamProject { exprs: [t.a, t.b, sum(t.c)] }
            └─StreamHashAgg { group_key: [t.a, t.b], aggs: [sum(t.c), count] }
              └─StreamExchange { dist: HashShard(t.a, t.b) }
                └─StreamTableScan { table: t, columns: [t.a, t.b, t.c, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    CREATE TABLE t (a int, b int, c int, d int, e int);
    SELECT a, b, sum(sum(c)) OVER (PARTITION BY a, avg(d) ORDER BY max(e), b)
    FROM t
    GROUP BY a, b;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.a, t.b, sum] }
      └─BatchOverWindow { window_functions: [sum(sum(t.c)) OVER(PARTITION BY t.a, $expr1 ORDER BY max(t.e) ASC, t.b ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)] }
        └─BatchExchange { order: [t.a ASC, $expr1 ASC, max(t.e) ASC, t.b ASC], dist: HashShard(t.a, $expr1) }
          └─BatchSort { order: [t.a ASC, $expr1 ASC, max(t.e) ASC, t.b ASC] }
            └─BatchProject { exprs: [t.a, t.b, sum(t.c), max(t.e), (sum(t.d)::Decimal / count(t.d)::Decimal) as $expr1] }
              └─BatchHashAgg { group_key: [t.a, t.b], aggs: [sum(t.c), sum(t.d), count(t.d), max(t.e)] }
                └─BatchExchange { order: [], dist: HashShard(t.a, t.b) }
                  └─BatchScan { table: t, columns: [t.a, t.b, t.c, t.d, t.e], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [a, b, sum, $expr1(hidden)], stream_key: [a, b, $expr1], pk_columns: [a, b, $expr1], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.a, t.b, sum, $expr1] }
      └─StreamOverWindow { window_functions: [sum(sum(t.c)) OVER(PARTITION BY t.a, $expr1 ORDER BY max(t.e) ASC, t.b ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)] }
        └─StreamExchange { dist: HashShard(t.a, $expr1) }
          └─StreamProject { exprs: [t.a, t.b, sum(t.c), max(t.e), (sum(t.d)::Decimal / count(t.d)::Decimal) as $expr1] }
            └─StreamHashAgg { group_key: [t.a, t.b], aggs: [sum(t.c), sum(t.d), count(t.d), max(t.e), count] }
              └─StreamExchange { dist: HashShard(t.a, t.b) }
                └─StreamTableScan { table: t, columns: [t.a, t.b, t.c, t.d, t.e, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: test duplicate agg
  sql: |
    CREATE TABLE t (v1 int);
    SELECT sum(v1) as x, count(v1) as y, sum(v1) as z, count(v1) as w from t;
  logical_plan: |-
    LogicalProject { exprs: [sum(t.v1), count(t.v1), sum(t.v1), count(t.v1)] }
    └─LogicalAgg { aggs: [sum(t.v1), count(t.v1)] }
      └─LogicalProject { exprs: [t.v1] }
        └─LogicalScan { table: t, columns: [t.v1, t._row_id, t._rw_timestamp] }
  stream_plan: |-
    StreamMaterialize { columns: [x, y, z, w], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum(sum(t.v1)), sum0(count(t.v1)), sum(sum(t.v1)), sum0(count(t.v1))] }
      └─StreamSimpleAgg { aggs: [sum(sum(t.v1)), sum0(count(t.v1)), count] }
        └─StreamExchange { dist: Single }
          └─StreamStatelessSimpleAgg { aggs: [sum(t.v1), count(t.v1)] }
            └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: test simple approx_percentile alone
  sql: |
    CREATE TABLE t (v1 int);
    SELECT approx_percentile(0.5, 0.01) WITHIN GROUP (order by v1) from t;
  logical_plan: |-
    LogicalProject { exprs: [approx_percentile($expr1)] }
    └─LogicalAgg { aggs: [approx_percentile($expr1)] }
      └─LogicalProject { exprs: [t.v1::Float64 as $expr1] }
        └─LogicalScan { table: t, columns: [t.v1, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchSimpleAgg { aggs: [approx_percentile($expr1)] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchProject { exprs: [t.v1::Float64 as $expr1] }
        └─BatchScan { table: t, columns: [t.v1], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [approx_percentile], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamGlobalApproxPercentile { quantile: 0.5:Float64, relative_error: 0.01:Float64 }
      └─StreamExchange { dist: Single }
        └─StreamLocalApproxPercentile { percentile_col: $expr1, quantile: 0.5:Float64, relative_error: 0.01:Float64 }
          └─StreamProject { exprs: [t.v1::Float64 as $expr1, t._row_id] }
            └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: test simple approx_percentile with other simple aggs
  sql: |
    CREATE TABLE t (v1 int);
    SELECT approx_percentile(0.5, 0.01) WITHIN GROUP (order by v1), sum(v1) from t;
  logical_plan: |-
    LogicalProject { exprs: [approx_percentile($expr1), sum(t.v1)] }
    └─LogicalAgg { aggs: [approx_percentile($expr1), sum(t.v1)] }
      └─LogicalProject { exprs: [t.v1::Float64 as $expr1, t.v1] }
        └─LogicalScan { table: t, columns: [t.v1, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchSimpleAgg { aggs: [approx_percentile($expr1), sum(t.v1)] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchProject { exprs: [t.v1::Float64 as $expr1, t.v1] }
        └─BatchScan { table: t, columns: [t.v1], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [approx_percentile, sum], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamRowMerge { output: [approx_percentile:Float64, sum(sum(t.v1)):Int64] }
      ├─StreamGlobalApproxPercentile { quantile: 0.5:Float64, relative_error: 0.01:Float64 }
      │ └─StreamExchange { dist: Single }
      │   └─StreamLocalApproxPercentile { percentile_col: $expr1, quantile: 0.5:Float64, relative_error: 0.01:Float64 }
      │     └─StreamShare { id: 2 }
      │       └─StreamProject { exprs: [t.v1::Float64 as $expr1, t.v1, t._row_id] }
      │         └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
      └─StreamSimpleAgg { aggs: [sum(sum(t.v1)), count], must_output_per_barrier: true }
        └─StreamExchange { dist: Single }
          └─StreamStatelessSimpleAgg { aggs: [sum(t.v1)] }
            └─StreamShare { id: 2 }
              └─StreamProject { exprs: [t.v1::Float64 as $expr1, t.v1, t._row_id] }
                └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: test simple approx_percentile with other simple aggs (sum, count)
  sql: |
    CREATE TABLE t (v1 int);
    SELECT sum(v1) as s1, approx_percentile(0.5, 0.01) WITHIN GROUP (order by v1), sum(v1) as s2, count(v1) from t;
  logical_plan: |-
    LogicalProject { exprs: [sum(t.v1), approx_percentile($expr1), sum(t.v1), count(t.v1)] }
    └─LogicalAgg { aggs: [sum(t.v1), approx_percentile($expr1), count(t.v1)] }
      └─LogicalProject { exprs: [t.v1, t.v1::Float64 as $expr1] }
        └─LogicalScan { table: t, columns: [t.v1, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchProject { exprs: [sum(t.v1), approx_percentile($expr1), sum(t.v1), count(t.v1)] }
    └─BatchSimpleAgg { aggs: [sum(t.v1), approx_percentile($expr1), count(t.v1)] }
      └─BatchExchange { order: [], dist: Single }
        └─BatchProject { exprs: [t.v1, t.v1::Float64 as $expr1] }
          └─BatchScan { table: t, columns: [t.v1], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [s1, approx_percentile, s2, count], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum(sum(t.v1)), approx_percentile, sum(sum(t.v1)), sum0(count(t.v1))] }
      └─StreamRowMerge { output: [sum(sum(t.v1)):Int64, approx_percentile:Float64, sum0(count(t.v1)):Int64] }
        ├─StreamGlobalApproxPercentile { quantile: 0.5:Float64, relative_error: 0.01:Float64 }
        │ └─StreamExchange { dist: Single }
        │   └─StreamLocalApproxPercentile { percentile_col: $expr1, quantile: 0.5:Float64, relative_error: 0.01:Float64 }
        │     └─StreamShare { id: 2 }
        │       └─StreamProject { exprs: [t.v1, t.v1::Float64 as $expr1, t._row_id] }
        │         └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
        └─StreamSimpleAgg { aggs: [sum(sum(t.v1)), sum0(count(t.v1)), count], must_output_per_barrier: true }
          └─StreamExchange { dist: Single }
            └─StreamStatelessSimpleAgg { aggs: [sum(t.v1), count(t.v1)] }
              └─StreamShare { id: 2 }
                └─StreamProject { exprs: [t.v1, t.v1::Float64 as $expr1, t._row_id] }
                  └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: test simple approx_percentile with duplicate approx_percentile
  sql: |
    CREATE TABLE t (v1 int);
    SELECT approx_percentile(0.5, 0.01) WITHIN GROUP (order by v1) as x, approx_percentile(0.5, 0.01) WITHIN GROUP (order by v1) as y from t;
  logical_plan: |-
    LogicalProject { exprs: [approx_percentile($expr1), approx_percentile($expr1)] }
    └─LogicalAgg { aggs: [approx_percentile($expr1)] }
      └─LogicalProject { exprs: [t.v1::Float64 as $expr1] }
        └─LogicalScan { table: t, columns: [t.v1, t._row_id, t._rw_timestamp] }
  stream_plan: |-
    StreamMaterialize { columns: [x, y], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [approx_percentile, approx_percentile] }
      └─StreamGlobalApproxPercentile { quantile: 0.5:Float64, relative_error: 0.01:Float64 }
        └─StreamExchange { dist: Single }
          └─StreamLocalApproxPercentile { percentile_col: $expr1, quantile: 0.5:Float64, relative_error: 0.01:Float64 }
            └─StreamProject { exprs: [t.v1::Float64 as $expr1, t._row_id] }
              └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: test simple approx_percentile with different approx_percentile
  sql: |
    CREATE TABLE t (v1 int, v2 int);
    SELECT approx_percentile(0.5, 0.01) WITHIN GROUP (order by v1) as x, approx_percentile(0.5, 0.01) WITHIN GROUP (order by v2) as y from t;
  logical_plan: |-
    LogicalProject { exprs: [approx_percentile($expr1), approx_percentile($expr2)] }
    └─LogicalAgg { aggs: [approx_percentile($expr1), approx_percentile($expr2)] }
      └─LogicalProject { exprs: [t.v1::Float64 as $expr1, t.v2::Float64 as $expr2] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchSimpleAgg { aggs: [approx_percentile($expr1), approx_percentile($expr2)] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchProject { exprs: [t.v1::Float64 as $expr1, t.v2::Float64 as $expr2] }
        └─BatchScan { table: t, columns: [t.v1, t.v2], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, y], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamRowMerge { output: [approx_percentile:Float64, approx_percentile:Float64] }
      ├─StreamGlobalApproxPercentile { quantile: 0.5:Float64, relative_error: 0.01:Float64 }
      │ └─StreamExchange { dist: Single }
      │   └─StreamLocalApproxPercentile { percentile_col: $expr1, quantile: 0.5:Float64, relative_error: 0.01:Float64 }
      │     └─StreamShare { id: 2 }
      │       └─StreamProject { exprs: [t.v1::Float64 as $expr1, t.v2::Float64 as $expr2, t._row_id] }
      │         └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
      └─StreamGlobalApproxPercentile { quantile: 0.5:Float64, relative_error: 0.01:Float64 }
        └─StreamExchange { dist: Single }
          └─StreamLocalApproxPercentile { percentile_col: $expr2, quantile: 0.5:Float64, relative_error: 0.01:Float64 }
            └─StreamShare { id: 2 }
              └─StreamProject { exprs: [t.v1::Float64 as $expr1, t.v2::Float64 as $expr2, t._row_id] }
                └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: test simple approx_percentile with different approx_percentile interleaved with stateless simple aggs
  sql: |
    CREATE TABLE t (v1 int, v2 int);
    SELECT sum(v1) as s1, approx_percentile(0.5, 0.01) WITHIN GROUP (order by v1) as x, count(*), sum(v2) + approx_percentile(0.9, 0.01) WITHIN GROUP (order by v2) as y from t;
  logical_plan: |-
    LogicalProject { exprs: [sum(t.v1), approx_percentile($expr1), count, (sum(t.v2)::Float64 + approx_percentile($expr2)) as $expr3] }
    └─LogicalAgg { aggs: [sum(t.v1), approx_percentile($expr1), count, sum(t.v2), approx_percentile($expr2)] }
      └─LogicalProject { exprs: [t.v1, t.v1::Float64 as $expr1, t.v2, t.v2::Float64 as $expr2] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchProject { exprs: [sum(t.v1), approx_percentile($expr1), count, (sum(t.v2)::Float64 + approx_percentile($expr2)) as $expr3] }
    └─BatchSimpleAgg { aggs: [sum(t.v1), approx_percentile($expr1), count, sum(t.v2), approx_percentile($expr2)] }
      └─BatchExchange { order: [], dist: Single }
        └─BatchProject { exprs: [t.v1, t.v1::Float64 as $expr1, t.v2, t.v2::Float64 as $expr2] }
          └─BatchScan { table: t, columns: [t.v1, t.v2], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [s1, x, count, y], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum(sum(t.v1)), approx_percentile, sum0(count), (sum(sum(t.v2))::Float64 + approx_percentile) as $expr3] }
      └─StreamRowMerge { output: [sum(sum(t.v1)):Int64, approx_percentile:Float64, sum0(count):Int64, sum(sum(t.v2)):Int64, approx_percentile:Float64] }
        ├─StreamRowMerge { output: [approx_percentile:Float64, approx_percentile:Float64] }
        │ ├─StreamGlobalApproxPercentile { quantile: 0.5:Float64, relative_error: 0.01:Float64 }
        │ │ └─StreamExchange { dist: Single }
        │ │   └─StreamLocalApproxPercentile { percentile_col: $expr1, quantile: 0.5:Float64, relative_error: 0.01:Float64 }
        │ │     └─StreamShare { id: 2 }
        │ │       └─StreamProject { exprs: [t.v1, t.v1::Float64 as $expr1, t.v2, t.v2::Float64 as $expr2, t._row_id] }
        │ │         └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
        │ └─StreamGlobalApproxPercentile { quantile: 0.9:Float64, relative_error: 0.01:Float64 }
        │   └─StreamExchange { dist: Single }
        │     └─StreamLocalApproxPercentile { percentile_col: $expr2, quantile: 0.9:Float64, relative_error: 0.01:Float64 }
        │       └─StreamShare { id: 2 }
        │         └─StreamProject { exprs: [t.v1, t.v1::Float64 as $expr1, t.v2, t.v2::Float64 as $expr2, t._row_id] }
        │           └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
        └─StreamSimpleAgg { aggs: [sum(sum(t.v1)), sum0(count), sum(sum(t.v2)), count], must_output_per_barrier: true }
          └─StreamExchange { dist: Single }
            └─StreamStatelessSimpleAgg { aggs: [sum(t.v1), count, sum(t.v2)] }
              └─StreamShare { id: 2 }
                └─StreamProject { exprs: [t.v1, t.v1::Float64 as $expr1, t.v2, t.v2::Float64 as $expr2, t._row_id] }
                  └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: test simple approx_percentile with duplicated approx_percentile interleaved with stateless simple aggs
  sql: |
    CREATE TABLE t (v1 int, v2 int);
    SELECT sum(v1) as s1, approx_percentile(0.5, 0.01) WITHIN GROUP (order by v1) as x, count(*), sum(v2) + approx_percentile(0.5, 0.01) WITHIN GROUP (order by v2) as y from t;
  logical_plan: |-
    LogicalProject { exprs: [sum(t.v1), approx_percentile($expr1), count, (sum(t.v2)::Float64 + approx_percentile($expr2)) as $expr3] }
    └─LogicalAgg { aggs: [sum(t.v1), approx_percentile($expr1), count, sum(t.v2), approx_percentile($expr2)] }
      └─LogicalProject { exprs: [t.v1, t.v1::Float64 as $expr1, t.v2, t.v2::Float64 as $expr2] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchProject { exprs: [sum(t.v1), approx_percentile($expr1), count, (sum(t.v2)::Float64 + approx_percentile($expr2)) as $expr3] }
    └─BatchSimpleAgg { aggs: [sum(t.v1), approx_percentile($expr1), count, sum(t.v2), approx_percentile($expr2)] }
      └─BatchExchange { order: [], dist: Single }
        └─BatchProject { exprs: [t.v1, t.v1::Float64 as $expr1, t.v2, t.v2::Float64 as $expr2] }
          └─BatchScan { table: t, columns: [t.v1, t.v2], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [s1, x, count, y], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [sum(sum(t.v1)), approx_percentile, sum0(count), (sum(sum(t.v2))::Float64 + approx_percentile) as $expr3] }
      └─StreamRowMerge { output: [sum(sum(t.v1)):Int64, approx_percentile:Float64, sum0(count):Int64, sum(sum(t.v2)):Int64, approx_percentile:Float64] }
        ├─StreamRowMerge { output: [approx_percentile:Float64, approx_percentile:Float64] }
        │ ├─StreamGlobalApproxPercentile { quantile: 0.5:Float64, relative_error: 0.01:Float64 }
        │ │ └─StreamExchange { dist: Single }
        │ │   └─StreamLocalApproxPercentile { percentile_col: $expr1, quantile: 0.5:Float64, relative_error: 0.01:Float64 }
        │ │     └─StreamShare { id: 2 }
        │ │       └─StreamProject { exprs: [t.v1, t.v1::Float64 as $expr1, t.v2, t.v2::Float64 as $expr2, t._row_id] }
        │ │         └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
        │ └─StreamGlobalApproxPercentile { quantile: 0.5:Float64, relative_error: 0.01:Float64 }
        │   └─StreamExchange { dist: Single }
        │     └─StreamLocalApproxPercentile { percentile_col: $expr2, quantile: 0.5:Float64, relative_error: 0.01:Float64 }
        │       └─StreamShare { id: 2 }
        │         └─StreamProject { exprs: [t.v1, t.v1::Float64 as $expr1, t.v2, t.v2::Float64 as $expr2, t._row_id] }
        │           └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
        └─StreamSimpleAgg { aggs: [sum(sum(t.v1)), sum0(count), sum(sum(t.v2)), count], must_output_per_barrier: true }
          └─StreamExchange { dist: Single }
            └─StreamStatelessSimpleAgg { aggs: [sum(t.v1), count, sum(t.v2)] }
              └─StreamShare { id: 2 }
                └─StreamProject { exprs: [t.v1, t.v1::Float64 as $expr1, t.v2, t.v2::Float64 as $expr2, t._row_id] }
                  └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: test simple approx_percentile with descending order
  sql: |
    CREATE TABLE t (v1 int, v2 int);
    SELECT sum(v1) as s1, approx_percentile(0.2, 0.01) WITHIN GROUP (order by v1 desc) from t;
  logical_plan: |-
    LogicalProject { exprs: [sum(t.v1), approx_percentile($expr1)] }
    └─LogicalAgg { aggs: [sum(t.v1), approx_percentile($expr1)] }
      └─LogicalProject { exprs: [t.v1, t.v1::Float64 as $expr1] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchSimpleAgg { aggs: [sum(t.v1), approx_percentile($expr1)] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchProject { exprs: [t.v1, t.v1::Float64 as $expr1] }
        └─BatchScan { table: t, columns: [t.v1], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [s1, approx_percentile], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamRowMerge { output: [sum(sum(t.v1)):Int64, approx_percentile:Float64] }
      ├─StreamGlobalApproxPercentile { quantile: 0.8:Float64, relative_error: 0.01:Float64 }
      │ └─StreamExchange { dist: Single }
      │   └─StreamLocalApproxPercentile { percentile_col: $expr1, quantile: 0.8:Float64, relative_error: 0.01:Float64 }
      │     └─StreamShare { id: 2 }
      │       └─StreamProject { exprs: [t.v1, t.v1::Float64 as $expr1, t._row_id] }
      │         └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
      └─StreamSimpleAgg { aggs: [sum(sum(t.v1)), count], must_output_per_barrier: true }
        └─StreamExchange { dist: Single }
          └─StreamStatelessSimpleAgg { aggs: [sum(t.v1)] }
            └─StreamShare { id: 2 }
              └─StreamProject { exprs: [t.v1, t.v1::Float64 as $expr1, t._row_id] }
                └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: test simple approx_percentile with different approx_percentile interleaved with stateless + stateful simple aggs
  sql: |
    CREATE TABLE t (v1 int, v2 int);
    SELECT sum(v1) as s1, approx_percentile(0.5, 0.01) WITHIN GROUP (order by v1) as x, count(*), max(v2) as m2, approx_percentile(0.5, 0.01) WITHIN GROUP (order by v2) as y from t;
  logical_plan: |-
    LogicalProject { exprs: [sum(t.v1), approx_percentile($expr1), count, max(t.v2), approx_percentile($expr2)] }
    └─LogicalAgg { aggs: [sum(t.v1), approx_percentile($expr1), count, max(t.v2), approx_percentile($expr2)] }
      └─LogicalProject { exprs: [t.v1, t.v1::Float64 as $expr1, t.v2, t.v2::Float64 as $expr2] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchSimpleAgg { aggs: [sum(t.v1), approx_percentile($expr1), count, max(t.v2), approx_percentile($expr2)] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchProject { exprs: [t.v1, t.v1::Float64 as $expr1, t.v2, t.v2::Float64 as $expr2] }
        └─BatchScan { table: t, columns: [t.v1, t.v2], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [s1, x, count, m2, y], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamRowMerge { output: [sum(sum(t.v1)):Int64, approx_percentile:Float64, sum0(count):Int64, max(max(t.v2)):Int32, approx_percentile:Float64] }
      ├─StreamRowMerge { output: [approx_percentile:Float64, approx_percentile:Float64] }
      │ ├─StreamGlobalApproxPercentile { quantile: 0.5:Float64, relative_error: 0.01:Float64 }
      │ │ └─StreamExchange { dist: Single }
      │ │   └─StreamLocalApproxPercentile { percentile_col: $expr1, quantile: 0.5:Float64, relative_error: 0.01:Float64 }
      │ │     └─StreamShare { id: 2 }
      │ │       └─StreamProject { exprs: [t.v1, t.v1::Float64 as $expr1, t.v2, t.v2::Float64 as $expr2, t._row_id] }
      │ │         └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
      │ └─StreamGlobalApproxPercentile { quantile: 0.5:Float64, relative_error: 0.01:Float64 }
      │   └─StreamExchange { dist: Single }
      │     └─StreamLocalApproxPercentile { percentile_col: $expr2, quantile: 0.5:Float64, relative_error: 0.01:Float64 }
      │       └─StreamShare { id: 2 }
      │         └─StreamProject { exprs: [t.v1, t.v1::Float64 as $expr1, t.v2, t.v2::Float64 as $expr2, t._row_id] }
      │           └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
      └─StreamSimpleAgg { aggs: [sum(sum(t.v1)), sum0(count), max(max(t.v2)), count], must_output_per_barrier: true }
        └─StreamExchange { dist: Single }
          └─StreamHashAgg { group_key: [$expr5], aggs: [sum(t.v1), count, max(t.v2)] }
            └─StreamProject { exprs: [t.v1, t.v1::Float64 as $expr3, t.v2, t.v2::Float64 as $expr4, t._row_id, Vnode(t._row_id) as $expr5] }
              └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: test hash approx_percentile
  sql: |
    CREATE TABLE t (v1 int, v2 int);
    SELECT approx_percentile(0.5, 0.01) WITHIN GROUP (order by v1) from t group by v2;
  logical_plan: |-
    LogicalProject { exprs: [approx_percentile($expr1)] }
    └─LogicalAgg { group_key: [t.v2], aggs: [approx_percentile($expr1)] }
      └─LogicalProject { exprs: [t.v2, t.v1::Float64 as $expr1] }
        └─LogicalScan { table: t, columns: [t.v1, t.v2, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [approx_percentile($expr1)] }
      └─BatchHashAgg { group_key: [t.v2], aggs: [approx_percentile($expr1)] }
        └─BatchExchange { order: [], dist: HashShard(t.v2) }
          └─BatchProject { exprs: [t.v2, t.v1::Float64 as $expr1] }
            └─BatchScan { table: t, columns: [t.v1, t.v2], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [approx_percentile, t.v2(hidden)], stream_key: [t.v2], pk_columns: [t.v2], pk_conflict: NoCheck }
    └─StreamProject { exprs: [approx_percentile($expr1), t.v2] }
      └─StreamHashAgg { group_key: [t.v2], aggs: [approx_percentile($expr1), count] }
        └─StreamExchange { dist: HashShard(t.v2) }
          └─StreamProject { exprs: [t.v2, t.v1::Float64 as $expr1, t._row_id] }
            └─StreamTableScan { table: t, columns: [t.v1, t.v2, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: test approx_percentile hash_agg forced should use single phase agg
  sql: |
    SET RW_FORCE_TWO_PHASE_AGG=true;
    create table t (v1 int, grp_col int);
    select approx_percentile(0.5, 0.01) WITHIN GROUP (order by v1) from t group by grp_col;
  stream_error: |-
    Feature is not yet implemented: two-phase streaming approx percentile aggregation with group key, please use single phase aggregation instead
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- name: test approx percentile with default relative_error
  sql: |
    CREATE TABLE t (v1 int);
    SELECT approx_percentile(0.5) WITHIN GROUP (order by v1) from t;
  logical_plan: |-
    LogicalProject { exprs: [approx_percentile($expr1)] }
    └─LogicalAgg { aggs: [approx_percentile($expr1)] }
      └─LogicalProject { exprs: [t.v1::Float64 as $expr1] }
        └─LogicalScan { table: t, columns: [t.v1, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchSimpleAgg { aggs: [approx_percentile($expr1)] }
    └─BatchExchange { order: [], dist: Single }
      └─BatchProject { exprs: [t.v1::Float64 as $expr1] }
        └─BatchScan { table: t, columns: [t.v1], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [approx_percentile], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamGlobalApproxPercentile { quantile: 0.5:Float64, relative_error: 0.01:Float64 }
      └─StreamExchange { dist: Single }
        └─StreamLocalApproxPercentile { percentile_col: $expr1, quantile: 0.5:Float64, relative_error: 0.01:Float64 }
          └─StreamProject { exprs: [t.v1::Float64 as $expr1, t._row_id] }
            └─StreamTableScan { table: t, columns: [t.v1, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: test approx percentile with invalid relative_error
  sql: |
    CREATE TABLE t (v1 int);
    SELECT approx_percentile(0.5, 0.0) WITHIN GROUP (order by v1) from t;
  binder_error: |
    Failed to bind expression: approx_percentile(0.5, 0.0) WITHIN GROUP (ORDER BY v1)

    Caused by:
      relative_error=0 does not satisfy 0.0 < relative_error < 1.0
- name: test approx percentile with invalid relative_error 0.0
  sql: |
    CREATE TABLE t (v1 int);
    SELECT approx_percentile(0.5, 0.0) WITHIN GROUP (order by v1) from t;
  binder_error: |
    Failed to bind expression: approx_percentile(0.5, 0.0) WITHIN GROUP (ORDER BY v1)

    Caused by:
      relative_error=0 does not satisfy 0.0 < relative_error < 1.0
- name: test approx percentile with invalid relative_error 1.0 with group by.
  sql: |
    CREATE TABLE t (v1 int, v2 int);
    SELECT approx_percentile(0.0, 1.0) WITHIN GROUP (order by v1) from t group by v2;
  binder_error: |
    Failed to bind expression: approx_percentile(0.0, 1.0) WITHIN GROUP (ORDER BY v1)

    Caused by:
      relative_error=1 does not satisfy 0.0 < relative_error < 1.0
