# This file is automatically generated. See `src/frontend/planner_test/README.md` for more information.
- sql: |
    create table t (v int);
    select v+v from t limit 4;
  logical_plan: |-
    LogicalLimit { limit: 4, offset: 0 }
    └─LogicalProject { exprs: [(t.v + t.v) as $expr1] }
      └─LogicalScan { table: t, columns: [t.v, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalProject { exprs: [(t.v + t.v) as $expr1] }
    └─LogicalLimit { limit: 4, offset: 0 }
      └─LogicalScan { table: t, columns: [t.v] }
- sql: |
    create table t (v int);
    select * from t offset 4;
  logical_plan: |-
    LogicalLimit { limit: 9223372036854775807, offset: 4 }
    └─LogicalProject { exprs: [t.v] }
      └─LogicalScan { table: t, columns: [t.v, t._row_id, t._rw_timestamp] }
- sql: |
    create table t (v int);
    select * from ( select v+v from t limit 5 ) limit 4;
  logical_plan: |-
    LogicalLimit { limit: 4, offset: 0 }
    └─LogicalProject { exprs: [$expr1] }
      └─LogicalLimit { limit: 5, offset: 0 }
        └─LogicalProject { exprs: [(t.v + t.v) as $expr1] }
          └─LogicalScan { table: t, columns: [t.v, t._row_id, t._rw_timestamp] }
  optimized_logical_plan_for_batch: |-
    LogicalLimit { limit: 4, offset: 0 }
    └─LogicalProject { exprs: [(t.v + t.v) as $expr1] }
      └─LogicalLimit { limit: 5, offset: 0 }
        └─LogicalScan { table: t, columns: [t.v] }
- sql: |
    create table t (v int);
    select * from t fetch first 4 rows only;
  logical_plan: |-
    LogicalLimit { limit: 4, offset: 0 }
    └─LogicalProject { exprs: [t.v] }
      └─LogicalScan { table: t, columns: [t.v, t._row_id, t._rw_timestamp] }
- sql: |
    create table t (v int);
    select * from t offset 3 fetch first 4 rows only;
  logical_plan: |-
    LogicalLimit { limit: 4, offset: 3 }
    └─LogicalProject { exprs: [t.v] }
      └─LogicalScan { table: t, columns: [t.v, t._row_id, t._rw_timestamp] }
- sql: |
    create table t (v int);
    select * from t fetch next rows only;
  logical_plan: |-
    LogicalLimit { limit: 1, offset: 0 }
    └─LogicalProject { exprs: [t.v] }
      └─LogicalScan { table: t, columns: [t.v, t._row_id, t._rw_timestamp] }
- sql: |
    create table t (v int);
    select * from t order by v fetch next 2 rows with ties;
  logical_plan: |-
    LogicalTopN { order: [t.v ASC], limit: 2, offset: 0, with_ties: true }
    └─LogicalProject { exprs: [t.v] }
      └─LogicalScan { table: t, columns: [t.v, t._row_id, t._rw_timestamp] }
- sql: |
    -- Should be equivalent to above
    create table t (v int);
    select v from
    (select *, RANK() OVER (ORDER BY v) AS rank from t)
    where rank <= 2;
  optimized_logical_plan_for_batch: |-
    LogicalTopN { order: [t.v ASC], limit: 2, offset: 0, with_ties: true }
    └─LogicalScan { table: t, columns: [t.v] }
- sql: |
    create table t (v int);
    select * from t order by v offset 1 fetch next 2 rows with ties;
  planner_error: |-
    Feature is not yet implemented: WITH TIES is not supported with OFFSET
    No tracking issue yet. Feel free to submit a feature request at https://github.com/risingwavelabs/risingwave/issues/new?labels=type%2Ffeature&template=feature_request.yml
- sql: |
    select 1 c limit 1
  batch_plan: |-
    BatchLimit { limit: 1, offset: 0 }
    └─BatchValues { rows: [[1:Int32]] }
  stream_plan: |-
    StreamMaterialize { columns: [c, _row_id(hidden)], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamTopN [append_only] { order: [1:Int32 ASC], limit: 1, offset: 0 }
      └─StreamValues { rows: [[1:Int32, 0:Int64]] }
- sql: |
    select 1 c limit 1 offset 2
  batch_plan: |-
    BatchLimit { limit: 1, offset: 2 }
    └─BatchValues { rows: [[1:Int32]] }
  stream_plan: |-
    StreamMaterialize { columns: [c, _row_id(hidden)], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamTopN [append_only] { order: [1:Int32 ASC], limit: 1, offset: 2 }
      └─StreamValues { rows: [[1:Int32, 0:Int64]] }
- sql: |
    select 1 c order by 1 limit 1
  batch_plan: |-
    BatchTopN { order: [1:Int32 ASC], limit: 1, offset: 0 }
    └─BatchValues { rows: [[1:Int32]] }
  stream_plan: |-
    StreamMaterialize { columns: [c, _row_id(hidden)], stream_key: [], pk_columns: [c], pk_conflict: NoCheck }
    └─StreamTopN [append_only] { order: [1:Int32 ASC], limit: 1, offset: 0 }
      └─StreamValues { rows: [[1:Int32, 0:Int64]] }
- sql: |
    select 1 c union all select 1 c limit 10
  batch_plan: |-
    BatchLimit { limit: 10, offset: 0 }
    └─BatchValues { rows: [[1:Int32], [1:Int32]] }
  stream_plan: |-
    StreamMaterialize { columns: [c, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: NoCheck }
    └─StreamTopN [append_only] { order: [1:Int32 ASC], limit: 10, offset: 0 }
      └─StreamValues { rows: [[1:Int32, 0:Int64], [1:Int32, 1:Int64]] }
- sql: |
    select 1 c union all select 1 c order by 1 limit 10
  batch_plan: |-
    BatchTopN { order: [1:Int32 ASC], limit: 10, offset: 0 }
    └─BatchValues { rows: [[1:Int32], [1:Int32]] }
  stream_plan: |-
    StreamMaterialize { columns: [c, _row_id(hidden)], stream_key: [_row_id], pk_columns: [c, _row_id], pk_conflict: NoCheck }
    └─StreamTopN [append_only] { order: [1:Int32 ASC], limit: 10, offset: 0 }
      └─StreamValues { rows: [[1:Int32, 0:Int64], [1:Int32, 1:Int64]] }
- sql: |
    create table t (a int);
    select count(*) from t limit 1;
  batch_plan: |-
    BatchLimit { limit: 1, offset: 0 }
    └─BatchSimpleAgg { aggs: [sum0(count)] }
      └─BatchExchange { order: [], dist: Single }
        └─BatchSimpleAgg { aggs: [count] }
          └─BatchScan { table: t, columns: [], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [count], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamTopN { order: [sum0(count) ASC], limit: 1, offset: 0 }
      └─StreamProject { exprs: [sum0(count)] }
        └─StreamSimpleAgg { aggs: [sum0(count), count] }
          └─StreamExchange { dist: Single }
            └─StreamStatelessSimpleAgg { aggs: [count] }
              └─StreamTableScan { table: t, columns: [t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t (a int);
    select count(*) from t order by 1 limit 1;
  logical_plan: |-
    LogicalLimit { limit: 1, offset: 0 }
    └─LogicalProject { exprs: [count] }
      └─LogicalAgg { aggs: [count] }
        └─LogicalProject { exprs: [] }
          └─LogicalScan { table: t, columns: [t.a, t._row_id, t._rw_timestamp] }
  batch_plan: |-
    BatchLimit { limit: 1, offset: 0 }
    └─BatchSimpleAgg { aggs: [sum0(count)] }
      └─BatchExchange { order: [], dist: Single }
        └─BatchSimpleAgg { aggs: [count] }
          └─BatchScan { table: t, columns: [], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [count], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamTopN { order: [sum0(count) ASC], limit: 1, offset: 0 }
      └─StreamProject { exprs: [sum0(count)] }
        └─StreamSimpleAgg { aggs: [sum0(count), count] }
          └─StreamExchange { dist: Single }
            └─StreamStatelessSimpleAgg { aggs: [count] }
              └─StreamTableScan { table: t, columns: [t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t (a int primary key);
    select * from t limit 1;
  batch_plan: |-
    BatchLimit { limit: 1, offset: 0 }
    └─BatchExchange { order: [], dist: Single, sequential: true }
      └─BatchLimit { limit: 1, offset: 0 }
        └─BatchScan { table: t, columns: [t.a], limit: 1, distribution: UpstreamHashShard(t.a) }
  stream_plan: |-
    StreamMaterialize { columns: [a], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.a] }
      └─StreamTopN { order: [t.a ASC], limit: 1, offset: 0 }
        └─StreamExchange { dist: Single }
          └─StreamGroupTopN { order: [t.a ASC], limit: 1, offset: 0, group_key: [_vnode] }
            └─StreamProject { exprs: [t.a, Vnode(t.a) as _vnode] }
              └─StreamTableScan { table: t, columns: [t.a], stream_scan_type: ArrangementBackfill, stream_key: [t.a], pk: [a], dist: UpstreamHashShard(t.a) }
